{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a3d804",
   "metadata": {},
   "source": [
    "# Exercises XP: W2_D3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec07c18",
   "metadata": {},
   "source": [
    "### Exercise 1: Duplicate Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c25c3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e165fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Titanic dataset\n",
    "df = pd.read_csv(\"train.csv\")  # Charger le fichier train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7b2649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before removing duplicates: 891\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check number of rows before removing duplicates\n",
    "rows_before = df.shape[0]  # Nombre de lignes avant suppression\n",
    "print(\"Rows before removing duplicates:\", rows_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc99c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Detect duplicates\n",
    "duplicates = df.duplicated()  # Renvoie True pour les lignes dupliquÃ©es\n",
    "print(\"Number of duplicate rows:\", duplicates.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8426229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Remove duplicate rows\n",
    "df = df.drop_duplicates()  # Supprimer les doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b40d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after removing duplicates: 891\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Verify the number of rows after removal\n",
    "rows_after = df.shape[0]\n",
    "print(\"Rows after removing duplicates:\", rows_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d77234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Confirmation\n",
    "print(\"Number of rows removed:\", rows_before - rows_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5e223",
   "metadata": {},
   "source": [
    "### Exercise 2: Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3987bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\julia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 6.8/10.7 MB 36.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 32.6 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.15.3-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 7.9/41.0 MB 39.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 16.0/41.0 MB 38.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 23.6/41.0 MB 37.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 30.4/41.0 MB 36.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 38.3/41.0 MB 36.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.0 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 29.0 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a0e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6c3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fadf99a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check which columns have missing values\n",
    "print(df.isnull().sum())  # Shows number of missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba78a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Drop rows with missing values (example: drop rows where Age is missing)\n",
    "df_dropped = df.dropna(subset=['Age'])  # Remove only rows with missing 'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea6a3905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fill missing values with a constant (example: fill missing Embarked with 'Unknown')\n",
    "df_filled_constant = df.copy()\n",
    "df_filled_constant['Embarked'] = df_filled_constant['Embarked'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5823aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Impute missing numerical values with mean using SimpleImputer (example: Age)\n",
    "imputer = SimpleImputer(strategy='mean')  # You can also try 'median' or 'most_frequent'\n",
    "df_imputed = df.copy()\n",
    "df_imputed[['Age']] = imputer.fit_transform(df_imputed[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845a8bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Show how many missing values remain (should be 0 for Age and Embarked)\n",
    "print(df_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af1eab",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36bbe128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0c3e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a5d08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature 'FamilySize' by adding siblings/spouses (SibSp) and parents/children (Parch), plus 1 for self\n",
    "df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "813d090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the passenger's title (Mr, Mrs, Miss, etc.) from the 'Name' column\n",
    "# The double backslash escapes the dot correctly\n",
    "df[\"Title\"] = df[\"Name\"].str.extract(r'([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b368f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group less frequent titles under a common label 'Rare'\n",
    "rare_titles = df[\"Title\"].value_counts()[df[\"Title\"].value_counts() < 10].index\n",
    "df[\"Title\"] = df[\"Title\"].replace(rare_titles, \"Rare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddbb91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to convert categorical variables into numeric format\n",
    "# 'drop_first=True' removes one category to avoid multicollinearity\n",
    "df = pd.get_dummies(df, columns=[\"Sex\", \"Embarked\", \"Title\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87a62282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical columns to normalize\n",
    "num_cols = [\"Age\", \"Fare\", \"FamilySize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d4d07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the numerical columns using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac09ba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name       Age  SibSp  Parch  \\\n",
      "0                            Braund, Mr. Owen Harris -0.530377      1      0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  0.571831      1      0   \n",
      "2                             Heikkinen, Miss. Laina -0.254825      0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  0.365167      1      0   \n",
      "4                           Allen, Mr. William Henry  0.365167      0      0   \n",
      "\n",
      "             Ticket      Fare Cabin  FamilySize  Sex_male  Embarked_Q  \\\n",
      "0         A/5 21171 -0.502445   NaN    0.059160      True       False   \n",
      "1          PC 17599  0.786845   C85    0.059160     False       False   \n",
      "2  STON/O2. 3101282 -0.488854   NaN   -0.560975     False       False   \n",
      "3            113803  0.420730  C123    0.059160     False       False   \n",
      "4            373450 -0.486337   NaN   -0.560975      True       False   \n",
      "\n",
      "   Embarked_S  Title_Miss  Title_Mr  Title_Mrs  Title_Rare  \n",
      "0        True       False      True      False       False  \n",
      "1       False       False     False       True       False  \n",
      "2        True        True     False      False       False  \n",
      "3        True       False     False       True       False  \n",
      "4        True       False      True      False       False  \n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the transformed dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a06812",
   "metadata": {},
   "source": [
    "###  Exercise 4: Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c16ae",
   "metadata": {},
   "source": [
    "#### 4.1 Outlier Detection and Removal using Interquartile Range (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1acd68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c57286d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abf282b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to detect and remove outliers using the IQR method\n",
    "def remove_outliers_iqr(data, column):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    \n",
    "    # Calculate the Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define lower and upper bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter the data to remove outliers\n",
    "    filtered_data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90dbef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original size\n",
    "original_shape = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ba16b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from 'Fare'\n",
    "df = remove_outliers_iqr(df, 'Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e05b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from 'Age'\n",
    "df = remove_outliers_iqr(df, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27bfe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (891, 12)\n",
      "Shape after outlier removal: (601, 12)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape before and after outlier removal\n",
    "print(\"Original shape:\", original_shape)\n",
    "print(\"Shape after outlier removal:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4f81f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Age        Fare\n",
      "count  601.000000  601.000000\n",
      "mean    28.152662   18.610960\n",
      "std     13.336564   13.669143\n",
      "min      0.420000    0.000000\n",
      "25%     20.000000    7.925000\n",
      "50%     28.000000   13.000000\n",
      "75%     36.000000   26.250000\n",
      "max     62.000000   65.000000\n"
     ]
    }
   ],
   "source": [
    "# Optional: show the summary statistics to see the effect\n",
    "print(df[['Age', 'Fare']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306249c",
   "metadata": {},
   "source": [
    "#### 4.2 Outlier Detection and Removal using Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79fdef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ad2c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89a65fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in Age and Fare to avoid NaNs during Z-score calculation\n",
    "df = df.dropna(subset=[\"Age\", \"Fare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c74186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Z-scores for Age and Fare\n",
    "z_scores = stats.zscore(df[[\"Age\", \"Fare\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef81f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of z-scores, using the same index as the original df to ensure alignment\n",
    "z_df = pd.DataFrame(z_scores, columns=[\"Age_z\", \"Fare_z\"], index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bdc5e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where both Age_z and Fare_z are between -3 and 3 (i.e., not outliers)\n",
    "df_clean = df[(z_df[\"Age_z\"].abs() < 3) & (z_df[\"Fare_z\"].abs() < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "458704cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before cleaning: 714\n",
      "Rows after cleaning: 694\n"
     ]
    }
   ],
   "source": [
    "# Display the number of rows before and after outlier removal\n",
    "print(f\"Rows before cleaning: {df.shape[0]}\")\n",
    "print(f\"Rows after cleaning: {df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3082bbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "# Optionally preview the cleaned dataset\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378ea2b",
   "metadata": {},
   "source": [
    "### Exercise 5: Data Standardization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0ea8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2551779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5377604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numerical columns we want to transform\n",
    "# You may adjust these columns depending on your analysis\n",
    "num_cols = [\"Age\", \"Fare\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ce3e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in selected columns\n",
    "df = df.dropna(subset=num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a76be4",
   "metadata": {},
   "source": [
    "\n",
    "##### 1 Standardization using StandardScaler (mean = 0, std = 1)\n",
    "##### This is useful for algorithms that assume data is normally distributed\n",
    "##### ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cf74b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "df_standardized = df.copy()  # Create a copy to store standardized data\n",
    "df_standardized[num_cols] = standard_scaler.fit_transform(df_standardized[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e8eaa",
   "metadata": {},
   "source": [
    "##### 1 Normalization using MinMaxScaler (values scaled between 0 and 1)\n",
    "##### This is useful when you want to compare features with different units or scales\n",
    "##### ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed1fb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "df_normalized = df.copy()  # Create another copy for normalized data\n",
    "df_normalized[num_cols] = minmax_scaler.fit_transform(df_normalized[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44218f",
   "metadata": {},
   "source": [
    "##### Show a preview of both versions for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "202a8814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Data:\n",
      "        Age      Fare\n",
      "0 -0.530377 -0.518978\n",
      "1  0.571831  0.691897\n",
      "2 -0.254825 -0.506214\n",
      "3  0.365167  0.348049\n",
      "4  0.365167 -0.503850\n"
     ]
    }
   ],
   "source": [
    "print(\"Standardized Data:\")\n",
    "print(df_standardized[num_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c299b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Data:\n",
      "        Age      Fare\n",
      "0  0.271174  0.014151\n",
      "1  0.472229  0.139136\n",
      "2  0.321438  0.015469\n",
      "3  0.434531  0.103644\n",
      "4  0.434531  0.015713\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNormalized Data:\")\n",
    "print(df_normalized[num_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d8547",
   "metadata": {},
   "source": [
    "### Exercise 6: Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "294af55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a09a3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b2897",
   "metadata": {},
   "source": [
    "##### ---------------------------------------------\n",
    "##### 1 Identify categorical columns\n",
    "##### We'll focus on 'Sex' and 'Embarked' for nominal variables\n",
    "##### And simulate 'Pclass' as an ordinal variable (already encoded as 1, 2, 3)\n",
    "##### ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e24a3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in 'Embarked' (to avoid issues when encoding)\n",
    "df = df.dropna(subset=[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7edc8",
   "metadata": {},
   "source": [
    "##### ---------------------------------------------\n",
    "##### 2 One-Hot Encoding for Nominal Variables\n",
    "##### This will create new binary columns for each category\n",
    "##### ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1394995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=[\"Sex\", \"Embarked\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc93d58",
   "metadata": {},
   "source": [
    "##### ---------------------------------------------\n",
    "##### 3 Label Encoding for Ordinal Variable (simulated here with 'Pclass')\n",
    "##### Normally, you'd use this if values have an order like: Low < Medium < High\n",
    "##### But 'Pclass' is already numeric, so we show the usage with a new example\n",
    "##### ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: suppose we have a custom ordinal feature\n",
    "# df[\"CabinClass\"] = [\"Basic\", \"Premium\", \"Luxury\", \"Basic\", \"Luxury\"]\n",
    "# label_encoder = LabelEncoder()\n",
    "# df[\"CabinClass_encoded\"] = label_encoder.fit_transform(df[\"CabinClass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c8a2479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CabinClass  CabinClass_encoded\n",
      "0      Basic                   0\n",
      "1    Premium                   2\n",
      "2     Luxury                   1\n",
      "3      Basic                   0\n",
      "4     Luxury                   1\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"CabinClass\": [\"Basic\", \"Premium\", \"Luxury\", \"Basic\", \"Luxury\"]\n",
    "})\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"CabinClass_encoded\"] = label_encoder.fit_transform(df[\"CabinClass\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b211baa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex_male  Embarked_Q  Embarked_S\n",
      "0      True       False        True\n",
      "1     False       False       False\n",
      "2     False       False        True\n",
      "3     False       False        True\n",
      "4      True       False        True\n"
     ]
    }
   ],
   "source": [
    "# Display the encoded dataset\n",
    "print(df_encoded[[\"Sex_male\", \"Embarked_Q\", \"Embarked_S\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a993bd",
   "metadata": {},
   "source": [
    "### Exercise 7: Data Transformation for Age Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a35b8faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12b6ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "76f0d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing Age values (to avoid issues during binning)\n",
    "df = df.dropna(subset=[\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68e81e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create age groups using pd.cut()\n",
    "# This will divide the Age column into 5 categories: Child, Teenager, Young Adult, Adult, and Senior\n",
    "age_bins = [0, 12, 18, 35, 60, 100]\n",
    "age_labels = [\"Child\", \"Teenager\", \"Young Adult\", \"Adult\", \"Senior\"]\n",
    "df[\"AgeGroup\"] = pd.cut(df[\"Age\"], bins=age_bins, labels=age_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47ab8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply one-hot encoding to the AgeGroup column\n",
    "df = pd.get_dummies(df, columns=[\"AgeGroup\"], prefix=\"AgeGroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "322acfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age  AgeGroup_Child  AgeGroup_Teenager  AgeGroup_Young Adult  \\\n",
      "0  22.0           False              False                  True   \n",
      "1  38.0           False              False                 False   \n",
      "2  26.0           False              False                  True   \n",
      "3  35.0           False              False                  True   \n",
      "4  35.0           False              False                  True   \n",
      "\n",
      "   AgeGroup_Adult  AgeGroup_Senior  \n",
      "0           False            False  \n",
      "1            True            False  \n",
      "2           False            False  \n",
      "3           False            False  \n",
      "4           False            False  \n"
     ]
    }
   ],
   "source": [
    "# 3. Preview the result\n",
    "print(df[[\"Age\", \"AgeGroup_Child\", \"AgeGroup_Teenager\", \"AgeGroup_Young Adult\", \"AgeGroup_Adult\", \"AgeGroup_Senior\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a763da1",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "##### pd.cut() is used to divide continuous values (like Age) into intervals or \"bins\".\n",
    "##### labels=age_labels assigns human-readable names to each age bin.\n",
    "##### pd.get_dummies() converts each age category into a binary feature column (e.g., AgeGroup_Child = 1 if the person is a child).\n",
    "##### We dropped rows with missing Age to avoid errors during binning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
