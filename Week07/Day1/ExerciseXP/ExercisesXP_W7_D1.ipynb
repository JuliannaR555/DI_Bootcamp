{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78dc00f3",
   "metadata": {},
   "source": [
    "## Exercises XP: W7_D1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292641ee",
   "metadata": {},
   "source": [
    "### LLM Concepts and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b5cb9",
   "metadata": {},
   "source": [
    "#### What You’ll Learn\n",
    "\n",
    "- Deepen your understanding of core Large Language Model (LLM) concepts.  \n",
    "- Apply theoretical knowledge to practical scenarios.  \n",
    "- Develop critical thinking skills regarding LLM applications and ethics.  \n",
    "- Gain proficiency in comparing and contrasting various LLM architectures and techniques.\n",
    "\n",
    "---\n",
    "\n",
    "#### What You Will Create\n",
    "\n",
    "- Comparative tables analyzing NLP paradigms and BERT variations.  \n",
    "- Detailed descriptions of LLM architectures and their applications.  \n",
    "- Explanations of pre-training benefits and ethical considerations.  \n",
    "- Analyses of self-attention, multi-head attention, and positional encoding.  \n",
    "- Justifications for model selection in various NLP tasks.  \n",
    "- Evaluations of softmax temperature’s impact on language model output.  \n",
    "- Practical applications of learned concepts through scenario-based responses.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1: Traditional vs. Modern NLP – A Comparative Analysis\n",
    "\n",
    "1. Create a table comparing and contrasting the traditional and modern NLP paradigms. Include the following aspects:\n",
    "\n",
    "- Feature Engineering (manual vs. automatic)  \n",
    "- Word Representations (static vs. contextual)  \n",
    "- Model Architectures (shallow vs. deep)  \n",
    "- Training Methodology (task-specific vs. pre-training/fine-tuning)  \n",
    "- Key Examples of Models (e.g., Naïve Bayes, BERT)  \n",
    "- Advantages and Disadvantages of each paradigm\n",
    "\n",
    "2. Discuss how the evolution from traditional to modern NLP has impacted the scalability and efficiency of NLP applications.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: LLM Architecture and Application Scenarios\n",
    "\n",
    "For each of the following LLM architectures (BERT, GPT, T5), describe:\n",
    "\n",
    "- The core architectural differences (e.g., bidirectional vs. unidirectional, masked language modeling vs. causal language modeling).  \n",
    "- A specific real-world application where that architecture excels.  \n",
    "- Explain why that specific architecture is well suited for that particular application.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3: The Benefits and Ethical Considerations of Pre-training\n",
    "\n",
    "- Explain in your own words the five key benefits of pre-trained models discussed in the lesson (improved generalization, reduced need for labeled data, faster fine-tuning, transfer learning, and robustness).  \n",
    "- Discuss potential ethical concerns associated with pre-training LLMs on massive datasets, such as bias, misinformation, and misuse.  \n",
    "- Propose potential mitigation strategies to address these ethical concerns.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 4: Transformer Architecture Deep Dive\n",
    "\n",
    "#### Explain Self-Attention and Multi-Head Attention:\n",
    "\n",
    "- Describe in detail how the self-attention mechanism works within a Transformer.  \n",
    "- Explain the purpose and advantages of multi-head attention compared to single-head attention.  \n",
    "- Provide a concrete example (different from the lesson) of a sentence and illustrate how multi-head attention might process it, focusing on different relationships between words.\n",
    "\n",
    "#### Pre-training Objectives:\n",
    "\n",
    "- Compare and contrast Masked Language Modeling (MLM) and Causal Language Modeling (CLM).  \n",
    "- Describe a scenario where MLM would be more appropriate and a scenario where CLM would be more appropriate.  \n",
    "- Explain why early BERT models used Next Sentence Prediction (NSP) and why modern models tend to avoid it.\n",
    "\n",
    "#### Transformer Model Selection:\n",
    "\n",
    "You are tasked with building the following NLP applications. For each, specify which type of Transformer model (Encoder-only, Decoder-only, or Encoder-Decoder) would be most suitable and justify your choice:\n",
    "\n",
    "- A system that analyzes customer reviews to determine if they are positive, negative, or neutral.  \n",
    "- A chatbot that can generate creative and engaging responses in a conversation.  \n",
    "- A service that automatically translates technical documents from English to Spanish.  \n",
    "\n",
    "Explain the advantages of the chosen model type for each particular task.\n",
    "\n",
    "#### Positional Encoding:\n",
    "\n",
    "- Explain the purpose of positional encoding, and why it is important for the transformer architecture.  \n",
    "- Give an example of a situation where the lack of positional encoding would cause a problem.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 5: BERT Variations – Choose Your Detective\n",
    "\n",
    "For each of the following scenarios, identify which BERT variation (RoBERTa, ALBERT, DistilBERT, ELECTRA, XLM-RoBERTa) would be most suitable and explain why:\n",
    "\n",
    "- Scenario 1: Real-time sentiment analysis on mobile app with limited resources.  \n",
    "- Scenario 2: Research on legal documents requiring high accuracy.  \n",
    "- Scenario 3: Global customer support in multiple languages.  \n",
    "- Scenario 4: Efficient pretraining and token replacement detection.  \n",
    "- Scenario 5: Efficient NLP in resource-constrained environments.\n",
    "\n",
    "Create a table comparing the key features and trade-offs of each BERT variation discussed in the lesson. Include aspects like:\n",
    "\n",
    "- Training data and methods  \n",
    "- Model size and efficiency  \n",
    "- Specific optimizations and innovations  \n",
    "- Ideal use cases\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 6: Softmax Temperature – The Randomness Regulator\n",
    "\n",
    "#### Temperature Scenarios:\n",
    "\n",
    "Describe how the output of a language model would differ in the following scenarios:\n",
    "\n",
    "- Softmax temperature set to 0.2  \n",
    "- Softmax temperature set to 1.5  \n",
    "- Softmax temperature set to 1\n",
    "\n",
    "#### Application Design:\n",
    "\n",
    "- You are designing a system that generates personalized bedtime stories for children. Explain how you would use softmax temperature to control the creativity and coherence of the stories.  \n",
    "- You are building a system that automatically generates summaries of financial reports. Explain how you would use softmax temperature to ensure accuracy and reliability.\n",
    "\n",
    "#### Temperature and Bias:\n",
    "\n",
    "- Discuss how adjusting softmax temperature might affect the potential for bias in a language model’s output.  \n",
    "- Give a practical example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905eaff2",
   "metadata": {},
   "source": [
    "### Exercise 1: Traditional vs. Modern NLP: A Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa0ce1",
   "metadata": {},
   "source": [
    "#### Traditional NLP vs Modern NLP\n",
    "\n",
    "| Aspect                  | Traditional NLP                              | Modern NLP (LLMs)                               |\n",
    "|-------------------------|----------------------------------------------|------------------------------------------------|\n",
    "| **Feature Engineering** | Manual, handcrafted features (e.g., TF-IDF)  | Automatic, learned representations via deep nets |\n",
    "| **Word Representations**| Static embeddings (Word2Vec, GloVe)          | Contextual embeddings (BERT, GPT)               |\n",
    "| **Model Architectures** | Shallow models (Naïve Bayes, SVM, HMM)       | Deep architectures (Transformers)               |\n",
    "| **Training Methodology**| Task-specific training from scratch          | Pre-training + fine-tuning (transfer learning)  |\n",
    "| **Examples of Models**  | Naïve Bayes, Logistic Regression, HMM        | BERT, GPT, T5, RoBERTa                          |\n",
    "| **Advantages**          | Simple, fast, low data requirements          | High accuracy, adaptable across tasks           |\n",
    "| **Disadvantages**       | Poor generalization, heavy manual effort     | Computationally expensive, requires large data  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec7d4f",
   "metadata": {},
   "source": [
    "#### Impact of Evolution from Traditional to Modern NLP\n",
    "\n",
    "- **Scalability**:  \n",
    "  Modern NLP models scale much better because they rely on pre-trained representations. Traditional methods required building separate feature sets for each task, which was time-consuming and limited scalability.\n",
    "\n",
    "- **Efficiency**:  \n",
    "  Pre-training allows modern models to adapt quickly to new tasks with minimal labeled data, reducing both development time and cost compared to traditional task-specific pipelines.\n",
    "\n",
    "- **Performance**:  \n",
    "  Contextual embeddings in modern NLP capture word meaning depending on context (e.g., \"bank\" as a riverbank vs financial institution), which significantly improves accuracy in real-world applications.\n",
    "\n",
    "- **Practical Impact**:  \n",
    "  This shift has enabled applications like conversational AI, multilingual translation, and zero-shot learning, which were nearly impossible with traditional NLP approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4bdb2d",
   "metadata": {},
   "source": [
    "### Evolution: Traditional NLP → Modern NLP\n",
    "\n",
    "```\n",
    "Traditional NLP\n",
    "   |\n",
    "   |-- Manual feature engineering\n",
    "   |-- Static word embeddings\n",
    "   |-- Shallow models (SVM, Naïve Bayes)\n",
    "   V\n",
    "Modern NLP (LLMs)\n",
    "   |\n",
    "   |-- Automatic representation learning\n",
    "   |-- Contextual embeddings\n",
    "   |-- Transformers (BERT, GPT, T5)\n",
    "   |-- Pre-training + fine-tuning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e13d8a",
   "metadata": {},
   "source": [
    "### Exercise 2: LLM Architecture and Application Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de245c47",
   "metadata": {},
   "source": [
    "#### Comparison of LLM Architectures: BERT vs GPT vs T5\n",
    "\n",
    "| Model | Architecture Type | Directionality | Pre-training Objective | Strengths | Typical Use Cases |\n",
    "|-------|-------------------|----------------|------------------------|-----------|-------------------|\n",
    "| **BERT** | Encoder-only | Bidirectional | Masked Language Modeling (MLM) + (NSP in early versions) | Strong understanding of context, good for classification and retrieval tasks | Sentiment analysis, Named Entity Recognition, Question Answering |\n",
    "| **GPT**  | Decoder-only | Unidirectional (left-to-right) | Causal Language Modeling (next word prediction) | Excellent text generation, fluent and coherent outputs | Chatbots, Story generation, Autocomplete |\n",
    "| **T5**   | Encoder-Decoder | Seq2Seq (input → output) | Text-to-text framework (span corruption for pre-training) | Flexible: handles multiple tasks with unified text-to-text approach | Translation, Summarization, Question answering |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c68f58e",
   "metadata": {},
   "source": [
    "### Real-world Applications and Suitability\n",
    "\n",
    "**BERT: Legal Document Classification**  \n",
    "- BERT’s bidirectional context allows it to understand nuanced language in legal texts.  \n",
    "- Pre-training with MLM helps capture precise semantics, crucial for domain-specific classification.  \n",
    "\n",
    "**GPT: Conversational Chatbots**  \n",
    "- GPT’s causal modeling excels in generating fluent, natural-sounding replies.  \n",
    "- Its autoregressive nature makes it ideal for creative content like stories, dialogue, and code completion.  \n",
    "\n",
    "**T5: Document Translation and Summarization**  \n",
    "- T5’s encoder-decoder architecture is perfect for tasks that transform input text into new text formats.  \n",
    "- Unified text-to-text framework allows it to handle translation and summarization without major architectural changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20f43c",
   "metadata": {},
   "source": [
    "### Exercise 3: The Benefits and Ethical Considerations of Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97177847",
   "metadata": {},
   "source": [
    "#### 1. Five Key Benefits of Pre-trained Models\n",
    "\n",
    "1. **Improved Generalization**  \n",
    "   - Pre-trained models capture broad language patterns, allowing them to perform well on new, unseen tasks.\n",
    "\n",
    "2. **Reduced Need for Labeled Data**  \n",
    "   - Fine-tuning requires fewer labeled examples since the model already understands general language features.\n",
    "\n",
    "3. **Faster Fine-tuning**  \n",
    "   - Training time is reduced because the model starts from a strong baseline rather than from scratch.\n",
    "\n",
    "4. **Transfer Learning**  \n",
    "   - Knowledge gained from one domain (e.g., Wikipedia) can transfer to another (e.g., medical text classification).\n",
    "\n",
    "5. **Robustness**  \n",
    "   - Models are more resilient to noisy or varied inputs due to exposure to massive and diverse datasets during pre-training.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Ethical Concerns of Pre-training on Massive Datasets\n",
    "\n",
    "- **Bias**  \n",
    "  - Pre-trained models inherit biases present in their training data (e.g., gender, racial stereotypes).  \n",
    "\n",
    "- **Misinformation**  \n",
    "  - Large-scale scraping may include inaccurate or harmful information, leading to potential misuse in critical domains.  \n",
    "\n",
    "- **Misuse**  \n",
    "  - Models can be exploited to generate harmful content (e.g., disinformation, spam, deepfakes).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Mitigation Strategies\n",
    "\n",
    "- **Bias Auditing and Debiasing**  \n",
    "  - Conduct regular audits and apply techniques such as bias correction and reweighting of datasets.\n",
    "\n",
    "- **Data Curation and Filtering**  \n",
    "  - Use high-quality and domain-specific datasets, removing harmful or low-quality content.\n",
    "\n",
    "- **Human-in-the-Loop Monitoring**  \n",
    "  - Include human oversight during critical applications (e.g., legal, medical systems).\n",
    "\n",
    "- **Transparency and Explainability**  \n",
    "  - Provide clear documentation (model cards) about training data and limitations of the model.\n",
    "\n",
    "- **Access Control and Usage Policies**  \n",
    "  - Restrict usage of powerful LLMs to prevent malicious exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a4d63",
   "metadata": {},
   "source": [
    "### Exercise 4 : Transformer Architecture Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7a17d",
   "metadata": {},
   "source": [
    "#### A. Self-Attention and Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be6939",
   "metadata": {},
   "source": [
    "#### 1. Self-Attention Mechanism\n",
    "\n",
    "- **Goal**: Determine how much each word in a sentence should focus on every other word.  \n",
    "- **Key idea**: For each token, compute relationships with all other tokens to build contextualized representations.  \n",
    "\n",
    "**Process:**\n",
    "1. Transform each input token into **Query (Q)**, **Key (K)**, and **Value (V)** vectors.\n",
    "2. Compute attention scores = *Q · K^T / sqrt(d_k)* (scaled dot-product).\n",
    "3. Apply **softmax** to normalize scores → attention weights.\n",
    "4. Multiply weights by V to get weighted representations.\n",
    "5. Sum weighted values to produce the output for each token.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Multi-Head Attention\n",
    "\n",
    "- **Purpose**: Instead of one attention mechanism, use multiple \"heads\" to capture **different relationships** (e.g., syntactic vs semantic).  \n",
    "- Each head performs attention independently, then results are concatenated and projected.  \n",
    "\n",
    "**Advantage:**\n",
    "- Allows the model to capture **varied contextual information** simultaneously (e.g., subject-object vs temporal relations).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Example\n",
    "\n",
    "Sentence:  \n",
    "\n",
    "*The cat chased the mouse across the garden.*\n",
    "\n",
    "\n",
    "- One head might focus on **subject-object** relation (\"cat\" → \"chased\").  \n",
    "- Another head might focus on **location** (\"mouse\" → \"across the garden\").  \n",
    "- Combined heads provide **richer understanding** than a single-head mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d51fb",
   "metadata": {},
   "source": [
    "#### B. Pre-training Objectives: MLM vs CLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ee0f8",
   "metadata": {},
   "source": [
    "#### 1. Masked Language Modeling (MLM)\n",
    "\n",
    "- **Used by**: BERT  \n",
    "- **How it works**: Randomly masks words in the input and trains the model to predict them.  \n",
    "- **Benefit**: Captures **bidirectional context** (both left and right of the masked word).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Causal Language Modeling (CLM)\n",
    "\n",
    "- **Used by**: GPT  \n",
    "- **How it works**: Predicts the next word given previous words (left-to-right).  \n",
    "- **Benefit**: Ideal for **text generation** where future context is unknown.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. When to use\n",
    "\n",
    "- **MLM scenario**: Sentiment analysis, NER → Needs full sentence context.  \n",
    "- **CLM scenario**: Chatbots, story generation → Needs autoregressive prediction.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Next Sentence Prediction (NSP)\n",
    "\n",
    "- **Early BERT used NSP** to predict if two sentences follow each other.  \n",
    "- **Modern models remove NSP** (e.g., RoBERTa) because it adds little value and can introduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e725344",
   "metadata": {},
   "source": [
    "#### C. Transformer Model Selection for Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be15868",
   "metadata": {},
   "source": [
    "#### 1. Sentiment Analysis (Customer Reviews)\n",
    "\n",
    "- **Best choice**: Encoder-only (e.g., BERT)  \n",
    "- **Why**: Bidirectional context is essential for classification; no text generation required.\n",
    "\n",
    "#### 2. Chatbot (Creative Conversations)\n",
    "\n",
    "- **Best choice**: Decoder-only (e.g., GPT)  \n",
    "- **Why**: Autoregressive nature suits generating fluent and creative responses.\n",
    "\n",
    "#### 3. Translation (English → Spanish)\n",
    "\n",
    "- **Best choice**: Encoder-Decoder (e.g., T5)  \n",
    "- **Why**: Encoder processes input language; decoder generates target language output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc065d",
   "metadata": {},
   "source": [
    "#### D. Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbef5a3",
   "metadata": {},
   "source": [
    "#### Purpose\n",
    "\n",
    "- Transformers process tokens **in parallel** (no inherent notion of order).  \n",
    "- Positional encoding injects **sequence order information** into token embeddings.\n",
    "\n",
    "\n",
    "#### Why Important\n",
    "\n",
    "- Without positional encoding, the model treats input as a **bag of words** → loses sentence structure.\n",
    "\n",
    "\n",
    "#### Example of Problem\n",
    "\n",
    "Sentence:  \n",
    "\n",
    "\"Dog bites man.\" vs \"Man bites dog.\"\n",
    "\n",
    "- Same words, different order → positional encoding ensures the model distinguishes meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91adf5f",
   "metadata": {},
   "source": [
    "### Exercise 5: BERT Variations - Choose Your Detective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf9cbd",
   "metadata": {},
   "source": [
    "#### Comparison of BERT Variations\n",
    "\n",
    "| Model         | Key Innovations / Optimizations             | Model Size & Efficiency | Training Data / Method | Ideal Use Cases |\n",
    "|---------------|---------------------------------------------|-------------------------|-----------------------|-----------------|\n",
    "| **RoBERTa**   | Removed NSP, dynamic masking, more data      | Large, high accuracy    | Trained on 160GB data | Tasks needing high accuracy (QA, NLU) |\n",
    "| **ALBERT**    | Parameter sharing, factorized embeddings     | Very efficient, smaller | Same tasks as BERT    | Low memory environments, classification |\n",
    "| **DistilBERT**| Distillation of BERT (smaller, faster)       | 40% smaller, 60% faster | Derived from BERT      | Real-time apps (mobile, web) |\n",
    "| **ELECTRA**   | Replaces MLM with Replaced Token Detection   | More sample efficient   | Pre-trained with RDT   | Faster pretraining, low compute tasks |\n",
    "| **XLM-R**     | Multilingual pretraining (100 languages)     | Large, multilingual     | Trained on CC-100 data | Cross-lingual tasks (translation, global QA) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4e676",
   "metadata": {},
   "source": [
    "#### Best BERT Variation for Each Scenario\n",
    "\n",
    "#### Scenario 1: Real-time sentiment analysis on a mobile app with limited resources\n",
    "- **Choice**: DistilBERT  \n",
    "- **Reason**: Lightweight and fast, ideal for mobile deployment while maintaining good accuracy.\n",
    "\n",
    "#### Scenario 2: Research on legal documents requiring high accuracy\n",
    "- **Choice**: RoBERTa  \n",
    "- **Reason**: High accuracy due to large-scale pretraining and removal of NSP, suitable for nuanced legal language.\n",
    "\n",
    "#### Scenario 3: Global customer support in multiple languages\n",
    "- **Choice**: XLM-RoBERTa  \n",
    "- **Reason**: Pre-trained on 100 languages, excels at cross-lingual understanding for multilingual support.\n",
    "\n",
    "#### Scenario 4: Efficient pretraining and token replacement detection\n",
    "- **Choice**: ELECTRA  \n",
    "- **Reason**: Replaced Token Detection is more sample-efficient than MLM, suitable for faster and cheaper pretraining.\n",
    "\n",
    "#### Scenario 5: Efficient NLP in resource-constrained environments\n",
    "- **Choice**: ALBERT  \n",
    "- **Reason**: Parameter sharing drastically reduces memory usage without losing much accuracy, ideal for constrained setups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666483d",
   "metadata": {},
   "source": [
    "### Exercise 6: Softmax Temperature - The Randomness Regulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91c386",
   "metadata": {},
   "source": [
    "#### 1. Temperature Scenarios\n",
    "\n",
    "**Temperature = 0.2 (Low Temperature)**\n",
    "- Output distribution becomes very **peaked**.\n",
    "- Model chooses the most likely word almost deterministically.\n",
    "- Leads to **high coherence** but **low creativity** (safe, repetitive outputs).\n",
    "\n",
    "**Temperature = 1 (Default)**\n",
    "- Standard behavior: balance between randomness and determinism.\n",
    "- Suitable for most tasks (balanced creativity and accuracy).\n",
    "\n",
    "**Temperature = 1.5 (High Temperature)**\n",
    "- Output distribution becomes **flatter** (more randomness).\n",
    "- Model explores less likely words → **creative but less coherent** responses.\n",
    "\n",
    "#### 2. Application Design\n",
    "\n",
    "**Bedtime Story Generation (Creative, Personalized)**\n",
    "- Use a **higher temperature** (e.g., 1.2 – 1.5) to allow imaginative variations.\n",
    "- Ensures unique and engaging storytelling for children.\n",
    "\n",
    "**Financial Report Summarization (Accurate, Reliable)**\n",
    "- Use a **low temperature** (e.g., 0.3 – 0.5) to prioritize precision.\n",
    "- Minimizes hallucinations and maintains factual accuracy.\n",
    "\n",
    "#### 3. Temperature and Bias\n",
    "\n",
    "- **Effect on Bias**:\n",
    "  - Low temperature → model outputs the most probable (and possibly biased) token more often.\n",
    "  - High temperature → introduces randomness, which may **reduce repetitive bias** but could also generate irrelevant or unsafe outputs.\n",
    "\n",
    "**Example**:\n",
    "- Low temperature chatbot: Always answers \"doctor = male, nurse = female\" (reinforcing bias).\n",
    "- Higher temperature: Might occasionally break bias but at cost of coherence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
