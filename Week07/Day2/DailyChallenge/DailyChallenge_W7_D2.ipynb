{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3iqcN_vTLaz"
   },
   "source": [
    "## Daily Challenge: W7_D2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS-30gFOTLOC"
   },
   "source": [
    "### How to Finetune LLMs with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EdPEMC6_bvJ"
   },
   "source": [
    "#### Introduction\n",
    "\n",
    "Parameter-Efficient Fine-Tuning (PEFT) methods, like **LoRA**, address the challenges of fine-tuning large language models (LLMs) by only updating a small subset of the model’s parameters.  \n",
    "This approach significantly **reduces computational and storage costs**, making LLM fine-tuning more accessible.\n",
    "\n",
    "PEFT techniques allow developers to **adapt pre-trained models to specific tasks without retraining the entire model**, leading to faster development cycles and reduced resource consumption.\n",
    "\n",
    "In this challenge, you will implement LoRA-based fine-tuning on a real dataset.\n",
    "\n",
    "\n",
    "#### What You’ll Learn\n",
    "\n",
    "- How to apply **Low-Rank Adaptation (LoRA)** to a pre-trained language model.\n",
    "- How to fine-tune a **LoRA-adapted model** using the Hugging Face PEFT library.\n",
    "- How to **save and load** a fine-tuned LoRA model.\n",
    "- How to **perform inference** using a fine-tuned LoRA model.\n",
    "\n",
    "\n",
    "#### What You Will Create\n",
    "\n",
    "You will build a **fine-tuned language model** that generates text based on a specific dataset of quotes, using LoRA adaptation.\n",
    "\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "We will use the dataset:  \n",
    "**`Abirate/english_quotes`**  \n",
    "Specifically: **a 10% sample of the training split**.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Install necessary libraries (`peft`, `datasets`, `transformers`).\n",
    "2. Load a pre-trained language model (`bigscience/bloomz-560m`) and its tokenizer.\n",
    "3. Load the dataset and **preprocess it** (tokenization).\n",
    "4. Configure LoRA using `LoraConfig`.\n",
    "5. Apply LoRA to the pre-trained model using `get_peft_model`.\n",
    "6. Set up training arguments using `TrainingArguments`.\n",
    "7. Initialize and **train the model** using `Trainer`.\n",
    "8. Save the **fine-tuned LoRA model**.\n",
    "9. Load the saved model using `PeftModel.from_pretrained`.\n",
    "10. Generate text using the fine-tuned model and the tokenizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN6WGd9CVp63"
   },
   "source": [
    "### 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruuPkXEgcdAe"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-R4-nYk9kCj"
   },
   "outputs": [],
   "source": [
    "pip install -U datasets huggingface_hub fsspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSd9J55NHQlJ"
   },
   "source": [
    "### Load model, preprocess dataset and configure LoRA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "54e09600b82e4c358b54d9684cc3259b",
      "3605fff808524864ab7559831b6cb717",
      "94af1614913049428aa065ba54d3c3ca",
      "c0ab571c23b54e48aa220bd387ce4434",
      "183ebae2fb074fa1bae131771c6195e8",
      "a57fe9fd98bc4bec981cb203dfe3ec41",
      "67c74a08325b4396b4f4f278205bf871",
      "ec4e2fb4a1d740dba9bc66319b6f6cb9",
      "6f4f36cb5ed446d28e7d19f72d54757c",
      "7259c1cda84c4e22967002fefe7bf1b9",
      "46c6302934e14752ab85f13148d98d0c",
      "dba51de6b5de4a3481fe2c39aa762345",
      "bcf0be4350dc4c028a043615e30ee997",
      "22b4e600b50d47b2916a38d18bb1b108",
      "86fc5aa35e364285b30694bc049d53a5",
      "4e90c4f100ab449982edcae1a93fd3b8",
      "0326d56aa0cc4dc387d7f1543e890089",
      "4b1c7f3ca5684b76abb508816d783689",
      "da560aa8d08046cc9ce8cbd2616cd81e",
      "39719c62a892478db6aa140c7e747f04",
      "5b43a322ef8c451cb8c4aa0de0e328ea",
      "8396475d212d4583bebb2ef04234cf7f",
      "6e44251fb47540fabe8ccf6963fe1198",
      "3ed7d9288b874fe3957fdd23afd04e3c",
      "2c6a893e45d5470bafe10a39a15d87da",
      "d2f222e15f3a4669a06021f0ff564552",
      "c2bc3c6302d34d7b81c31aa6b0015eb2",
      "3746ce2ba25b429ca6e0333494fbd5d1",
      "957d9bc55ae24b51b19ad5f73fe575be",
      "2e47464fc5554fae84906b46ca879d60",
      "a6aa094cf7ea468eb7e630b8b25f96a8",
      "108140897be54da0a38a816169d9c989",
      "d6fe95a6eaae4f88889cdd23c90771c9",
      "1b8fada73d5f4e2dbcd87c005212f438",
      "5e3366924c384385afc41822ccb4de09",
      "6c40daff041f43b4883cdd5d322fb9a9",
      "b3625dee843e40a09e2746d13443462e",
      "db71d46c6a8a4b2f8051b0ebf14cec6a",
      "2a949d8172e44badb6ad2cf7ae347e81",
      "75316cc824a747a69cdce57c81862d68",
      "bf327b0c90104dd1b2aa2737fa1957a6",
      "014f176be60649399735be7e4d517b88",
      "9762a8b2b16e4cc893955b9754f866f4",
      "c86f299e29ae4495930631dca3bc9cba"
     ]
    },
    "id": "hC7kW7sjWV7X",
    "outputId": "70d9b5e6-e4ad-46cf-f070-1271c2e523f0"
   },
   "outputs": [],
   "source": [
    "# Installer les dépendances\n",
    "!pip install -q peft transformers accelerate\n",
    "\n",
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Device & dtype\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_bfloat16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "dtype = torch.bfloat16 if use_bfloat16 else torch.float32\n",
    "\n",
    "print(f\" Using device: {device}, dtype: {dtype}\")\n",
    "\n",
    "# Tokenizer & modèle de base\n",
    "model_name = \"bigscience/bloom-560m\"\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Correction cruciale\n",
    "\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if device.type == \"cuda\" else None\n",
    ")\n",
    "\n",
    "# Charger un petit dataset (quotes)\n",
    "ds = load_dataset(\"Abirate/english_quotes\")\n",
    "from datasets import Dataset\n",
    "\n",
    "max_seq_len = 128\n",
    "\n",
    "def tokenize_fn(samples):\n",
    "    tokens = tokenizer(\n",
    "        samples[\"quote\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_len,\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "data = ds[\"train\"].shuffle(seed=42).select(range(int(0.1 * len(ds[\"train\"]))))\n",
    "data = data.map(tokenize_fn, batched=True)\n",
    "\n",
    "# 3. Debug si besoin\n",
    "sample = data[0]\n",
    "print(\"Quote:\", sample[\"quote\"])\n",
    "print(\"input_ids:\", sample[\"input_ids\"][:30])\n",
    "print(\"input length:\", len(sample[\"input_ids\"]))\n",
    "\n",
    "# Config LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_key_value\", \"dense\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Injection des adaptateurs LoRA\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Setup entraînement\n",
    "output_directory = \"./peft_lab_outputs\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    report_to=\"none\",\n",
    "    learning_rate=3e-2,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    fp16=(dtype == torch.float16),\n",
    "    bf16=(dtype == torch.bfloat16),\n",
    "    no_cuda=(device.type == \"cpu\")\n",
    ")\n",
    "sample = data[0]\n",
    "print(\"Quote:\", sample[\"quote\"])\n",
    "print(\"input_ids:\", sample[\"input_ids\"])\n",
    "print(\"input length:\", len(sample[\"input_ids\"]))\n",
    "# Entraînement avec Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J0GNddVHmId"
   },
   "source": [
    "#### Training Initialization Output - Interpretation\n",
    "\n",
    "- **Device Used:**  \n",
    "  `cuda` indicates that the model is using the GPU for training, which significantly speeds up computation compared to CPU.\n",
    "\n",
    "- **Data Type (dtype):**  \n",
    "  `torch.float32` means that the model is using 32-bit floating point precision. This is standard and accurate for most training tasks.\n",
    "\n",
    "- **Dataset Loaded:**  \n",
    "  The `\"Abirate/english_quotes\"` dataset was successfully loaded with 2,508 examples in the training split.  \n",
    "\n",
    "- **Tokenization Output:**  \n",
    "  The quote `\"I don't mind making jokes, but I don't want to look like one.\"` has been tokenized into 128 tokens (`input_ids`) with padding applied to reach `max_length=128`.\n",
    "\n",
    "- **Trainable Parameters:**  \n",
    "  Only `147,456` parameters are trainable out of a total of `559,362,048` model parameters.  \n",
    "  This means **only 0.0264% of the model** is updated during training thanks to LoRA (Low-Rank Adaptation).  \n",
    "  This is what makes training so efficient in terms of memory and compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK37QiaBH9_D"
   },
   "source": [
    "### Train the LoRA-adapted model using Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "HOQ-Xgux9txL",
    "outputId": "18510f88-4f75-483d-b2b0-787129a2a731"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeACPr5SIN_l"
   },
   "source": [
    "#### Training Output Summary\n",
    "\n",
    "- **Trainer Warning:**\n",
    "  `No label_names provided for model class PeftModelForCausalLM.`  \n",
    "  → This is a known warning when using PEFT models with `Trainer`.  \n",
    "  It simply means that no explicit label field is passed, so Trainer will assume a default. You can safely ignore it for causal language modeling.\n",
    "\n",
    "- **Training Loss (by step):**\n",
    "  The model's loss decreases over the course of training, indicating learning progress:\n",
    "\n",
    "#### Training Loss (by step)\n",
    "\n",
    "| Step | Loss    |\n",
    "|------|---------|\n",
    "| 10   | 30.42   |\n",
    "| 20   | 16.91   |\n",
    "| 30   | 12.98   |\n",
    "| 40   | 9.91    |\n",
    "| 50   | 9.66    |\n",
    "| 60   | 10.12   |\n",
    "| 70   | 10.16   |\n",
    "| 80   | 10.30   |\n",
    "| 90   | 9.45    |\n",
    "| 100  | 7.84    |\n",
    "| 110  | 8.11    |\n",
    "| 120  | 7.66    |\n",
    "\n",
    "\n",
    "- **Final Training Loss:**  \n",
    "  `11.80` (average across all steps)\n",
    "\n",
    "- **Training Stats:**\n",
    "  - Runtime: `44.47 seconds`\n",
    "  - Samples/sec: `5.62`\n",
    "  - Steps/sec: `2.81`\n",
    "  - FLOPs: `58.07 TFLOPs`\n",
    "  - Epochs: `1.0`\n",
    "\n",
    "**Interpretation:**  \n",
    "The training loss decreases steadily, showing that the LoRA-adapted model is learning to better predict the tokenized quotes. The training was efficient and successfully completed one full epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIIbhi61JQmR"
   },
   "source": [
    "### Visualize training loss and speed metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "61KPDtx9WgaC",
    "outputId": "f042231f-09d0-45f0-ae15-d77dfd5b02ea"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Données de ton output HuggingFace Trainer\n",
    "train_output = {\n",
    "    \"global_step\": 125,\n",
    "    \"training_loss\": 9.96220669555664,\n",
    "    \"metrics\": {\n",
    "        \"train_runtime\": 29.6052,\n",
    "        \"train_samples_per_second\": 8.444,\n",
    "        \"train_steps_per_second\": 4.222,\n",
    "        \"total_flos\": 5.807e+13,\n",
    "        \"train_loss\": 9.96220669555664,\n",
    "        \"epoch\": 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot principal\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 1. Training loss\n",
    "ax[0].bar([\"Training Loss\"], [train_output[\"training_loss\"]], color=\"coral\")\n",
    "ax[0].set_title(\" Training Loss Final\")\n",
    "ax[0].set_ylim(0, max(10, train_output[\"training_loss\"] + 1))\n",
    "ax[0].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# 2. Training Speed\n",
    "ax[1].bar([\"Samples/s\", \"Steps/s\"], [\n",
    "    train_output[\"metrics\"][\"train_samples_per_second\"],\n",
    "    train_output[\"metrics\"][\"train_steps_per_second\"]\n",
    "], color=[\"skyblue\", \"lightgreen\"])\n",
    "ax[1].set_title(\" Training Speed\")\n",
    "ax[1].set_ylim(0, max(train_output[\"metrics\"][\"train_samples_per_second\"], 10) + 1)\n",
    "ax[1].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Layout\n",
    "plt.suptitle(f\" Training Summary — {train_output['global_step']} steps | Epoch {train_output['metrics']['epoch']}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fDrFuZtKAbg"
   },
   "source": [
    "### Interpretation: Training Summary Visualization\n",
    "\n",
    "The image above summarizes two key aspects of the fine-tuning process using LoRA:\n",
    "\n",
    "#### Training Loss Final (Left Chart)\n",
    "- The final **training loss** is approximately `10.0`.\n",
    "- This value reflects the average difference between the predicted and actual tokens during training.\n",
    "- The steady decrease (seen in earlier logs) indicates that the model was learning meaningful patterns from the quote dataset.\n",
    "\n",
    "#### Training Speed (Right Chart)\n",
    "- **Samples per second:** ≈ 8.44  \n",
    "  → This shows how many examples the model processed each second during training.\n",
    "- **Steps per second:** ≈ 4.22  \n",
    "  → This indicates how quickly the optimizer updated the weights.\n",
    "\n",
    "**Conclusion:**  \n",
    "Training was completed efficiently using GPU acceleration. Despite only 0.02% of the parameters being trainable (thanks to LoRA), the model successfully reduced its loss and learned from the data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
