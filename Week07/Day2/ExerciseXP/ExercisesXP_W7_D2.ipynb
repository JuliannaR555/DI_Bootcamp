{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f9e3c8",
   "metadata": {},
   "source": [
    "## Exercises XP: W7_D2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cb653",
   "metadata": {},
   "source": [
    "#### What Youâ€™ll Learn\n",
    "\n",
    "- The fundamentals of LoRA (Low-Rank Adaptation) and its application in deep learning models.  \n",
    "- How to implement LoRA layers in PyTorch and integrate them into existing neural network architectures.  \n",
    "- The differences between standard linear layers and LoRA-enhanced layers.  \n",
    "- How to merge LoRA matrices with original weights for efficiency.  \n",
    "- Freezing original layers to fine-tune only the LoRA layers in a model.\n",
    "\n",
    "---\n",
    "\n",
    "#### What You Will Create\n",
    "\n",
    "- A custom LoRALayer module for efficient model adaptation.  \n",
    "- A LinearWithLoRA class that applies LoRA to existing linear layers.  \n",
    "- A modified Multi-Layer Perceptron (MLP) incorporating LoRA layers.  \n",
    "- A complete training pipeline using LoRA-enhanced models.  \n",
    "- A workflow to freeze and fine-tune only the LoRA parameters for efficient training.\n",
    "\n",
    "---\n",
    "\n",
    "#### LoRA Implementation Exercises\n",
    "\n",
    "#### Exercise 1: Implementing the LoRALayer\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a custom PyTorch module called *LoRALayer* that introduces low-rank adaptation matrices A and B.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a PyTorch class *LoRALayer* inheriting from *nn.Module*.  \n",
    "2. Define the *__init__* method to initialize matrices A and B with the appropriate dimensions.  \n",
    "3. Implement the *forward* method to compute the LoRA transformation on input *x*.  \n",
    "4. Test the class with a small input tensor to verify its functionality.\n",
    "\n",
    "---\n",
    "\n",
    "#### Exercise 2: Implementing the LinearWithLoRA Layer\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Extend a standard PyTorch Linear layer to incorporate the *LoRALayer* for adaptable training.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a new class *LinearWithLoRA* that wraps an existing *nn.Linear* layer.  \n",
    "2. Add an instance of *LoRALayer* to introduce low-rank adaptation.  \n",
    "3. Implement the *forward* method to return the sum of the standard Linear transformation and the LoRA adaptation.  \n",
    "4. Test this new layer with an input tensor.\n",
    "\n",
    "---\n",
    "\n",
    "#### Exercise 3: Creating a Small Neural Network and Applying LoRA\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a simple feedforward neural network and apply LoRA to one of its layers.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Define a single-layer neural network using *nn.Linear*.  \n",
    "2. Generate a random input tensor to test the layer.  \n",
    "3. Replace the Linear layer with *LinearWithLoRA* and verify that the outputs remain unchanged initially.\n",
    "\n",
    "---\n",
    "\n",
    "#### Exercise 4: Merging LoRA Matrices and Testing Equivalence\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement an alternative approach where LoRA matrices are merged with the original weights for efficiency.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a new class *LinearWithLoRAMerged* that computes the combined weight matrix.  \n",
    "2. Ensure that the output remains the same as *LinearWithLoRA*.  \n",
    "3. Test with a sample input to verify correctness.\n",
    "\n",
    "---\n",
    "\n",
    "#### Exercise 5: Implementing a Multilayer Perceptron (MLP) and Replacing Layers with LoRA\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Extend a simple MLP and modify its layers to use LoRA.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Implement a 3-layer MLP.  \n",
    "2. Replace each Linear layer with *LinearWithLoRAMerged*.  \n",
    "3. Print the model architecture to verify modifications.\n",
    "\n",
    "---\n",
    "\n",
    "#### Exercise 6: Freezing the Original Linear Layers and Training LoRA\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Ensure only LoRA layers are trainable and train the model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Implement a function to freeze standard Linear layers.  \n",
    "2. Apply it to the MLP model.  \n",
    "3. Print trainable parameters to confirm only LoRA layers are trainable.  \n",
    "4. Train the model on a dataset and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83212434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545909dd",
   "metadata": {},
   "source": [
    "### Exercise 1: Implementing the LoRALayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        \"\"\"\n",
    "        LoRALayer initializes two low-rank matrices A and B\n",
    "        for efficient fine-tuning of large models.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Input dimension\n",
    "            out_dim (int): Output dimension\n",
    "            rank (int): Rank of the low-rank decomposition\n",
    "            alpha (float): Scaling factor for LoRA contribution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Standard deviation for initialization (scaled by rank)\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "\n",
    "        # Matrix A: projects input to low-rank space\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "\n",
    "        # Matrix B: projects back to output space (initialized as zeros)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "\n",
    "        # Scaling factor for LoRA contribution\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass applies low-rank adaptation:\n",
    "        (x @ A) @ B scaled by alpha\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, in_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: LoRA-transformed output of shape (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        # Step 1: Project input x into low-rank space using A\n",
    "        low_rank = x @ self.A\n",
    "\n",
    "        # Step 2: Project back to output space using B\n",
    "        lora_output = low_rank @ self.B\n",
    "\n",
    "        # Step 3: Scale the output by alpha\n",
    "        return self.alpha * lora_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ef1ef",
   "metadata": {},
   "source": [
    "### Testing LoRALayer with dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0acec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[ 0.3072,  0.1348, -1.7869,  3.1623]])\n",
      "Output: tensor([[0., 0., 0.]], grad_fn=<MulBackward0>)\n",
      "Output shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "in_dim = 4      # input dimension\n",
    "out_dim = 3     # output dimension\n",
    "rank = 2        # low-rank dimension\n",
    "alpha = 1.0     # scaling factor\n",
    "\n",
    "# Create layer\n",
    "lora_layer = LoRALayer(in_dim, out_dim, rank, alpha)\n",
    "\n",
    "# Dummy input tensor (batch_size=1)\n",
    "x = torch.randn(1, in_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = lora_layer(x)\n",
    "print(\"Input:\", x)\n",
    "print(\"Output:\", output)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49740025",
   "metadata": {},
   "source": [
    "### Manually modify B to test non-zero output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9476de3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after modifying B: tensor([[-1.2190, -1.2040, -0.5797]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Manually modify B to test non-zero output\n",
    "with torch.no_grad():\n",
    "    lora_layer.B.copy_(torch.randn(rank, out_dim))  # random non-zero B\n",
    "\n",
    "# Forward pass again\n",
    "output = lora_layer(x)\n",
    "print(\"Output after modifying B:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ec5ac",
   "metadata": {},
   "source": [
    "### Interpretation of the Two Outputs\n",
    "\n",
    "**First Output (zeros):**\n",
    "\n",
    "- The initial output of the *LoRALayer* is zero because matrix **B** is initialized with zeros.\n",
    "- Even if the multiplication *x @ A* produces non-zero values, multiplying by *B* results in zeros.\n",
    "- This is intentional in LoRA: starting with no contribution ensures the pre-trained weights are not disturbed at the beginning of fine-tuning.\n",
    "\n",
    "**Second Output (non-zero after modifying B):**\n",
    "\n",
    "- After manually setting matrix **B** to random values, the output becomes non-zero.\n",
    "- This confirms that the computation *(x @ A) @ B* alpha is working as expected.\n",
    "- During training, *A* and *B* will learn task-specific adaptations while keeping the original model parameters frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec77375",
   "metadata": {},
   "source": [
    "### Exercise 2: Implementing the LinearWithLoRA Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27c51a",
   "metadata": {},
   "source": [
    "#### LinearWithLoRA: Standard Linear + LoRA Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff10f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, rank: int, alpha: float):\n",
    "        \"\"\"\n",
    "        Wraps a standard nn.Linear layer and adds a LoRALayer for low-rank adaptation.\n",
    "\n",
    "        Args:\n",
    "            linear (nn.Linear): Pre-existing linear layer\n",
    "            rank (int): Rank for LoRA decomposition\n",
    "            alpha (float): Scaling factor for LoRA contribution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = linear  # Standard linear transformation\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features,  # input dimension\n",
    "            linear.out_features, # output dimension\n",
    "            rank,\n",
    "            alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sum of:\n",
    "        - Standard Linear output\n",
    "        - LoRA low-rank adaptation\n",
    "        \"\"\"\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae7287",
   "metadata": {},
   "source": [
    "#### Testing LinearWithLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b506aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[-0.8154, -0.7798, -0.6719,  0.5486]])\n",
      "Standard Linear output: tensor([[-0.3813, -0.1985, -0.5075]], grad_fn=<AddmmBackward0>)\n",
      "LinearWithLoRA output: tensor([[-0.3813, -0.1985, -0.5075]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "in_dim = 4\n",
    "out_dim = 3\n",
    "rank = 2\n",
    "alpha = 1.0\n",
    "\n",
    "# Create a standard linear layer\n",
    "linear_layer = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "# Wrap it with LoRA\n",
    "linear_with_lora = LinearWithLoRA(linear_layer, rank, alpha)\n",
    "\n",
    "# Dummy input tensor\n",
    "x = torch.randn(1, in_dim)\n",
    "\n",
    "# Outputs\n",
    "print(\"Input:\", x)\n",
    "print(\"Standard Linear output:\", linear_layer(x))\n",
    "print(\"LinearWithLoRA output:\", linear_with_lora(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb566b80",
   "metadata": {},
   "source": [
    "### Interpretation of LinearWithLoRA Test\n",
    "\n",
    "- The output of *LinearWithLoRA* is **identical** to the output of the standard *Linear* layer.  \n",
    "- This happens because matrix **B** inside the *LoRALayer* is initialized with zeros, meaning the LoRA contribution is initially **zero**.  \n",
    "- As a result, *LinearWithLoRA(x) = Linear(x)* at the beginning of training.  \n",
    "- During fine-tuning, the LoRA parameters (*A* and *B*) will update and add **low-rank adaptations** to the output without changing the pre-trained linear weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0aeb1",
   "metadata": {},
   "source": [
    "### Exercise 3: Creating a Small Neural Network and Applying LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56058a70",
   "metadata": {},
   "source": [
    "#### Interpretation of Small Network with LoRA\n",
    "\n",
    "- A simple neural network with a single *Linear* layer was created and tested.  \n",
    "- After replacing the *Linear* layer with *LinearWithLoRA*, the output remains **identical** at initialization.  \n",
    "- This demonstrates that adding LoRA layers does **not affect the initial behavior** of the model.  \n",
    "- During training, LoRA layers will adapt to the task while keeping the original weights frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da87cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[-0.0342, -1.9971, -0.8241, -1.4815]])\n",
      "Original network output: tensor([[-1.5232, -1.2068,  0.0345]], grad_fn=<AddmmBackward0>)\n",
      "Network with LoRA output: tensor([[-1.5232, -1.2068,  0.0345]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1. Define a simple single-layer network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Single Linear layer network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 2. Create and test the network\n",
    "in_dim = 4\n",
    "out_dim = 3\n",
    "net = SimpleNet(in_dim, out_dim)\n",
    "\n",
    "x = torch.randn(1, in_dim)\n",
    "print(\"Input:\", x)\n",
    "print(\"Original network output:\", net(x))\n",
    "\n",
    "# 3. Replace the Linear layer with LinearWithLoRA\n",
    "net.linear = LinearWithLoRA(net.linear, rank=2, alpha=1.0)\n",
    "print(\"Network with LoRA output:\", net(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de2b8a",
   "metadata": {},
   "source": [
    "### Interpretation of Small Network with LoRA\n",
    "\n",
    "- Input used: *tensor([[-0.0342, -1.9971, -0.8241, -1.4815]])*  \n",
    "- Original network output: *tensor([[-1.5232, -1.2068,  0.0345]])*  \n",
    "- Network with LoRA output: *tensor([[-1.5232, -1.2068,  0.0345]])*\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "- The outputs are identical because the LoRA contribution is initialized to zero (matrix **B** is filled with zeros).  \n",
    "- This confirms that introducing LoRA layers does **not change the original behavior** of the model at initialization.  \n",
    "- When training begins, the LoRA parameters (*A* and *B*) will learn task-specific adaptations while leaving the original network weights untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ced6c",
   "metadata": {},
   "source": [
    "### Exercise 4: Merging LoRA Matrices and Testing Equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32bd2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, rank: int, alpha: float):\n",
    "        \"\"\"\n",
    "        Combines LoRA matrices into a single weight update for efficiency.\n",
    "\n",
    "        Args:\n",
    "            linear (nn.Linear): Pre-existing linear layer\n",
    "            rank (int): Rank for LoRA decomposition\n",
    "            alpha (float): Scaling factor for LoRA contribution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, \n",
    "            linear.out_features,\n",
    "            rank,\n",
    "            alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the forward pass using merged LoRA weights:\n",
    "        combined_weight = W + alpha * (A @ B)\n",
    "        \"\"\"\n",
    "        # Combine low-rank matrices A and B\n",
    "        lora_matrix = self.lora.A @ self.lora.B  # shape: (in_dim, out_dim)\n",
    "\n",
    "        # Merge with original weights (transpose because nn.Linear stores weight as (out_dim, in_dim))\n",
    "        combined_weight = self.linear.weight + self.lora.alpha * lora_matrix.T\n",
    "\n",
    "        # Standard linear operation with combined weights\n",
    "        return F.linear(x, combined_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5797f",
   "metadata": {},
   "source": [
    "#### Testing LinearWithLoRAMerged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff4757d",
   "metadata": {},
   "source": [
    "##### Interpretation of LinearWithLoRAMerged Test\n",
    "\n",
    "- We tested two implementations:\n",
    "  1. *LinearWithLoRA* (standard, non-merged)\n",
    "  2. *LinearWithLoRAMerged* (merged weights)\n",
    "\n",
    "- Both outputs should be **identical** at initialization because:\n",
    "  - Matrix **B** is initialized to zeros in both cases.\n",
    "  - Therefore, the contribution *(A @ B)* is zero, and only the original linear weights are active.\n",
    "\n",
    "- This merged version is more **efficient** for inference because it avoids two separate matrix multiplications and directly uses a combined weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7438fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[ 2.1742, -1.3012,  1.3707,  0.1366]])\n",
      "LinearWithLoRA output: tensor([[-1.0264,  0.9748,  1.0572]], grad_fn=<AddBackward0>)\n",
      "LinearWithLoRAMerged output: tensor([[-1.0264,  0.9748,  1.0572]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a standard linear layer\n",
    "linear_layer = nn.Linear(4, 3)\n",
    "\n",
    "# Wrap with LinearWithLoRA (non-merged)\n",
    "layer_lora = LinearWithLoRA(linear_layer, rank=2, alpha=1.0)\n",
    "\n",
    "# Wrap with LinearWithLoRAMerged\n",
    "layer_lora_merged = LinearWithLoRAMerged(linear_layer, rank=2, alpha=1.0)\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(1, 4)\n",
    "\n",
    "# Outputs\n",
    "print(\"Input:\", x)\n",
    "print(\"LinearWithLoRA output:\", layer_lora(x))\n",
    "print(\"LinearWithLoRAMerged output:\", layer_lora_merged(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074ab9e",
   "metadata": {},
   "source": [
    "#### Interpretation of LinearWithLoRAMerged Test\n",
    "\n",
    "- Input used: *tensor([[ 2.1742, -1.3012,  1.3707,  0.1366]])* \n",
    "- *LinearWithLoRA* output: *tensor([[-1.0264,  0.9748,  1.0572]])*  \n",
    "- *LinearWithLoRAMerged* output: *tensor([[-1.0264,  0.9748,  1.0572]])*\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "- The outputs are identical, confirming that the merged implementation is mathematically equivalent to the non-merged version.  \n",
    "- At initialization, LoRA does not contribute (matrix **B** is zeros), so both methods behave like a standard *Linear* layer.  \n",
    "- The merged version is advantageous for **inference speed and memory** because it combines weights into a single matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4a3e4",
   "metadata": {},
   "source": [
    "### Exercise 5: Implementing a Multilayer Perceptron (MLP) and Replacing Layers with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e217a94",
   "metadata": {},
   "source": [
    "#### Interpretation of MLP with LoRA\n",
    "\n",
    "- A standard 3-layer MLP was built and tested on a dummy input.  \n",
    "- The same MLP was then modified by replacing each *Linear* layer with *LinearWithLoRAMerged*.  \n",
    "\n",
    "**Observations:**\n",
    "- The outputs of the original MLP and the LoRA-enhanced MLP are **identical at initialization** because the LoRA matrices start with zero contribution.  \n",
    "- This confirms that integrating LoRA layers does not disrupt the base modelâ€™s behavior.  \n",
    "- During fine-tuning, only the LoRA matrices will update, enabling parameter-efficient adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841cc618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[-1.0206,  0.8164,  1.1792, -1.3017]])\n",
      "Standard MLP output: tensor([[-0.4673, -0.3002,  0.0748]], grad_fn=<AddmmBackward0>)\n",
      "MLP with LoRA output: tensor([[-0.4673, -0.3002,  0.0748]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes):\n",
    "        \"\"\"\n",
    "        Standard 3-layer MLP:\n",
    "        Input -> Linear -> ReLU -> Linear -> ReLU -> Linear -> Output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# =========================================\n",
    "# Create and test the standard MLP\n",
    "# =========================================\n",
    "num_features = 4\n",
    "num_hidden_1 = 8\n",
    "num_hidden_2 = 6\n",
    "num_classes = 3\n",
    "\n",
    "# Create standard MLP\n",
    "mlp = MultilayerPerceptron(num_features, num_hidden_1, num_hidden_2, num_classes)\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(1, num_features)\n",
    "print(\"Input:\", x)\n",
    "print(\"Standard MLP output:\", mlp(x))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Replace Linear layers with LoRA-enhanced layers\n",
    "# =========================================\n",
    "# Deep copy to preserve original MLP\n",
    "import copy\n",
    "mlp_lora = copy.deepcopy(mlp)\n",
    "\n",
    "# Replace each Linear with LinearWithLoRAMerged\n",
    "mlp_lora.layers[0] = LinearWithLoRAMerged(mlp_lora.layers[0], rank=2, alpha=1.0)\n",
    "mlp_lora.layers[2] = LinearWithLoRAMerged(mlp_lora.layers[2], rank=2, alpha=1.0)\n",
    "mlp_lora.layers[4] = LinearWithLoRAMerged(mlp_lora.layers[4], rank=2, alpha=1.0)\n",
    "\n",
    "# Test the LoRA-enhanced MLP\n",
    "print(\"MLP with LoRA output:\", mlp_lora(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecff544",
   "metadata": {},
   "source": [
    "#### Interpretation of MLP with LoRA\n",
    "\n",
    "- Input used: *tensor([[-1.0206,  0.8164,  1.1792, -1.3017]])*  \n",
    "- Standard MLP output: *tensor([[-0.4673, -0.3002,  0.0748]])*  \n",
    "- MLP with LoRA output: *tensor([[-0.4673, -0.3002,  0.0748]])*\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "- The outputs are identical because LoRA matrices are initialized with zeros.  \n",
    "- This confirms that adding LoRA does **not affect the modelâ€™s initial predictions**.  \n",
    "- Once training starts, LoRA parameters will adapt to the new task while the original weights remain frozen, enabling **parameter-efficient fine-tuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbd962",
   "metadata": {},
   "source": [
    "### Exercise 6: Freezing the Original Linear Layers and Training LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b1340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters after freezing:\n",
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n"
     ]
    }
   ],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    \"\"\"\n",
    "    Recursively freeze parameters of all nn.Linear layers\n",
    "    inside the given model.\n",
    "    \"\"\"\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            # Recursively check child modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "# =========================================\n",
    "# Apply freezing to the MLP with LoRA\n",
    "# =========================================\n",
    "freeze_linear_layers(mlp_lora)\n",
    "\n",
    "# Check which parameters are trainable\n",
    "print(\"Trainable parameters after freezing:\")\n",
    "for name, param in mlp_lora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f92825d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.0680\n",
      "Epoch 2: Loss = 1.0657\n",
      "Epoch 3: Loss = 1.0634\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Mini training loop (dummy example)\n",
    "# =========================================\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy data (random)\n",
    "X_train = torch.randn(10, num_features)   # 10 samples\n",
    "y_train = torch.randint(0, num_classes, (10,))  # random labels\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, mlp_lora.parameters()), lr=0.01)\n",
    "\n",
    "# Training for a few epochs\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp_lora(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa73eb",
   "metadata": {},
   "source": [
    "#### Interpretation of Freezing and Training LoRA\n",
    "\n",
    "**Trainable parameters check:**\n",
    "\n",
    "- layers.0.linear.weight: False  \n",
    "- layers.0.linear.bias: False  \n",
    "- layers.0.lora.A: True  \n",
    "- layers.0.lora.B: True  \n",
    "- layers.2.linear.weight: False  \n",
    "- layers.2.linear.bias: False  \n",
    "- layers.2.lora.A: True  \n",
    "- layers.2.lora.B: True  \n",
    "- layers.4.linear.weight: False  \n",
    "- layers.4.linear.bias: False  \n",
    "- layers.4.lora.A: True  \n",
    "- layers.4.lora.B: True  \n",
    "\n",
    "Only the LoRA parameters (A and B) are trainable, while all original Linear weights are frozen.\n",
    "\n",
    "---\n",
    "\n",
    "**Mini training results (loss over 3 epochs):**\n",
    "\n",
    "- Epoch 1: Loss = 1.0680  \n",
    "- Epoch 2: Loss = 1.0657  \n",
    "- Epoch 3: Loss = 1.0634  \n",
    "\n",
    "The slight decrease in loss confirms that the network is learning using only the LoRA parameters.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "This setup allows efficient fine-tuning:  \n",
    "- Original model weights stay intact (pre-trained knowledge preserved).  \n",
    "- Only a small number of LoRA parameters are updated, reducing memory and compute costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaca5f",
   "metadata": {},
   "source": [
    "### LoRA Implementation on MNIST - Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8a9845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training standard MLP...\n",
      "Epoch 1/2 | Batch 0/938 | Loss: 2.3064\n",
      "Epoch 1/2 | Batch 400/938 | Loss: 0.1621\n",
      "Epoch 1/2 | Batch 800/938 | Loss: 0.1703\n",
      "Epoch 1/2 Training Accuracy: 94.98%\n",
      "Epoch 2/2 | Batch 0/938 | Loss: 0.1207\n",
      "Epoch 2/2 | Batch 400/938 | Loss: 0.2612\n",
      "Epoch 2/2 | Batch 800/938 | Loss: 0.2854\n",
      "Epoch 2/2 Training Accuracy: 96.71%\n",
      "Total Training Time: 0.49 min\n",
      "Test Accuracy (Standard MLP): 96.19%\n",
      "Test Accuracy (LoRA MLP before fine-tuning):\n",
      "96.19%\n",
      "Trainable parameters after freezing:\n",
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n",
      "Fine-tuning LoRA MLP...\n",
      "Epoch 1/2 | Batch 0/938 | Loss: 0.0400\n",
      "Epoch 1/2 | Batch 400/938 | Loss: 0.1347\n",
      "Epoch 1/2 | Batch 800/938 | Loss: 0.0904\n",
      "Epoch 1/2 Training Accuracy: 96.81%\n",
      "Epoch 2/2 | Batch 0/938 | Loss: 0.0806\n",
      "Epoch 2/2 | Batch 400/938 | Loss: 0.1236\n",
      "Epoch 2/2 | Batch 800/938 | Loss: 0.2125\n",
      "Epoch 2/2 Training Accuracy: 97.05%\n",
      "Total Training Time: 0.51 min\n",
      "Test Accuracy (LoRA Fine-tuned): 96.45%\n",
      "Test Accuracy (Original MLP): 96.19%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 1. LoRALayer\n",
    "# -----------------------------------------\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        \"\"\"\n",
    "        Implements low-rank adaptation using matrices A and B.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute (x @ A) @ B scaled by alpha\n",
    "        return self.alpha * (x @ self.A @ self.B)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2. LinearWithLoRA (non-merged version)\n",
    "# -----------------------------------------\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, rank: int, alpha: float):\n",
    "        \"\"\"\n",
    "        Adds a LoRA adaptation to a standard Linear layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. LinearWithLoRAMerged (merged weights)\n",
    "# -----------------------------------------\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, rank: int, alpha: float):\n",
    "        \"\"\"\n",
    "        Merges LoRA matrices with original weight for efficient inference.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Combine LoRA matrices\n",
    "        lora_matrix = self.lora.A @ self.lora.B\n",
    "        combined_weight = self.linear.weight + self.lora.alpha * lora_matrix.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. Multilayer Perceptron (MLP)\n",
    "# -----------------------------------------\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes):\n",
    "        \"\"\"\n",
    "        Standard 3-layer MLP: Linear -> ReLU -> Linear -> ReLU -> Linear\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5. Dataset and DataLoader\n",
    "# -----------------------------------------\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 6. Training and Evaluation Functions\n",
    "# -----------------------------------------\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.view(-1, 28*28).to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum().item()\n",
    "    return correct_pred / num_examples * 100\n",
    "\n",
    "def train(num_epochs, model, optimizer, train_loader, device):\n",
    "    start_time = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            features = features.view(-1, 28*28).to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward\n",
    "            logits = model(features)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if not batch_idx % 400:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss:.4f}\")\n",
    "\n",
    "        acc = compute_accuracy(model, train_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Training Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    print(f\"Total Training Time: {(time.time() - start_time)/60:.2f} min\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 7. Hyperparameters and Model Initialization\n",
    "# -----------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_features = 28 * 28\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2  # for demo, keep it low\n",
    "\n",
    "# Standard MLP\n",
    "model = MultilayerPerceptron(num_features, num_hidden_1, num_hidden_2, num_classes).to(DEVICE)\n",
    "optimizer_pretrained = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Training standard MLP...\")\n",
    "train(num_epochs, model, optimizer_pretrained, train_loader, DEVICE)\n",
    "print(f\"Test Accuracy (Standard MLP): {compute_accuracy(model, test_loader, DEVICE):.2f}%\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 8. Apply LoRA to MLP\n",
    "# -----------------------------------------\n",
    "model_lora = copy.deepcopy(model)\n",
    "\n",
    "# Replace all linear layers by LoRA merged\n",
    "model_lora.layers[0] = LinearWithLoRAMerged(model_lora.layers[0], rank=4, alpha=8)\n",
    "model_lora.layers[2] = LinearWithLoRAMerged(model_lora.layers[2], rank=4, alpha=8)\n",
    "model_lora.layers[4] = LinearWithLoRAMerged(model_lora.layers[4], rank=4, alpha=8)\n",
    "\n",
    "print(\"Test Accuracy (LoRA MLP before fine-tuning):\")\n",
    "print(f\"{compute_accuracy(model_lora, test_loader, DEVICE):.2f}%\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 9. Freeze original Linear weights\n",
    "# -----------------------------------------\n",
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(model_lora)\n",
    "\n",
    "# Check trainable params\n",
    "print(\"Trainable parameters after freezing:\")\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 10. Fine-tune LoRA\n",
    "# -----------------------------------------\n",
    "optimizer_lora = torch.optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=learning_rate)\n",
    "\n",
    "print(\"Fine-tuning LoRA MLP...\")\n",
    "train(num_epochs, model_lora, optimizer_lora, train_loader, DEVICE)\n",
    "print(f\"Test Accuracy (LoRA Fine-tuned): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%\")\n",
    "\n",
    "print(f\"Test Accuracy (Original MLP): {compute_accuracy(model, test_loader, DEVICE):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31397b",
   "metadata": {},
   "source": [
    "### LoRA Implementation on MNIST â€“ Final Results\n",
    "\n",
    "**Training standard MLP:**\n",
    "- Epoch 1 Training Accuracy: 94.98%\n",
    "- Epoch 2 Training Accuracy: 96.71%\n",
    "- Test Accuracy: **96.19%**\n",
    "\n",
    "**LoRA MLP before fine-tuning:**\n",
    "- Test Accuracy: **96.19%** (identical to standard MLP at initialization)\n",
    "\n",
    "**Trainable parameters after freezing:**\n",
    "- *linear.weight* and *linear.bias* â†’ **False** (frozen)  \n",
    "- *lora.A* and *lora.B* â†’ **True** (trainable)  \n",
    "\n",
    "**Fine-tuning LoRA:**\n",
    "- Epoch 1 Training Accuracy: 96.81%\n",
    "- Epoch 2 Training Accuracy: 97.05%\n",
    "- Test Accuracy: **96.45%**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "- LoRA integration preserves the original model behavior at start (no accuracy loss).  \n",
    "- Fine-tuning only LoRA parameters achieves comparable (even slightly higher) accuracy.  \n",
    "- This confirms **parameter-efficient fine-tuning**: only small matrices *A* and *B* are trained, saving memory and computation while keeping pre-trained weights intact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
