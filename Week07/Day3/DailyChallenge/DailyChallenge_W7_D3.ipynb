{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16016cf",
   "metadata": {},
   "source": [
    "## Daily Challenge: W7_D3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce20903",
   "metadata": {},
   "source": [
    "### Daily Challenge: Evaluating Large Language Models\n",
    "\n",
    "---\n",
    "\n",
    "#### What You’ll Learn\n",
    "\n",
    "- The importance of evaluating LLMs for performance, reliability, and safety.  \n",
    "- The challenges involved in LLM evaluation.  \n",
    "- An overview of different evaluation methods, including content overlap metrics, model-based metrics, human evaluation, and adversarial testing.  \n",
    "- In-depth understanding of BLEU, ROUGE, and Perplexity metrics.  \n",
    "- Critical thinking in choosing the right evaluation metric for different applications.  \n",
    "\n",
    "---\n",
    "\n",
    "#### What You Will Create\n",
    "\n",
    "You will complete exercises to:  \n",
    "- Apply BLEU and ROUGE scores to sample text  \n",
    "- Analyze perplexity scores  \n",
    "- Conduct adversarial testing  \n",
    "- Propose improvements for LLM evaluation methodologies  \n",
    "\n",
    "---\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Understanding LLM Evaluation**\n",
    "\n",
    "- Explain why evaluating LLMs is more complex than traditional software.  \n",
    "- Identify key reasons for evaluating an LLM’s safety.  \n",
    "- Describe how adversarial testing contributes to LLM improvement.  \n",
    "- Discuss the limitations of automated evaluation metrics and how they compare to human evaluation.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Applying BLEU and ROUGE Metrics**\n",
    "\n",
    "- Calculate the BLEU score for the following example:  \n",
    "\n",
    "**Reference:**  \n",
    "“Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.”  \n",
    "\n",
    "**Generated:**  \n",
    "“Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.”  \n",
    "\n",
    "---\n",
    "\n",
    "- Calculate the ROUGE score for the following example:  \n",
    "\n",
    "**Reference:**  \n",
    "“In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.”  \n",
    "\n",
    "**Generated:**  \n",
    "“To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.”  \n",
    "\n",
    "---\n",
    "\n",
    "- Provide an analysis of the limitations of BLEU and ROUGE when evaluating creative or context-sensitive text.  \n",
    "- Suggest improvements or alternative methods for evaluating text generation.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Perplexity Analysis**\n",
    "\n",
    "- Compare the perplexity of the two language models based on the probability assigned to a word:  \n",
    "\n",
    "**Model A:** Assigns 0.8 probability to “mitigation.”  \n",
    "**Model B:** Assigns 0.4 probability to “mitigation.”  \n",
    "\n",
    "- Determine which model has lower perplexity and explain why.  \n",
    "\n",
    "- Given a language model that has a perplexity score of 100, discuss its performance implications and possible ways to improve it.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Human Evaluation Exercise**\n",
    "\n",
    "- Rate the fluency of this chatbot response using a Likert scale (1-5):  \n",
    "\n",
    "**Response:**  \n",
    "“Apologies, but comprehend I do not. Could you rephrase your question?”  \n",
    "\n",
    "- Justify your rating.  \n",
    "- Propose an improved version of the response and explain why it is better.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Adversarial Testing Exercise**\n",
    "\n",
    "- Identify the potential mistake an LLM might make when answering the prompt:  \n",
    "\n",
    "**Prompt:** “What is the capitol of France?”  \n",
    "**Expected:** “Paris.”  \n",
    "\n",
    "- Suggest a method to improve robustness against such errors.  \n",
    "- Create at least three tricky prompts that could challenge an LLM’s robustness, bias detection, or factual accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Comparative Analysis of Evaluation Methods**\n",
    "\n",
    "- Choose an NLP task (e.g., machine translation, text summarization, question answering).  \n",
    "- Compare and contrast at least three different evaluation metrics (BLEU, ROUGE, BERTScore, Perplexity, Human Evaluation, etc.).  \n",
    "- Discuss which metric is most appropriate for the chosen task and why.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095582d",
   "metadata": {},
   "source": [
    "### 1. Understanding LLM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71cdc0c",
   "metadata": {},
   "source": [
    "#### 1. Why is evaluating LLMs more complex than traditional software?\n",
    "\n",
    "Evaluating LLMs is more complex because their outputs are probabilistic rather than deterministic. Traditional software produces fixed outputs for given inputs, while LLMs can generate different responses each time. Additionally, their performance depends on context, prompt phrasing, and training data, which makes standard testing harder.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Why is it important to evaluate an LLM’s safety?\n",
    "\n",
    "Safety evaluation ensures the model does not produce harmful, biased, or misleading outputs. LLMs can unintentionally spread misinformation, reinforce stereotypes, or generate unsafe instructions. Evaluating safety reduces these risks and builds trust in real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. How does adversarial testing contribute to LLM improvement?\n",
    "\n",
    "Adversarial testing involves intentionally crafting challenging prompts to expose weaknesses in the model. By analyzing where the model fails, developers can improve training data, fine-tuning strategies, and safety mechanisms to make the model more robust.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. What are the limitations of automated evaluation metrics compared to human evaluation?\n",
    "\n",
    "Automated metrics like BLEU or ROUGE measure surface similarity but do not capture meaning, creativity, or factual correctness. Human evaluation, while time-consuming, can assess fluency, relevance, and nuanced understanding better than automated scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f0eae",
   "metadata": {},
   "source": [
    "### 2. Applying BLEU and ROUGE Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dca7da",
   "metadata": {},
   "source": [
    "We will first calculate the BLEU score for a sample reference and generated sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb7cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.06296221772093169\n"
     ]
    }
   ],
   "source": [
    "# --- BLEU Score Calculation ---\n",
    "from rouge_score import rouge_scorer\n",
    "import math\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Reference and candidate sentences\n",
    "reference = \"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\".split()\n",
    "candidate = \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\".split()\n",
    "\n",
    "# Create smoothing function (to avoid zero scores for 4-gram)\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# Calculate BLEU score with smoothing\n",
    "bleu_score = sentence_bleu([reference], candidate, smoothing_function=smooth)\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3f624",
   "metadata": {},
   "source": [
    "#### BLEU Score Interpretation\n",
    "\n",
    "The BLEU score is 0.06, which is very low. This means that there is little overlap between the generated sentence and the reference sentence in terms of exact words or word sequences. \n",
    "\n",
    "However, this low score does not necessarily mean the generated text is wrong — it might still convey the same meaning using synonyms or different phrasing. This highlights a limitation of BLEU: it focuses on exact n-gram matches and does not capture semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861be618",
   "metadata": {},
   "source": [
    "To complement BLEU, we will now calculate ROUGE scores. ROUGE focuses more on recall and overlap of key words, which can be more informative for tasks like summarization or when paraphrasing is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dae243",
   "metadata": {},
   "source": [
    "#### ROUGE Score Calculation\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures how much of the reference text’s key content appears in the generated text. \n",
    "\n",
    "We will calculate ROUGE-1 (unigrams) and ROUGE-L (longest common subsequence) for the same sentences used in the BLEU example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f348cdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: Score(precision=0.3333333333333333, recall=0.3, fmeasure=0.3157894736842105)\n",
      "ROUGE-L: Score(precision=0.3333333333333333, recall=0.3, fmeasure=0.3157894736842105)\n"
     ]
    }
   ],
   "source": [
    "# --- ROUGE Score Calculation ---\n",
    "\n",
    "# Initialize ROUGE scorer for ROUGE-1 and ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate scores\n",
    "scores = scorer.score(\n",
    "    \"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\",\n",
    "    \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\"\n",
    ")\n",
    "\n",
    "# Display the scores\n",
    "print(\"ROUGE-1:\", scores['rouge1'])\n",
    "print(\"ROUGE-L:\", scores['rougeL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8cfc2c",
   "metadata": {},
   "source": [
    "#### ROUGE Score Interpretation\n",
    "\n",
    "The ROUGE-1 and ROUGE-L scores are around 0.3. This means that about 30% of the key words or sequences from the reference appear in the generated text. \n",
    "\n",
    "This score is higher than BLEU (0.06), showing that ROUGE is more forgiving when synonyms or different phrasing are used. ROUGE focuses more on recall — it checks how much of the important content from the reference is present in the generated text, even if the wording is different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188c629",
   "metadata": {},
   "source": [
    "#### Limitations of BLEU and ROUGE\n",
    "\n",
    "BLEU and ROUGE focus on surface-level similarity, comparing exact words or n-grams between the generated text and the reference. This creates several limitations:\n",
    "\n",
    "- **Lack of semantic understanding**: They do not capture meaning or synonyms, so paraphrases may score poorly even if correct.\n",
    "- **Insensitive to creativity**: In tasks like story generation or open-ended responses, exact matches are less relevant.\n",
    "- **Context blindness**: These metrics ignore long-range coherence, factual accuracy, or tone.\n",
    "- **Single-reference bias**: If only one reference is used, alternative valid outputs are unfairly penalized.\n",
    "\n",
    "---\n",
    "\n",
    "#### Suggested Improvements or Alternatives\n",
    "\n",
    "- **BERTScore**: Uses contextual embeddings from BERT to measure semantic similarity rather than exact matches.\n",
    "- **Human evaluation**: Involving humans to rate fluency, coherence, and relevance captures qualitative aspects.\n",
    "- **Task-specific metrics**: For summarization, metrics like QAEval (question-answer evaluation) or factual consistency checks can be better.\n",
    "- **Combined evaluation**: Use automated metrics for speed and human checks for nuanced quality assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6bd24",
   "metadata": {},
   "source": [
    "#### Comparison of BLEU, ROUGE, and Alternatives\n",
    "\n",
    "| Metric       | What it Measures                | Strengths                                  | Weaknesses                                     |\n",
    "|--------------|---------------------------------|--------------------------------------------|------------------------------------------------|\n",
    "| **BLEU**     | n-gram precision                | Simple, widely used for MT evaluation      | Ignores meaning, penalizes synonyms, context-blind |\n",
    "| **ROUGE**    | n-gram recall & overlap         | Good for summarization, captures coverage  | Still surface-level, ignores semantics and coherence |\n",
    "| **BERTScore**| Semantic similarity (embeddings)| Captures meaning and synonyms, context-aware| Requires large models, slower than BLEU/ROUGE |\n",
    "| **Human Eval**| Fluency, coherence, relevance  | Captures nuanced quality, creativity       | Time-consuming, subjective, not scalable       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b844b88",
   "metadata": {},
   "source": [
    "### 3. Perplexity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5dcc0d",
   "metadata": {},
   "source": [
    "Perplexity measures how well a language model predicts a given sequence of words.  \n",
    "- Lower perplexity = the model is more confident and better at predicting the sequence.  \n",
    "- Higher perplexity = the model is uncertain or poorly predicts the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb4d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (Model A): 1.25\n",
      "Perplexity (Model B): 2.5\n",
      "Model A has lower perplexity and is better at predicting this word.\n"
     ]
    }
   ],
   "source": [
    "# --- Perplexity Comparison ---\n",
    "\n",
    "# Probabilities given by two models for the same word\n",
    "p_model_A = 0.8\n",
    "p_model_B = 0.4\n",
    "\n",
    "# Perplexity formula: 2^(-log2(p))\n",
    "perplexity_A = 1 / p_model_A\n",
    "perplexity_B = 1 / p_model_B\n",
    "\n",
    "print(\"Perplexity (Model A):\", perplexity_A)\n",
    "print(\"Perplexity (Model B):\", perplexity_B)\n",
    "\n",
    "# Lower perplexity = better\n",
    "if perplexity_A < perplexity_B:\n",
    "    print(\"Model A has lower perplexity and is better at predicting this word.\")\n",
    "else:\n",
    "    print(\"Model B has lower perplexity and is better at predicting this word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba79185",
   "metadata": {},
   "source": [
    "#### Perplexity Interpretation\n",
    "\n",
    "Model A has a lower perplexity (1.25) compared to Model B (2.5).  \n",
    "This means Model A is more confident and better at predicting the target word.  \n",
    "\n",
    "In general:\n",
    "- Lower perplexity = better model performance\n",
    "- Higher perplexity = more uncertainty in predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e6749e",
   "metadata": {},
   "source": [
    "#### Given a language model with perplexity score of 100, what does this imply?\n",
    "\n",
    "A perplexity of 100 means the model is quite uncertain when predicting words — it considers about 100 equally likely options at each step.  \n",
    "To improve this score, one could:\n",
    "- Train on more relevant data\n",
    "- Fine-tune the model for the specific domain\n",
    "- Use larger models with better contextual understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281b948",
   "metadata": {},
   "source": [
    "### 4. Human Evaluation Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991f744",
   "metadata": {},
   "source": [
    "**Prompt**\n",
    "Rate the fluency of this chatbot response using a Likert scale (1–5):\n",
    "\n",
    "**Response:**  \n",
    "\"Apologies, but comprehend I do not. Could you rephrase your question?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c708ff4",
   "metadata": {},
   "source": [
    "### Fluency Rating\n",
    "\n",
    "**Rating:** 2/5\n",
    "\n",
    "**Reasoning:**  \n",
    "The response is grammatically unusual and awkward (\"comprehend I do not\") which affects clarity and fluency.  \n",
    "It sounds unnatural for a chatbot and may confuse users.\n",
    "\n",
    "---\n",
    "\n",
    "### Improved Version\n",
    "\n",
    "\"Sorry, I don’t quite understand. Could you please rephrase your question?\"\n",
    "\n",
    "**Why is it better?**  \n",
    "This version is polite, clear, and uses natural phrasing, making it easier for users to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94376d55",
   "metadata": {},
   "source": [
    "### 5. Adversarial Testing Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13820d",
   "metadata": {},
   "source": [
    "**Prompt:** \n",
    "\"What is the capitol of France?\"\n",
    "\n",
    "**Expected Answer:**  \n",
    "\"Paris\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e22af9",
   "metadata": {},
   "source": [
    "#### Potential Mistake\n",
    "\n",
    "The model might misinterpret the word \"capitol\" (a building) instead of \"capital\" (a city) and give an irrelevant answer, or confuse it with another location.\n",
    "\n",
    "---\n",
    "\n",
    "#### Method to Improve Robustness\n",
    "\n",
    "- Use adversarial training: expose the model to misspellings, homonyms, and tricky prompts during training.\n",
    "- Implement spelling/grammar normalization before processing user queries.\n",
    "- Add validation layers to detect when the question is ambiguous and request clarification.\n",
    "\n",
    "---\n",
    "\n",
    "#### Three Tricky Prompts for Robustness Testing\n",
    "\n",
    "1. \"What’s the capital of the country where Mount Fuji is located?\"\n",
    "2. \"Which city is both the capital of England and home to Big Ben?\"\n",
    "3. \"Name the capital of the country whose flag is red and white with a maple leaf.\"\n",
    "\n",
    "*(Answers expected: Tokyo, London, Ottawa)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a6289",
   "metadata": {},
   "source": [
    "### 6. Comparative Analysis of Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce00be4",
   "metadata": {},
   "source": [
    "#### Chosen NLP Task\n",
    "**Text Summarization**\n",
    "\n",
    "Summarization requires capturing the key meaning of the original text rather than exact wording. Therefore, metrics that focus only on word overlap may be insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005cea1",
   "metadata": {},
   "source": [
    "### Comparison of Metrics\n",
    "\n",
    "| Metric       | Measures                    | Strengths                                  | Weaknesses                                      |\n",
    "|--------------|-----------------------------|--------------------------------------------|------------------------------------------------|\n",
    "| **BLEU**     | n-gram precision            | Good for translation; easy to compute      | Ignores meaning; bad for paraphrasing          |\n",
    "| **ROUGE**    | n-gram recall & overlap     | Good for summarization; measures coverage  | Surface-level; does not capture semantic similarity |\n",
    "| **BERTScore**| Semantic similarity (contextual embeddings) | Captures meaning, synonyms, context        | Slower; requires large models                   |\n",
    "| **Human Eval**| Fluency, relevance, coherence| Best for nuanced judgment                  | Time-consuming, subjective, not scalable        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed2f8d",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "For text summarization, ROUGE is widely used because it measures how much important content from the reference appears in the summary (recall).  \n",
    "However, it fails to capture semantic meaning when synonyms or paraphrases are used.\n",
    "\n",
    "BERTScore is more appropriate when we want to evaluate meaning rather than exact word matches, especially for creative or abstractive summaries.  \n",
    "In practice, a combination of ROUGE (for coverage) and BERTScore or human evaluation (for quality and fluency) provides the most reliable assessment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
