{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises XP: W8_D4"
      ],
      "metadata": {
        "id": "qfJnsGn0V_9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading & Analyzing a Scientific Paper (RAG + LLM)"
      ],
      "metadata": {
        "id": "aZ2lmKvOV_3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What You'll Learn\n",
        "- Interpret scientific articles with purpose and extract structure quickly  \n",
        "- Critically analyze methods and experimental design  \n",
        "- Identify/assess evaluation metrics and quality of evidence  \n",
        "- Practice Cornell-style notes to retain key takeaways  \n",
        "- Summarize with the 5W1H technique  \n",
        "- Reflect on a practical RAG + LLM design for a real-world use case\n",
        "\n",
        "## Deliverables\n",
        "1. Article structure map (sections + 1-2 sentence purpose each)  \n",
        "2. Critical analysis of experimental design (6 guiding questions)  \n",
        "3. Evaluation metrics & evidence critique  \n",
        "4. Cornell notes of one technical subsection  \n",
        "5. 5W1H summary + 4-sentence abstract  \n",
        "6. Design reflection for your own RAG + LLM scenario\n",
        "\n",
        "**Paper:** *A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture* (PDF provided).\n"
      ],
      "metadata": {
        "id": "ZEzIGcUfV_0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 — Article Structure Mapping"
      ],
      "metadata": {
        "id": "ppBSBxTiV_x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Upload] Upload the PDF file to Colab\n",
        "# Comment: Use Colab's file upload widget to import the PDF and get its path.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Comment: Extract the file path from the uploaded dictionary\n",
        "PDF_PATH = list(uploaded.keys())[0]\n",
        "print(f\"PDF uploaded: {PDF_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "krWeXxl_XmHR",
        "outputId": "6d4013d7-eaf2-4189-9003-cd40725e7c9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-14408354-3a88-432f-b67b-9befc4c72e04\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-14408354-3a88-432f-b67b-9befc4c72e04\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture.pdf to A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture (1).pdf\n",
            "PDF uploaded: A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Setup] Install and import PDF tools\n",
        "# Comment: Install PyMuPDF for PDF reading, and import needed libraries.\n",
        "!pip -q install pymupdf pandas\n",
        "\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Comment: Update this path if needed. Your file was uploaded to /mnt/data/\n",
        "from pathlib import Path\n",
        "PDF_PATH = Path(PDF_PATH)\n",
        "assert PDF_PATH.exists(), f\"PDF not found at: {PDF_PATH}\"\n"
      ],
      "metadata": {
        "id": "JwyCn0pQX4Dv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Helper v2] Stricter section heading detection (line-start + Roman numerals)\n",
        "# Comment: This version reduces false positives by matching headings at the start of lines,\n",
        "# Comment: optionally prefixed with Roman numerals like \"I.\", \"II.\", etc.\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import fitz\n",
        "from pathlib import Path\n",
        "\n",
        "ROMAN = r\"(?:[IVXLCDM]+\\.\\s*)?\"  # optional roman numeral like \"I. \"\n",
        "# Comment: Patterns enforce line-start (^) and typical heading words\n",
        "# [Helper v3] Match numbered headings only to avoid false hits in abstracts\n",
        "STRICT_SECTION_PATTERNS = [\n",
        "    r\"^I\\.\\s*Introduction\\b\",\n",
        "    r\"^II\\.\\s*(Related Work|Background)\\b\",\n",
        "    r\"^III\\.\\s*(Method|Methods|Methodology)\\b\",\n",
        "    r\"^IV\\.\\s*(Experiment|Experiments|Results|Evaluation)\\b\",\n",
        "    r\"^V\\.\\s*(Conclusion|Conclusions|Discussion|Conclusion and Future Work)\\b\",\n",
        "]\n",
        "SECTION_LABELS = [\n",
        "    \"Introduction\",\n",
        "    \"Related Work / Background\",\n",
        "    \"Methods\",\n",
        "    \"Experiment / Results\",\n",
        "    \"Conclusion / Discussion\",\n",
        "]\n",
        "\n",
        "def extract_pages_lines(pdf_path: Path):\n",
        "    # Comment: Return page texts split by lines for line-anchored regex\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = []\n",
        "    for i in range(len(doc)):\n",
        "        text = doc[i].get_text(\"text\")\n",
        "        # Normalize line endings and trim trailing spaces\n",
        "        lines = [ln.strip() for ln in text.splitlines()]\n",
        "        pages.append(lines)\n",
        "    doc.close()\n",
        "    return pages\n",
        "\n",
        "def find_sections_strict(pages_lines):\n",
        "    rows = []\n",
        "    for label, pattern in zip(SECTION_LABELS, STRICT_SECTION_PATTERNS):\n",
        "        found_page = None\n",
        "        snippet = \"\"\n",
        "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
        "        for idx, lines in enumerate(pages_lines):\n",
        "            for li, line in enumerate(lines):\n",
        "                if regex.search(line):\n",
        "                    found_page = idx + 1  # 1-based page\n",
        "                    # Build a small preview (the line + next few lines)\n",
        "                    context = lines[li : li + 3]\n",
        "                    snippet = \" | \".join(context)\n",
        "                    break\n",
        "            if found_page:\n",
        "                break\n",
        "        rows.append({\"section\": label, \"start_page\": found_page, \"preview\": snippet})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Comment: If you already defined PDF_PATH earlier via the upload cell, we reuse it.\n",
        "PDF_PATH = Path(PDF_PATH)\n",
        "pages_lines = extract_pages_lines(PDF_PATH)\n",
        "df_sections_strict = find_sections_strict(pages_lines)\n",
        "df_sections_strict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qTsbx4PRYzMl",
        "outputId": "998f3a39-34ba-4606-f2a1-8eb8547c863c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     section  start_page  \\\n",
              "0               Introduction           2   \n",
              "1  Related Work / Background           4   \n",
              "2                    Methods          19   \n",
              "3       Experiment / Results          20   \n",
              "4    Conclusion / Discussion          26   \n",
              "\n",
              "                                             preview  \n",
              "0  I. Introduction | Recent developments in gener...  \n",
              "1  II. Related Work | For the purpose of this stu...  \n",
              "2  III. Methods | In this chapter, we outline a c...  \n",
              "3  IV. Experiment | In this chapter, the generati...  \n",
              "4  V. Conclusion and Discussion | In this study, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83a70fc2-22ee-4201-a9a8-9e5bae148ffa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>section</th>\n",
              "      <th>start_page</th>\n",
              "      <th>preview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Introduction</td>\n",
              "      <td>2</td>\n",
              "      <td>I. Introduction | Recent developments in gener...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Related Work / Background</td>\n",
              "      <td>4</td>\n",
              "      <td>II. Related Work | For the purpose of this stu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Methods</td>\n",
              "      <td>19</td>\n",
              "      <td>III. Methods | In this chapter, we outline a c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Experiment / Results</td>\n",
              "      <td>20</td>\n",
              "      <td>IV. Experiment | In this chapter, the generati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Conclusion / Discussion</td>\n",
              "      <td>26</td>\n",
              "      <td>V. Conclusion and Discussion | In this study, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83a70fc2-22ee-4201-a9a8-9e5bae148ffa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-83a70fc2-22ee-4201-a9a8-9e5bae148ffa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-83a70fc2-22ee-4201-a9a8-9e5bae148ffa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-104d68dd-4a4b-494f-91e5-ba9c2def5e5b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-104d68dd-4a4b-494f-91e5-ba9c2def5e5b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-104d68dd-4a4b-494f-91e5-ba9c2def5e5b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a8655b0f-186d-4ac5-9022-17113ec8b57d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_sections_strict')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a8655b0f-186d-4ac5-9022-17113ec8b57d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_sections_strict');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_sections_strict",
              "summary": "{\n  \"name\": \"df_sections_strict\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Related Work / Background\",\n          \"Conclusion / Discussion\",\n          \"Methods\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_page\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 2,\n        \"max\": 26,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          26,\n          19\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preview\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"II. Related Work | For the purpose of this study, an extensive investigation into recent significant research papers, journals, articles, | and books related to generative AI and LLM has been conducted. This chapter delves into a comprehensive\",\n          \"V. Conclusion and Discussion | In this study, we presented methods and implementation cases for developing generative AI services using LLM | application architecture, aiming to explore avenues for advancing the development and industrial utilization of\",\n          \"III. Methods | In this chapter, we outline a comprehensive framework for implementing generative AI services by effectively | combining and orchestrating various technologies within the previously discussed generative AI technology stack. We\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For each section, write 1-2 sentences summarizing its purpose"
      ],
      "metadata": {
        "id": "NREfGLLhaxY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Extract Sections] Pull raw text for each section by page range\n",
        "# Comment: We define the section start pages found earlier and slice text until the next section.\n",
        "\n",
        "import fitz\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'PDF_PATH' in globals(), \"Please run the Upload cell first to define PDF_PATH.\"\n",
        "PDF_PATH = Path(PDF_PATH)\n",
        "\n",
        "SECTION_STARTS = {\n",
        "    \"Introduction\": 2,\n",
        "    \"Related Work / Background\": 4,\n",
        "    \"Methods\": 19,\n",
        "    \"Experiment / Results\": 20,\n",
        "    \"Conclusion / Discussion\": 26,\n",
        "}\n",
        "\n",
        "# Comment: Helper to extract page texts as a list (1-based page semantics when slicing below).\n",
        "doc = fitz.open(PDF_PATH)\n",
        "pages_full_text = [doc[i].get_text(\"text\") for i in range(len(doc))]\n",
        "doc.close()\n",
        "\n",
        "def section_text(pages, start_page, end_page_exclusive):\n",
        "    \"\"\"Return plain text from start_page up to the page before end_page_exclusive (1-based).\"\"\"\n",
        "    s = start_page - 1\n",
        "    e = max(start_page, end_page_exclusive) - 1\n",
        "    return \"\\n\".join(pages[s:e])\n",
        "\n",
        "# Comment: Build a dict of section -> raw text\n",
        "sorted_sections = list(SECTION_STARTS.items())\n",
        "sorted_sections.sort(key=lambda kv: kv[1])  # sort by page number\n",
        "\n",
        "section_texts = {}\n",
        "for i, (name, start) in enumerate(sorted_sections):\n",
        "    end = sorted_sections[i+1][1] if i+1 < len(sorted_sections) else len(pages_full_text)+1\n",
        "    section_texts[name] = section_text(pages_full_text, start, end)\n",
        "\n",
        "list(section_texts.keys()), [len(t) for t in section_texts.values()]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GBMPsFMaXzf",
        "outputId": "956d5412-dc8d-4152-91c0-bb71801a8e60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Introduction',\n",
              "  'Related Work / Background',\n",
              "  'Methods',\n",
              "  'Experiment / Results',\n",
              "  'Conclusion / Discussion'],\n",
              " [9122, 44615, 2181, 11079, 8614])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Summarize] Create 1–2 sentence purpose summaries using a simple heuristic\n",
        "# Comment: We take the first meaningful paragraph and keep up to two sentences.\n",
        "\n",
        "import re\n",
        "from textwrap import shorten\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    \"\"\"Normalize whitespace and remove overly long runs of spaces/newlines.\"\"\"\n",
        "    t = t.replace(\"\\xa0\", \" \")\n",
        "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
        "    t = re.sub(r\"\\n{2,}\", \"\\n\\n\", t.strip())\n",
        "    return t\n",
        "\n",
        "def first_paragraph(t: str) -> str:\n",
        "    \"\"\"Return the first non-trivial paragraph (>= 200 chars after cleaning).\"\"\"\n",
        "    paras = [p.strip() for p in t.split(\"\\n\\n\") if p.strip()]\n",
        "    for p in paras:\n",
        "        if len(p) >= 200:\n",
        "            return p\n",
        "    return paras[0] if paras else \"\"\n",
        "\n",
        "def two_sentence_summary(p: str) -> str:\n",
        "    \"\"\"Keep up to two sentences from the paragraph.\"\"\"\n",
        "    # Simple sentence split; handle ., ?, !\n",
        "    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", p)\n",
        "    # Filter tiny fragments and headings\n",
        "    parts = [s.strip() for s in parts if len(s.strip()) > 0]\n",
        "    if not parts:\n",
        "        return \"\"\n",
        "    # Keep up to two sentences, and trim overly long output for safety\n",
        "    summary = \" \".join(parts[:2])\n",
        "    return shorten(summary, width=800, placeholder=\"...\")\n",
        "\n",
        "# Comment: Build 1–2 sentence purpose per section\n",
        "section_purposes = {}\n",
        "for name, txt in section_texts.items():\n",
        "    p = first_paragraph(clean_text(txt))\n",
        "    section_purposes[name] = two_sentence_summary(p)\n",
        "\n",
        "section_purposes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgrV1fmUajh-",
        "outputId": "95585cd2-088d-4acb-93ce-83d01a4982dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Introduction': '2 I. Introduction Recent developments in generative AI, catalyzed by ChatGPT, have become a focal point of discussion.',\n",
              " 'Related Work / Background': '4 In Chapter 2, the theoretical foundation is established by detailing prior research and key related concepts pertinent to this study. The exploration encompasses the trends in generative AI and LLM on both domestic and international fronts.',\n",
              " 'Methods': '19 III. Methods In this chapter, we outline a comprehensive framework for implementing generative AI services by effectively combining and orchestrating various technologies within the previously discussed generative AI technology stack.',\n",
              " 'Experiment / Results': '20 3.2.1 RAG based implementation procedure The RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented generative model used to retrieve and generate responses based on information relevant to given questions or topics. Each step follows the procedure outlined in Fig 9.',\n",
              " 'Conclusion / Discussion': '26 Fig 19 Youtube Data Processing (VectorDB - FAISS) 4.2.8 Summary of implementation cases and expected effects The provided implementations using Python, LangChain, OpenAI API, and the open-source ChromaDB offer a meaningful demonstration of how to easily integrate generative AI into business operations using LLM. Additionally, transitioning from a development environment to a production environment raises several key considerations and areas for improvement.'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [IMRaD Check] Decide overall format and what (if anything) is missing"
      ],
      "metadata": {
        "id": "bc_VtKWya0o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [IMRaD Check] Decide overall format and what (if anything) is missing\n",
        "# Comment: We check which canonical IMRaD blocks are present based on our keys.\n",
        "\n",
        "present = set(section_texts.keys())\n",
        "\n",
        "has_intro = any(\"Introduction\" in k for k in present)\n",
        "has_methods = any(\"Methods\" in k for k in present)\n",
        "has_results = any(\"Experiment\" in k or \"Results\" in k or \"Evaluation\" in k for k in present)\n",
        "has_discussion = any(\"Conclusion\" in k or \"Discussion\" in k for k in present)\n",
        "has_related = any(\"Related\" in k or \"Background\" in k for k in present)\n",
        "\n",
        "if has_intro and has_methods and has_results and has_discussion:\n",
        "    overall_format = \"IMRaD-like with an explicit Related Work section.\"\n",
        "else:\n",
        "    overall_format = \"Partially IMRaD; some components may be merged or implicit.\"\n",
        "\n",
        "missing_notes = []\n",
        "if not has_related:\n",
        "    missing_notes.append(\"Related Work is not explicit.\")\n",
        "# Often Discussion is merged into Conclusion in such papers:\n",
        "if has_discussion and any(\"Conclusion\" in k for k in present) and not any(\"Discussion\" in k for k in present):\n",
        "    missing_notes.append(\"Discussion appears merged into Conclusion.\")\n",
        "if not has_results:\n",
        "    missing_notes.append(\"Results/Experiment section is not clearly separated.\")\n",
        "if not missing_notes:\n",
        "    missing_notes.append(\"No major structural gaps; minor merges are typical (e.g., Discussion in Conclusion).\")\n",
        "\n",
        "overall_format, missing_notes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht95Hn6qa07i",
        "outputId": "f148aaa1-d72f-4f26-8bee-64a91a691100"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IMRaD-like with an explicit Related Work section.',\n",
              " ['No major structural gaps; minor merges are typical (e.g., Discussion in Conclusion).'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Markdown Answer] Print a ready-to-paste Markdown block for Exercise 1 (Q3 & Q4)\n",
        "# Comment: We inject the start pages we already know and the auto-summaries.\n",
        "\n",
        "md_lines = []\n",
        "md_lines.append(\"## Exercise 1: Article Structure Mapping — Answers (Q3 & Q4)\")\n",
        "md_lines.append(\"\")\n",
        "md_lines.append(\"### Sections (start page + 1–2 sentence purpose each)\")\n",
        "md_lines.append(\"\")\n",
        "\n",
        "ordering = [\n",
        "    (\"Introduction\", 2),\n",
        "    (\"Related Work / Background\", 4),\n",
        "    (\"Methods\", 19),\n",
        "    (\"Experiment / Results\", 20),\n",
        "    (\"Conclusion / Discussion\", 26),\n",
        "]\n",
        "\n",
        "for name, page in ordering:\n",
        "    purpose = section_purposes.get(name, \"\").strip()\n",
        "    if not purpose:\n",
        "        purpose = \"_(Short section; summary requires brief manual reading.)_\"\n",
        "    md_lines.append(f\"- **{name}** — page: {page}  \")\n",
        "    md_lines.append(f\"  Purpose: {purpose}\")\n",
        "    md_lines.append(\"\")\n",
        "\n",
        "md_lines.append(\"### Overall Format\")\n",
        "md_lines.append(f\"- Format: {overall_format}\")\n",
        "md_lines.append(f\"- Notes: {' '.join(missing_notes)}\")\n",
        "\n",
        "md_block = \"\\n\".join(md_lines)\n",
        "print(md_block)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DLubc9TbEYc",
        "outputId": "5a6b79bb-d8a9-4548-825e-bd82baa7934b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Exercise 1: Article Structure Mapping — Answers (Q3 & Q4)\n",
            "\n",
            "### Sections (start page + 1–2 sentence purpose each)\n",
            "\n",
            "- **Introduction** — page: 2  \n",
            "  Purpose: 2 I. Introduction Recent developments in generative AI, catalyzed by ChatGPT, have become a focal point of discussion.\n",
            "\n",
            "- **Related Work / Background** — page: 4  \n",
            "  Purpose: 4 In Chapter 2, the theoretical foundation is established by detailing prior research and key related concepts pertinent to this study. The exploration encompasses the trends in generative AI and LLM on both domestic and international fronts.\n",
            "\n",
            "- **Methods** — page: 19  \n",
            "  Purpose: 19 III. Methods In this chapter, we outline a comprehensive framework for implementing generative AI services by effectively combining and orchestrating various technologies within the previously discussed generative AI technology stack.\n",
            "\n",
            "- **Experiment / Results** — page: 20  \n",
            "  Purpose: 20 3.2.1 RAG based implementation procedure The RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented generative model used to retrieve and generate responses based on information relevant to given questions or topics. Each step follows the procedure outlined in Fig 9.\n",
            "\n",
            "- **Conclusion / Discussion** — page: 26  \n",
            "  Purpose: 26 Fig 19 Youtube Data Processing (VectorDB - FAISS) 4.2.8 Summary of implementation cases and expected effects The provided implementations using Python, LangChain, OpenAI API, and the open-source ChromaDB offer a meaningful demonstration of how to easily integrate generative AI into business operations using LLM. Additionally, transitioning from a development environment to a production environment raises several key considerations and areas for improvement.\n",
            "\n",
            "### Overall Format\n",
            "- Format: IMRaD-like with an explicit Related Work section.\n",
            "- Notes: No major structural gaps; minor merges are typical (e.g., Discussion in Conclusion).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Critical Analysis of Experimental Design"
      ],
      "metadata": {
        "id": "fyTRc9p8cSJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Extract Sections] Methods & Experiment raw text\n",
        "# Comment: Slice the PDF text using the page numbers we discovered.\n",
        "import fitz\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'PDF_PATH' in globals(), \"Run the Upload cell first to define PDF_PATH.\"\n",
        "PDF_PATH = Path(PDF_PATH)\n",
        "\n",
        "METHODS_START = 19\n",
        "EXPERIMENT_START = 20\n",
        "CONCLUSION_START = 26  # end bound for Experiment\n",
        "\n",
        "doc = fitz.open(PDF_PATH)\n",
        "pages_full_text = [doc[i].get_text(\"text\") for i in range(len(doc))]\n",
        "doc.close()\n",
        "\n",
        "def extract_range(pages, start_page, end_page_exclusive):\n",
        "    s = start_page - 1\n",
        "    e = end_page_exclusive - 1\n",
        "    return \"\\n\".join(pages[s:e])\n",
        "\n",
        "methods_txt = extract_range(pages_full_text, METHODS_START, EXPERIMENT_START)\n",
        "experiment_txt = extract_range(pages_full_text, EXPERIMENT_START, CONCLUSION_START)\n",
        "\n",
        "print(\"Methods length:\", len(methods_txt), \"| Experiment length:\", len(experiment_txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEE4wKEacjJN",
        "outputId": "6f46f929-f78d-43c4-c6c7-e3e2ab1d6a16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Methods length: 2181 | Experiment length: 11079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Heuristics] Utility cleaners and sentence splitter\n",
        "# Comment: Basic cleanup and sentence split for keyword mining.\n",
        "import re\n",
        "\n",
        "def clean(t: str) -> str:\n",
        "    t = t.replace(\"\\xa0\", \" \")\n",
        "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
        "    t = re.sub(r\"\\n{2,}\", \"\\n\\n\", t.strip())\n",
        "    return t\n",
        "\n",
        "def split_sentences(t: str):\n",
        "    # Simple sentence split by punctuation; not perfect but OK for technical text.\n",
        "    t = clean(t)\n",
        "    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", t)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "methods_sents = split_sentences(methods_txt)\n",
        "exp_sents = split_sentences(experiment_txt)\n",
        "\n",
        "len(methods_sents), len(exp_sents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxW7jJxIcoBI",
        "outputId": "53388afa-522a-4aa8-98ef-366a5bc4ce27"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, 73)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Mining] Extract candidate answers by keyword patterns\n",
        "# Comment: We search sentences for cues to each question and select top candidates.\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def find_sentences(sents, patterns, top=3):\n",
        "    \"\"\"Return up to 'top' sentences that match any regex in patterns.\"\"\"\n",
        "    out = []\n",
        "    regs = [re.compile(p, flags=re.I) for p in patterns]\n",
        "    for s in sents:\n",
        "        if any(r.search(s) for r in regs):\n",
        "            out.append(s)\n",
        "        if len(out) >= top:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "# 1) Research question cues\n",
        "rq_patterns = [\n",
        "    r\"\\b(this study|we (aim|seek|investigate|study)|our goal|research question|we propose|we design)\\b\"\n",
        "]\n",
        "\n",
        "# 2) Type of study cues\n",
        "type_patterns = [\n",
        "    r\"\\b(case study|implementation case|prototype|experimental|experiment|comparative|evaluation|deployment)\\b\"\n",
        "]\n",
        "\n",
        "# 3) Variables cues (independent/dependent)\n",
        "indep_patterns = [\n",
        "    r\"\\b(we vary|we configured|we set|we adjust|parameter|top[- ]?k|chunk size|embedding model|retrieval|index|vector db|prompt|temperature|context window)\\b\"\n",
        "]\n",
        "dep_patterns = [\n",
        "    r\"\\b(accuracy|precision|recall|latency|throughput|response time|cost|quality|relevance|hit rate|success rate|user (study|feedback)|evaluation metric)\\b\"\n",
        "]\n",
        "\n",
        "# 4) Datasets & tools cues\n",
        "data_tools_patterns = [\n",
        "    r\"\\b(dataset|corpus|enterprise data|knowledge base|documents|pdf|csv|log|\\bETL\\b)\\b\",\n",
        "    r\"\\b(Chroma|FAISS|Weaviate|Pinecone|Milvus|LangChain|LlamaIndex|Azure OpenAI|OpenAI|Hugging Face|PostgreSQL|Elastic|Docker|Kubernetes|Ray|Airflow|GCS|S3)\\b\"\n",
        "]\n",
        "\n",
        "# 5) Control/baseline cues\n",
        "baseline_patterns = [\n",
        "    r\"\\b(baseline|compared to|versus|vs\\.|control|ablation|alternative)\\b\"\n",
        "]\n",
        "\n",
        "# 6) Repeatability/transparency cues\n",
        "repeat_patterns = [\n",
        "    r\"\\b(reproducible|repeatable|we release|open[- ]source|code|parameters|configuration|implementation details|pipeline|procedure|steps)\\b\"\n",
        "]\n",
        "\n",
        "candidates = defaultdict(list)\n",
        "\n",
        "# Research question -> search both sections (often phrased in Methods/Intro of Methods or Exp intro)\n",
        "candidates['research_question'] = find_sentences(methods_sents + exp_sents, rq_patterns, top=3)\n",
        "# Type of study -> search Experiment first, then Methods\n",
        "candidates['study_type'] = find_sentences(exp_sents + methods_sents, type_patterns, top=3)\n",
        "# Independent variables (likely in Methods)\n",
        "candidates['independent'] = find_sentences(methods_sents + exp_sents, indep_patterns, top=5)\n",
        "# Dependent variables / metrics (likely in Experiment)\n",
        "candidates['dependent'] = find_sentences(exp_sents + methods_sents, dep_patterns, top=5)\n",
        "# Datasets & tools\n",
        "candidates['data_tools'] = find_sentences(methods_sents + exp_sents, data_tools_patterns, top=8)\n",
        "# Baseline/control\n",
        "candidates['baseline'] = find_sentences(exp_sents + methods_sents, baseline_patterns, top=5)\n",
        "# Repeatability/transparency\n",
        "candidates['repeatability'] = find_sentences(methods_sents + exp_sents, repeat_patterns, top=5)\n",
        "\n",
        "candidates\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk-5jEFLcqSl",
        "outputId": "2bd18d7c-9be1-4da5-f006-c292b12e36f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'research_question': ['Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \\nrelies on open-source products in alignment with the findings from Chapter 2.'],\n",
              "             'study_type': ['Experiment \\nIn this chapter, the generative AI service implementation framework introduced in Chapter 3 is utilized to implement \\nvarious scenarios based on enterprise internal data using the integrated RAG model and LangChain according to the \\nimplementation procedure.'],\n",
              "             'independent': ['The following sections detail the implementation process for each technology \\ncomponent within the framework: \\n \\n3.1 Framework for Implementing Generative AI Services using RAG Model \\nBased on previous research, we have designed a comprehensive framework for implementing generative AI \\nservices using the Retrieval-Augmented Generation (RAG) model.',\n",
              "              \"LangChain's module is utilized to split data into chunks that are suitable \\nfor retrieval.\",\n",
              "              '5) Integration of Prompt and Search Results: This step involves searching for information based on the \\nprompted question and integrating relevant information.',\n",
              "              'To search for contextually relevant information \\nbased on the prompt, appropriate chunks are retrieved from the vector database.',\n",
              "              'Each of these vectors, generated in this \\nmanner, is stored in Chroma for future retrieval, as illustrated in Fig 14.'],\n",
              "             'dependent': ['Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \\nrelies on open-source products in alignment with the findings from Chapter 2.'],\n",
              "             'data_tools': ['The diagram conceptually illustrates the process of utilizing \\nLLMs to retrieve information from documents and outlines the key steps involved.',\n",
              "              'Fig 10 Framework for Implementing Generative AI Services using RAG Model \\n \\n3.2 RAG model and LangChain integration implementation process \\nWithin the framework for implementing generative AI services, various solutions exist for each step of the process.',\n",
              "              \"In this context, the framework takes \\nadvantage of both OpenAI's proprietary models and open-source models to trigger the generative AI capabilities of \\nLLMs.\",\n",
              "              'For the overall orchestration framework, LangChain is adopted, while specific tasks like chunking and embedding \\nare accomplished using a combination of OpenAI models and GPT4All.',\n",
              "              'The vector repository is facilitated through \\nChroma DB, chosen for its ease of implementation.',\n",
              "              \"Regarding the LLM component, OpenAI's GPT-3.5-turbo model \\nand GPT4All are integrated, allowing developers to harness diverse choices to achieve optimal development \\noutcomes.\",\n",
              "              'Structured data is stored in standardized formats such as CSV, JSON, or XML, while \\nunstructured data is stored in formats like PDF, TXT, HTML, images, and videos.',\n",
              "              'Preparatory materials \\nrelated to the task, such as regulations, user manuals, and terms and conditions, are loaded into \\nLangChain using the LangChain module.'],\n",
              "             'baseline': [],\n",
              "             'repeatability': ['The diagram conceptually illustrates the process of utilizing \\nLLMs to retrieve information from documents and outlines the key steps involved.',\n",
              "              'Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \\nrelies on open-source products in alignment with the findings from Chapter 2.',\n",
              "              \"In this context, the framework takes \\nadvantage of both OpenAI's proprietary models and open-source models to trigger the generative AI capabilities of \\nLLMs.\",\n",
              "              '20 \\n \\n3.2.1 RAG based implementation procedure \\nThe RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented \\ngenerative model used to retrieve and generate responses based on information relevant to given questions or topics.',\n",
              "              'Each step follows the procedure outlined in Fig 9.']})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Synthesis] Turn candidates into concise answers (fallbacks if nothing found)\n",
        "# Comment: We compose short, direct answers based on mined sentences + heuristics.\n",
        "\n",
        "def summarize_list(lst, max_len=2):\n",
        "    return \" \".join(lst[:max_len]) if lst else \"\"\n",
        "\n",
        "answers = {}\n",
        "\n",
        "# 1) Research question\n",
        "answers['research_question'] = summarize_list(candidates['research_question']) or \\\n",
        "    \"The study investigates how to implement enterprise-grade Generative AI services using a RAG-based LLM application architecture.\"\n",
        "\n",
        "# 2) Type of study\n",
        "type_guess = \"implementation case with experimental evaluation\"\n",
        "if candidates['study_type']:\n",
        "    if re.search(r\"comparative\", \" \".join(candidates['study_type']), flags=re.I):\n",
        "        type_guess = \"comparative experimental study\"\n",
        "    elif re.search(r\"experiment|experimental|evaluation\", \" \".join(candidates['study_type']), flags=re.I):\n",
        "        type_guess = \"experimental evaluation of an implementation\"\n",
        "answers['study_type'] = type_guess\n",
        "\n",
        "# 3) Independent / Dependent variables\n",
        "indep_guess = \"Configuration choices such as chunk size, top-k retrieval, embedding/LLM selection, and prompt parameters.\"\n",
        "dep_guess = \"Outcome measures such as relevance/quality of answers, and possibly latency or cost if reported.\"\n",
        "answers['independent_vars'] = summarize_list(candidates['independent']) or indep_guess\n",
        "answers['dependent_vars'] = summarize_list(candidates['dependent']) or dep_guess\n",
        "\n",
        "# 4) Datasets & tools\n",
        "answers['datasets_tools'] = summarize_list(candidates['data_tools'], max_len=4) or \\\n",
        "    \"Enterprise knowledge sources (documents) plus standard RAG components (vector DB such as Chroma/FAISS, framework like LangChain/LlamaIndex, and an LLM provider).\"\n",
        "\n",
        "# 5) Control / Baseline\n",
        "answers['baseline'] = summarize_list(candidates['baseline'], max_len=2) or \\\n",
        "    \"No explicit baseline/control is clearly stated in the extracted text; if present, it may be qualitative or implicit.\"\n",
        "\n",
        "# 6) Repeatability / Transparency\n",
        "answers['repeatability'] = summarize_list(candidates['repeatability'], max_len=3) or \\\n",
        "    \"The procedure is described at a system level (pipelines, components, and steps). Exact parameters/code availability may limit full reproducibility.\"\n",
        "\n",
        "answers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N2PnN04cvcH",
        "outputId": "852a26b6-ab0e-4ae4-c69a-79a53765882e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'research_question': 'Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \\nrelies on open-source products in alignment with the findings from Chapter 2.',\n",
              " 'study_type': 'experimental evaluation of an implementation',\n",
              " 'independent_vars': \"The following sections detail the implementation process for each technology \\ncomponent within the framework: \\n \\n3.1 Framework for Implementing Generative AI Services using RAG Model \\nBased on previous research, we have designed a comprehensive framework for implementing generative AI \\nservices using the Retrieval-Augmented Generation (RAG) model. LangChain's module is utilized to split data into chunks that are suitable \\nfor retrieval.\",\n",
              " 'dependent_vars': 'Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \\nrelies on open-source products in alignment with the findings from Chapter 2.',\n",
              " 'datasets_tools': \"The diagram conceptually illustrates the process of utilizing \\nLLMs to retrieve information from documents and outlines the key steps involved. Fig 10 Framework for Implementing Generative AI Services using RAG Model \\n \\n3.2 RAG model and LangChain integration implementation process \\nWithin the framework for implementing generative AI services, various solutions exist for each step of the process. In this context, the framework takes \\nadvantage of both OpenAI's proprietary models and open-source models to trigger the generative AI capabilities of \\nLLMs. For the overall orchestration framework, LangChain is adopted, while specific tasks like chunking and embedding \\nare accomplished using a combination of OpenAI models and GPT4All.\",\n",
              " 'baseline': 'No explicit baseline/control is clearly stated in the extracted text; if present, it may be qualitative or implicit.',\n",
              " 'repeatability': \"The diagram conceptually illustrates the process of utilizing \\nLLMs to retrieve information from documents and outlines the key steps involved. Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \\nrelies on open-source products in alignment with the findings from Chapter 2. In this context, the framework takes \\nadvantage of both OpenAI's proprietary models and open-source models to trigger the generative AI capabilities of \\nLLMs.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Markdown Output] Ready-to-paste answers for Exercise 2\n",
        "# Comment: Produce a clean Markdown block with concise responses.\n",
        "md2 = f\"\"\"## Exercise 2: Critical Analysis of Experimental Design\n",
        "\n",
        "**Target sections:** Methods (p.19) and Experiment (p.20–25)\n",
        "\n",
        "1) **Research question**\n",
        "{answers['research_question']}\n",
        "\n",
        "2) **Type of study (implementation case / experimental / comparative)**\n",
        "{answers['study_type']}\n",
        "\n",
        "3) **Independent and dependent variables**\n",
        "- **Independent:** {answers['independent_vars']}\n",
        "- **Dependent:** {answers['dependent_vars']}\n",
        "\n",
        "4) **Datasets and tools used**\n",
        "{answers['datasets_tools']}\n",
        "\n",
        "5) **Control or comparison method (baseline / alternative)**\n",
        "{answers['baseline']}\n",
        "\n",
        "6) **Repeatability and transparency**\n",
        "{answers['repeatability']}\n",
        "\"\"\"\n",
        "print(md2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnxcoNd0cyfR",
        "outputId": "fbb25e18-4962-47f6-bf01-f6aa4b772c4d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Exercise 2: Critical Analysis of Experimental Design\n",
            "\n",
            "**Target sections:** Methods (p.19) and Experiment (p.20–25)\n",
            "\n",
            "1) **Research question**  \n",
            "Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \n",
            "relies on open-source products in alignment with the findings from Chapter 2.\n",
            "\n",
            "2) **Type of study (implementation case / experimental / comparative)**  \n",
            "experimental evaluation of an implementation\n",
            "\n",
            "3) **Independent and dependent variables**  \n",
            "- **Independent:** The following sections detail the implementation process for each technology \n",
            "component within the framework: \n",
            " \n",
            "3.1 Framework for Implementing Generative AI Services using RAG Model \n",
            "Based on previous research, we have designed a comprehensive framework for implementing generative AI \n",
            "services using the Retrieval-Augmented Generation (RAG) model. LangChain's module is utilized to split data into chunks that are suitable \n",
            "for retrieval.  \n",
            "- **Dependent:** Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \n",
            "relies on open-source products in alignment with the findings from Chapter 2.\n",
            "\n",
            "4) **Datasets and tools used**  \n",
            "The diagram conceptually illustrates the process of utilizing \n",
            "LLMs to retrieve information from documents and outlines the key steps involved. Fig 10 Framework for Implementing Generative AI Services using RAG Model \n",
            " \n",
            "3.2 RAG model and LangChain integration implementation process \n",
            "Within the framework for implementing generative AI services, various solutions exist for each step of the process. In this context, the framework takes \n",
            "advantage of both OpenAI's proprietary models and open-source models to trigger the generative AI capabilities of \n",
            "LLMs. For the overall orchestration framework, LangChain is adopted, while specific tasks like chunking and embedding \n",
            "are accomplished using a combination of OpenAI models and GPT4All.\n",
            "\n",
            "5) **Control or comparison method (baseline / alternative)**  \n",
            "No explicit baseline/control is clearly stated in the extracted text; if present, it may be qualitative or implicit.\n",
            "\n",
            "6) **Repeatability and transparency**  \n",
            "The diagram conceptually illustrates the process of utilizing \n",
            "LLMs to retrieve information from documents and outlines the key steps involved. Considering the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \n",
            "relies on open-source products in alignment with the findings from Chapter 2. In this context, the framework takes \n",
            "advantage of both OpenAI's proprietary models and open-source models to trigger the generative AI capabilities of \n",
            "LLMs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Evaluation Metrics and Evidence"
      ],
      "metadata": {
        "id": "gWwv2RF6dbfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Extract] Focus on Experiment/Results region to mine claims & metrics\n",
        "# Comment: We reuse the page bounds found earlier.\n",
        "import fitz\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'PDF_PATH' in globals(), \"Run the Upload cell first.\"\n",
        "PDF_PATH = Path(PDF_PATH)\n",
        "\n",
        "EXPERIMENT_START = 20\n",
        "CONCLUSION_START = 26\n",
        "\n",
        "doc = fitz.open(PDF_PATH)\n",
        "pages_full_text = [doc[i].get_text(\"text\") for i in range(len(doc))]\n",
        "doc.close()\n",
        "\n",
        "def extract_range(pages, start_page, end_page_exclusive):\n",
        "    s = start_page - 1\n",
        "    e = end_page_exclusive - 1\n",
        "    return \"\\n\".join(pages[s:e])\n",
        "\n",
        "exp_txt = extract_range(pages_full_text, EXPERIMENT_START, CONCLUSION_START)\n",
        "len(exp_txt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAvT-kTNdcrt",
        "outputId": "38d7f3ef-bdd4-4fa6-d64d-03ed9f73ecf5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11079"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Prep] Clean text and split sentences\n",
        "# Comment: Simple normalization to improve regex search quality.\n",
        "import re\n",
        "\n",
        "def clean(t: str) -> str:\n",
        "    t = t.replace(\"\\xa0\", \" \")\n",
        "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
        "    t = re.sub(r\"\\n{2,}\", \"\\n\\n\", t.strip())\n",
        "    return t\n",
        "\n",
        "def split_sentences(t: str):\n",
        "    t = clean(t)\n",
        "    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", t)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "exp_sents = split_sentences(exp_txt)\n",
        "len(exp_sents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkms-Nu5dlfg",
        "outputId": "f1313b97-9f3b-4e04-d836-93ebccdd3e0f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Mining] Detect performance claims, model names, and metrics (heuristics)\n",
        "# Comment: We look for typical phrases (e.g., \"compared to\", metric names like accuracy/latency, model names).\n",
        "from collections import defaultdict\n",
        "\n",
        "patterns = {\n",
        "    \"claims\": [\n",
        "        r\"\\b(compared to|versus|vs\\.|improve(s|d)|outperform(s|ed)|better than|higher than|lower than)\\b\",\n",
        "        r\"\\b(RAG|retrieval[- ]?augmented|fine[- ]?tuning|zero[- ]?shot|few[- ]?shot)\\b\",\n",
        "        r\"\\b(ablations?|baseline)\\b\",\n",
        "    ],\n",
        "    \"models\": [\n",
        "        r\"\\bGPT[- ]?3\\.5\\b|\\bGPT[- ]?4\\b|\\bLlama[- ]?\\d\\b|\\bMistral\\b|\\bClaude\\b|\\bPaLM\\b|\\bGemini\\b\",\n",
        "        r\"\\bAzure OpenAI\\b|\\bOpenAI\\b|\\bLlamaIndex\\b|\\bLangChain\\b\",\n",
        "    ],\n",
        "    \"metrics\": [\n",
        "        r\"\\baccuracy\\b|\\bprecision\\b|\\brecall\\b|\\bF1\\b|\\bbleu\\b|\\brouge[- ]?(L|1|2)?\\b|\\bNDCG\\b|\\bMRR\\b|\\bhit rate\\b|\\bMAP\\b\",\n",
        "        r\"\\blatency\\b|\\bresponse time\\b|\\bthroughput\\b|\\bcost\\b|\\btoken(s)?\\b|\\bprice\\b\",\n",
        "        r\"\\bhuman (evaluation|feedback|study|ratings?)\\b|\\buser study\\b|\\bsubjective\\b|\\bLikert\\b\",\n",
        "        r\"\\brobust(ness)?\\b|\\bgeneralization\\b|\\baccuracy on\\b|\\berror rate\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "def find_by_patterns(sents, pats, top=50):\n",
        "    res = []\n",
        "    regs = [re.compile(p, flags=re.I) for p in pats]\n",
        "    for s in sents:\n",
        "        if any(r.search(s) for r in regs):\n",
        "            res.append(s)\n",
        "            if len(res) >= top:\n",
        "                break\n",
        "    return res\n",
        "\n",
        "mined = {\n",
        "    \"claims\": find_by_patterns(exp_sents, patterns[\"claims\"], top=80),\n",
        "    \"models\": find_by_patterns(exp_sents, patterns[\"models\"], top=80),\n",
        "    \"metrics\": find_by_patterns(exp_sents, patterns[\"metrics\"], top=80),\n",
        "}\n",
        "{ k: len(v) for k,v in mined.items() }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSw90qTLdoGF",
        "outputId": "744b26b5-4c9e-4182-8f72-01f3bed5640b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'claims': 3, 'models': 15, 'metrics': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Synthesis — Fixed] Build concise answers for the 4 questions\n",
        "# Comment: Use bool(re.search(...)) instead of any(re.search(...)).\n",
        "\n",
        "def shortlist(lines, n=4, max_chars=300):\n",
        "    out = []\n",
        "    for s in lines[:n]:\n",
        "        s_clean = s.strip()\n",
        "        if len(s_clean) > max_chars:\n",
        "            s_clean = s_clean[:max_chars].rstrip() + \"...\"\n",
        "        out.append(f\"- {s_clean}\")\n",
        "    if not out:\n",
        "        out = [\"- (No explicit sentences mined; the paper may be descriptive or qualitative in this part.)\"]\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "claims_block = shortlist(mined[\"claims\"], n=5)\n",
        "models_block = shortlist(mined[\"models\"], n=5)\n",
        "metrics_block = shortlist(mined[\"metrics\"], n=6)\n",
        "\n",
        "# Decide appropriateness & suggestions (fixed)\n",
        "text_metrics = \"\\n\".join(mined[\"metrics\"])\n",
        "has_quant_metrics = bool(re.search(r\"(accuracy|precision|recall|F1|BLEU|ROUGE|NDCG|MRR|MAP|hit rate)\", text_metrics, re.I))\n",
        "has_latency_cost  = bool(re.search(r\"(latency|response time|throughput|cost|token)\", text_metrics, re.I))\n",
        "has_human_eval    = bool(re.search(r\"(human (evaluation|feedback|study|ratings?)|user study|Likert)\", text_metrics, re.I))\n",
        "\n",
        "appropriateness = []\n",
        "if has_quant_metrics:\n",
        "    appropriateness.append(\"Quantitative metrics are mentioned, which are generally appropriate for evaluating retrieval/answer quality.\")\n",
        "else:\n",
        "    appropriateness.append(\"No clear quantitative metrics were detected; evidence appears descriptive or system-level.\")\n",
        "\n",
        "if has_latency_cost:\n",
        "    appropriateness.append(\"Operational aspects (latency/cost) are considered, which align with enterprise constraints.\")\n",
        "else:\n",
        "    appropriateness.append(\"Latency/cost are not explicitly reported; for enterprise deployment, these are important.\")\n",
        "\n",
        "if has_human_eval:\n",
        "    appropriateness.append(\"Human evaluation is referenced, which helps assess answer usefulness.\")\n",
        "else:\n",
        "    appropriateness.append(\"No human evaluation found; user studies or expert ratings would strengthen conclusions.\")\n",
        "\n",
        "suggestions = []\n",
        "if not has_quant_metrics:\n",
        "    suggestions.append(\"Add task-appropriate quantitative metrics (e.g., retrieval NDCG/MRR, QA Exact Match/F1).\")\n",
        "if not has_latency_cost:\n",
        "    suggestions.append(\"Report latency and cost per query (tokens, dollar cost) under realistic loads.\")\n",
        "if not has_human_eval:\n",
        "    suggestions.append(\"Include small-scale human evaluation (usefulness, correctness, citations) with clear rubrics.\")\n",
        "suggestions.append(\"Provide ablations (e.g., chunk size, top-k, embed model) and confidence intervals to show robustness.\")\n",
        "suggestions.append(\"If claiming improvements vs. baselines, specify baselines and test sets clearly.\")\n",
        "\n",
        "answers3 = {\n",
        "    \"claims\": claims_block,\n",
        "    \"metrics_found\": metrics_block,\n",
        "    \"models_mentioned\": models_block,\n",
        "    \"appropriateness\": \" \".join(appropriateness),\n",
        "    \"what_to_add\": \" \".join(suggestions)\n",
        "}\n",
        "\n",
        "answers3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWa7HIQjeMBg",
        "outputId": "69ae9980-4298-446a-9497-f8e09f2f167f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'claims': '- 20 \\n \\n3.2.1 RAG based implementation procedure \\nThe RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented \\ngenerative model used to retrieve and generate responses based on information relevant to given questions or topics.\\n- The RAG-based implementation procedure outlined above illustrates how the RAG model, in combination with \\nLangChain, can be effectively integrated into the generative AI service framework.\\n- Experiment \\nIn this chapter, the generative AI service implementation framework introduced in Chapter 3 is utilized to implement \\nvarious scenarios based on enterprise internal data using the integrated RAG model and LangChain according to the \\nimplementation procedure.',\n",
              " 'metrics_found': '- (No explicit sentences mined; the paper may be descriptive or qualitative in this part.)',\n",
              " 'models_mentioned': \"- Preparatory materials \\nrelated to the task, such as regulations, user manuals, and terms and conditions, are loaded into \\nLangChain using the LangChain module.\\n- LangChain's module is utilized to split data into chunks that are suitable \\nfor retrieval.\\n- This \\nstep involves mapping words or sentences to vectors, and libraries provided by OpenAI or GPT4All can be \\nemployed for this purpose.\\n- Various search engines available within \\nLangChain for vector store similarity search are utilized.\\n- LLM, such as OpenAI's \\nGPT-3.5-turbo model or GPT4All, uses the similarity search module in LangChain to retrieve relevant \\ndocuments and generate responses.\",\n",
              " 'appropriateness': 'No clear quantitative metrics were detected; evidence appears descriptive or system-level. Latency/cost are not explicitly reported; for enterprise deployment, these are important. No human evaluation found; user studies or expert ratings would strengthen conclusions.',\n",
              " 'what_to_add': 'Add task-appropriate quantitative metrics (e.g., retrieval NDCG/MRR, QA Exact Match/F1). Report latency and cost per query (tokens, dollar cost) under realistic loads. Include small-scale human evaluation (usefulness, correctness, citations) with clear rubrics. Provide ablations (e.g., chunk size, top-k, embed model) and confidence intervals to show robustness. If claiming improvements vs. baselines, specify baselines and test sets clearly.'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Markdown Output] Ready-to-paste block for Exercise 3\n",
        "md3 = f\"\"\"## Exercise 3: Evaluation Metrics and Evidence\n",
        "\n",
        "**Goal:** Understand and critique how the paper evaluates success.\n",
        "**Focus region:** Experiment/Results (pp. 20–25)\n",
        "\n",
        "### 1) Performance claims or comparisons (mined sentences)\n",
        "{answers3['claims']}\n",
        "\n",
        "**Models / frameworks mentioned (mined sentences)**\n",
        "{answers3['models_mentioned']}\n",
        "\n",
        "### 2) Evaluation metrics used (if any)\n",
        "{answers3['metrics_found']}\n",
        "\n",
        "### 3) Are these metrics appropriate for the task?\n",
        "{answers3['appropriateness']}\n",
        "\n",
        "### 4) What could be added to strengthen the evidence?\n",
        "- {answers3['what_to_add']}\n",
        "\"\"\"\n",
        "\n",
        "print(md3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLtVYeWmeQNL",
        "outputId": "6d53f79e-267d-4f6b-fdf4-6334489a148e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Exercise 3: Evaluation Metrics and Evidence\n",
            "\n",
            "**Goal:** Understand and critique how the paper evaluates success.  \n",
            "**Focus region:** Experiment/Results (pp. 20–25)\n",
            "\n",
            "### 1) Performance claims or comparisons (mined sentences)\n",
            "- 20 \n",
            " \n",
            "3.2.1 RAG based implementation procedure \n",
            "The RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented \n",
            "generative model used to retrieve and generate responses based on information relevant to given questions or topics.\n",
            "- The RAG-based implementation procedure outlined above illustrates how the RAG model, in combination with \n",
            "LangChain, can be effectively integrated into the generative AI service framework.\n",
            "- Experiment \n",
            "In this chapter, the generative AI service implementation framework introduced in Chapter 3 is utilized to implement \n",
            "various scenarios based on enterprise internal data using the integrated RAG model and LangChain according to the \n",
            "implementation procedure.\n",
            "\n",
            "**Models / frameworks mentioned (mined sentences)**  \n",
            "- Preparatory materials \n",
            "related to the task, such as regulations, user manuals, and terms and conditions, are loaded into \n",
            "LangChain using the LangChain module.\n",
            "- LangChain's module is utilized to split data into chunks that are suitable \n",
            "for retrieval.\n",
            "- This \n",
            "step involves mapping words or sentences to vectors, and libraries provided by OpenAI or GPT4All can be \n",
            "employed for this purpose.\n",
            "- Various search engines available within \n",
            "LangChain for vector store similarity search are utilized.\n",
            "- LLM, such as OpenAI's \n",
            "GPT-3.5-turbo model or GPT4All, uses the similarity search module in LangChain to retrieve relevant \n",
            "documents and generate responses.\n",
            "\n",
            "### 2) Evaluation metrics used (if any)\n",
            "- (No explicit sentences mined; the paper may be descriptive or qualitative in this part.)\n",
            "\n",
            "### 3) Are these metrics appropriate for the task?\n",
            "No clear quantitative metrics were detected; evidence appears descriptive or system-level. Latency/cost are not explicitly reported; for enterprise deployment, these are important. No human evaluation found; user studies or expert ratings would strengthen conclusions.\n",
            "\n",
            "### 4) What could be added to strengthen the evidence?\n",
            "- Add task-appropriate quantitative metrics (e.g., retrieval NDCG/MRR, QA Exact Match/F1). Report latency and cost per query (tokens, dollar cost) under realistic loads. Include small-scale human evaluation (usefulness, correctness, citations) with clear rubrics. Provide ablations (e.g., chunk size, top-k, embed model) and confidence intervals to show robustness. If claiming improvements vs. baselines, specify baselines and test sets clearly.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Take Notes Using the Cornell Method"
      ],
      "metadata": {
        "id": "RWXNIWgeemK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Config] Choose the technical subsection to extract\n",
        "# Comment: Set the exact heading label you want to target. We default to \"3.2.1\".\n",
        "TARGET_SUBSECTION_LABEL = \"3.2.1\"\n",
        "# Comment: (Optional) If you know part of the heading text, add it to tighten the match.\n",
        "TARGET_HEADING_HINT = \"RAG\"  # e.g., \"RAG Based Implementation Procedure\" (can be empty)\n",
        "\n",
        "# Comment: Safety check\n",
        "assert 'PDF_PATH' in globals(), \"Please run the upload cell first to define PDF_PATH.\"\n"
      ],
      "metadata": {
        "id": "O8Wyn2LGeowf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Extract Lines] Read PDF as lines per page for robust heading matching\n",
        "# Comment: We'll search for the subsection heading at line starts and capture until the next numbered heading.\n",
        "import fitz, re\n",
        "from pathlib import Path\n",
        "\n",
        "PDF_PATH = Path(PDF_PATH)\n",
        "\n",
        "def get_pages_lines(pdf_path: Path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = []\n",
        "    for i in range(len(doc)):\n",
        "        text = doc[i].get_text(\"text\")\n",
        "        lines = [ln.rstrip() for ln in text.splitlines()]\n",
        "        pages.append(lines)\n",
        "    doc.close()\n",
        "    return pages\n",
        "\n",
        "pages_lines = get_pages_lines(PDF_PATH)\n",
        "len(pages_lines)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz1RA9wMesYm",
        "outputId": "c1078f5d-cbee-4c23-95cb-7bd3f5426da8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Find Subsection] Locate the start line of the target subsection and slice until the next heading\n",
        "# Comment: Headings like \"3.2.1 ...\" then next numeric heading \"3.2.2 ...\" or \"3.3 ...\" mark the end.\n",
        "import itertools\n",
        "\n",
        "# Comment: Build a strict regex for the target heading at line start (e.g., \"^3.2.1 ...\")\n",
        "if TARGET_HEADING_HINT:\n",
        "    heading_re = re.compile(rf\"^{re.escape(TARGET_SUBSECTION_LABEL)}\\b.*{re.escape(TARGET_HEADING_HINT)}\", re.IGNORECASE)\n",
        "else:\n",
        "    heading_re = re.compile(rf\"^{re.escape(TARGET_SUBSECTION_LABEL)}\\b\", re.IGNORECASE)\n",
        "\n",
        "# Comment: Generic regex for next numbered heading like \"3.2.2\", \"3.3\", \"4.\", etc., at line start.\n",
        "next_heading_re = re.compile(r\"^\\d+(?:\\.\\d+){0,3}\\b\")\n",
        "\n",
        "start_pos = None  # (page_idx, line_idx)\n",
        "for p_idx, lines in enumerate(pages_lines):\n",
        "    for l_idx, line in enumerate(lines):\n",
        "        if heading_re.search(line.strip()):\n",
        "            start_pos = (p_idx, l_idx)\n",
        "            break\n",
        "    if start_pos:\n",
        "        break\n",
        "\n",
        "assert start_pos is not None, f\"Could not find subsection starting with '{TARGET_SUBSECTION_LABEL}' (hint='{TARGET_HEADING_HINT}').\"\n",
        "\n",
        "# Comment: Collect lines from start until the next numbered heading or document end\n",
        "p0, l0 = start_pos\n",
        "collected = []\n",
        "for p_idx in range(p0, len(pages_lines)):\n",
        "    lines = pages_lines[p_idx]\n",
        "    # determine starting line on first page\n",
        "    li = l0 if p_idx == p0 else 0\n",
        "    while li < len(lines):\n",
        "        line = lines[li]\n",
        "        if (p_idx, li) != start_pos and next_heading_re.match(line.strip()):\n",
        "            # Stop at the first subsequent heading\n",
        "            break\n",
        "        collected.append(line)\n",
        "        li += 1\n",
        "    # If we broke due to new heading, stop outer loop\n",
        "    if li < len(lines) and (p_idx, li) != start_pos and next_heading_re.match(lines[li].strip()):\n",
        "        break\n",
        "\n",
        "subsection_text = \"\\n\".join(collected).strip()\n",
        "print(f\"Start at page {p0+1}, line {l0+1}; collected {len(collected)} lines.\")\n",
        "print(subsection_text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQvUOKyaevh2",
        "outputId": "151d9549-cd13-4682-b517-e16743854fd1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start at page 20, line 3; collected 4 lines.\n",
            "3.2.1 RAG based implementation procedure\n",
            "The RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented\n",
            "generative model used to retrieve and generate responses based on information relevant to given questions or topics.\n",
            "Each step follows the procedure outlined in Fig 9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Mine Cues/Notes] Extract key terms and step-like sentences to pre-fill Cornell notes\n",
        "# Comment: Light heuristics: keyword spotting + sentences with procedure verbs (build, index, retrieve, embed, etc.)\n",
        "import re\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    t = t.replace(\"\\xa0\", \" \")\n",
        "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
        "    t = re.sub(r\"\\n{2,}\", \"\\n\\n\", t.strip())\n",
        "    return t\n",
        "\n",
        "text_clean = clean_text(subsection_text)\n",
        "\n",
        "# Comment: Candidate domain keywords commonly seen in RAG pipelines\n",
        "KEYWORD_CANDIDATES = [\n",
        "    \"chunk\", \"chunking\", \"overlap\", \"embedding\", \"embedding model\", \"vector\", \"vector DB\",\n",
        "    \"Chroma\", \"FAISS\", \"Milvus\", \"Pinecone\", \"Weaviate\",\n",
        "    \"retriever\", \"top-k\", \"similarity\", \"cosine\", \"index\",\n",
        "    \"LangChain\", \"LlamaIndex\", \"prompt\", \"template\", \"system prompt\",\n",
        "    \"context window\", \"document\", \"metadata\", \"citation\",\n",
        "    \"rerank\", \"BM25\", \"hybrid search\",\n",
        "    \"OpenAI\", \"Azure OpenAI\", \"Llama\", \"Mistral\", \"GPT-4\", \"GPT-3.5\",\n",
        "    \"API\", \"endpoint\", \"pipeline\", \"ETL\", \"preprocessing\"\n",
        "]\n",
        "\n",
        "present_keywords = []\n",
        "lower_text = text_clean.lower()\n",
        "for kw in KEYWORD_CANDIDATES:\n",
        "    if kw.lower() in lower_text and kw not in present_keywords:\n",
        "        present_keywords.append(kw)\n",
        "\n",
        "# Comment: Build \"Cues\" as questions like \"What is <kw>?\" for the first ~8 keywords\n",
        "def cues_from_keywords(kws, n=8):\n",
        "    items = []\n",
        "    for kw in kws[:n]:\n",
        "        items.append(f\"- What is **{kw}**?\")\n",
        "    return items\n",
        "\n",
        "# Comment: Extract procedure-like sentences (imperative or contains action verbs)\n",
        "sentences = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text_clean)\n",
        "ACTION_VERBS = r\"(configure|build|index|embed|retrieve|chunk|split|store|query|rerank|evaluate|deploy|scale|monitor|log)\"\n",
        "step_sents = [s for s in sentences if re.search(ACTION_VERBS, s, re.IGNORECASE)]\n",
        "step_sents = step_sents[:8] if step_sents else sentences[:6]\n",
        "\n",
        "cornell_cues = cues_from_keywords(present_keywords, n=8)\n",
        "cornell_notes_bullets = [f\"- {s.strip()}\" for s in step_sents]\n",
        "len(present_keywords), len(cornell_notes_bullets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tvjn6r1e0g3",
        "outputId": "56195d6a-09e0-4473-dc5f-0cfece38a4c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Draft Summary] Build a 3–4 sentence draft summary from the first paragraph\n",
        "# Comment: Keep it short; user can tweak wording.\n",
        "paras = [p.strip() for p in text_clean.split(\"\\n\\n\") if p.strip()]\n",
        "first_para = paras[0] if paras else text_clean\n",
        "draft_sents = re.split(r\"(?<=[\\.\\?\\!])\\s+\", first_para)\n",
        "draft_summary = \" \".join(draft_sents[:4]).strip()\n",
        "\n",
        "print(draft_summary[:800])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfrLZ9r_e4tY",
        "outputId": "820f62fb-8aca-429e-cef3-2a48f55ea0d8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2.1 RAG based implementation procedure\n",
            "The RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented\n",
            "generative model used to retrieve and generate responses based on information relevant to given questions or topics. Each step follows the procedure outlined in Fig 9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Markdown Output] Cornell notes ready-to-paste block\n",
        "# Comment: We render a Cornell layout with Cues (left), Notes (right), and Summary section.\n",
        "from textwrap import shorten\n",
        "\n",
        "def bullet_block(items, max_items=None, max_chars=300):\n",
        "    out = []\n",
        "    count = 0\n",
        "    for it in items:\n",
        "        if max_items is not None and count >= max_items: break\n",
        "        it_trim = it if len(it) <= max_chars else shorten(it, width=max_chars, placeholder=\"...\")\n",
        "        out.append(it_trim)\n",
        "        count += 1\n",
        "    return \"\\n\".join(out) if out else \"- (Fill manually)\"\n",
        "\n",
        "md4 = f\"\"\"## Exercise 4: Cornell Notes — Technical Subsection\n",
        "\n",
        "**Section chosen:** {TARGET_SUBSECTION_LABEL} (auto-detected)\n",
        "\n",
        "### Cues (Left Column)\n",
        "{bullet_block(cornell_cues, max_items=8, max_chars=140)}\n",
        "\n",
        "### Notes (Right Column)\n",
        "{bullet_block(cornell_notes_bullets, max_items=10, max_chars=300)}\n",
        "\n",
        "### Summary (3–4 sentences)\n",
        "{draft_summary}\n",
        "\"\"\"\n",
        "\n",
        "print(md4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKR84nQWe72s",
        "outputId": "333247d2-4f1a-422d-849c-a50872715d90"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Exercise 4: Cornell Notes — Technical Subsection\n",
            "\n",
            "**Section chosen:** 3.2.1 (auto-detected)\n",
            "\n",
            "### Cues (Left Column)\n",
            "- (Fill manually)\n",
            "\n",
            "### Notes (Right Column)\n",
            "- 3.2.1 RAG based implementation procedure\n",
            "The RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented\n",
            "generative model used to retrieve and generate responses based on information relevant to given questions or topics.\n",
            "\n",
            "### Summary (3–4 sentences)\n",
            "3.2.1 RAG based implementation procedure\n",
            "The RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search-augmented\n",
            "generative model used to retrieve and generate responses based on information relevant to given questions or topics. Each step follows the procedure outlined in Fig 9.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Cornell Notes — Technical Subsection\n",
        "\n",
        "**Section chosen:** 3.2.1 RAG Based Implementation Procedure\n",
        "\n",
        "| Cues (Key terms / Questions) | Notes (Explanations / Steps) |\n",
        "|------------------------------|------------------------------|\n",
        "| What is the RAG model? | The RAG model is a search-augmented generative model that retrieves and generates responses based on relevant information for a given question or topic. |\n",
        "| What is the purpose of RAG? | To combine retrieval from a knowledge base with generation, ensuring outputs are grounded in specific data. |\n",
        "| How is RAG implemented in this study? | Follows a multi-step pipeline as outlined in Fig. 9, including document chunking, embedding, storing in a vector database, and retrieval during generation. |\n",
        "| Which components are involved? | Chunking module, embedding model, vector database (e.g., Chroma), retriever, and LLM for generation. |\n",
        "| What triggers the retrieval? | User queries or prompts initiate a search in the vector DB for the most relevant chunks. |\n",
        "| How are retrieved results used? | They are appended to the prompt given to the LLM to provide context and improve factual accuracy. |\n",
        "\n",
        "**Summary (3-4 sentences)**  \n",
        "The 3.2.1 RAG Based Implementation Procedure section describes a search-augmented generative model architecture integrating retrieval and generation. The process involves chunking documents, embedding them, storing embeddings in a vector database, and retrieving the most relevant chunks at query time. Retrieved content is then appended to the LLM's input to produce accurate and context-aware responses. This approach ensures that generated answers are grounded in enterprise data.\n"
      ],
      "metadata": {
        "id": "Z209Jq6gfmLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: Summarize the Paper with 5W1H"
      ],
      "metadata": {
        "id": "XPwWTeUxgOuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Extract Full Intro + Conclusion] for 5W1H info mining\n",
        "# Comment: Intro and Conclusion usually contain Who, What, Why, and high-level How.\n",
        "INTRO_START = 2\n",
        "RELATED_START = 4\n",
        "CONCLUSION_START = 26\n",
        "END_PAGE = len(pages_full_text)\n",
        "\n",
        "intro_txt = extract_range(pages_full_text, INTRO_START, RELATED_START)\n",
        "conclusion_txt = extract_range(pages_full_text, CONCLUSION_START, END_PAGE)\n",
        "\n",
        "full_text_5w1h = intro_txt + \"\\n\" + conclusion_txt\n",
        "len(full_text_5w1h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N78gHivxgOax",
        "outputId": "dbbb830e-33b1-4618-ce81-ef5b4b071135"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15399"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Prep Sentences] Clean and split for keyword search\n",
        "import re\n",
        "\n",
        "def clean(t: str) -> str:\n",
        "    t = t.replace(\"\\xa0\", \" \")\n",
        "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
        "    t = re.sub(r\"\\n{2,}\", \"\\n\\n\", t.strip())\n",
        "    return t\n",
        "\n",
        "def split_sentences(t: str):\n",
        "    t = clean(t)\n",
        "    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", t)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "sent_5w1h = split_sentences(full_text_5w1h)\n",
        "len(sent_5w1h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAEqEmv-gVQz",
        "outputId": "2b0c848f-325a-4226-d625-34fc055dd7a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Mining] Simple keyword-based extraction for each W and H\n",
        "from collections import defaultdict\n",
        "\n",
        "patterns_5w1h = {\n",
        "    \"who\": [r\"\\b(author|we|research team|university|institute|company|organization)\\b\"],\n",
        "    \"what\": [r\"\\b(we propose|we present|this paper|this study|this work|developed|implemented|designed|introduced)\\b\"],\n",
        "    \"when_where\": [r\"\\b(published|conference|journal|in\\s\\d{4}|presented at|location|venue)\\b\"],\n",
        "    \"why\": [r\"\\b(problem|challenge|need|requirement|important|motivation|necessity|aim|goal)\\b\"],\n",
        "    \"how\": [r\"\\b(implemented|designed|architecture|approach|method|pipeline|framework|system)\\b\"]\n",
        "}\n",
        "\n",
        "def find_matches(sentences, patterns, top=3):\n",
        "    regs = [re.compile(p, flags=re.I) for p in patterns]\n",
        "    out = []\n",
        "    for s in sentences:\n",
        "        if any(r.search(s) for r in regs):\n",
        "            out.append(s)\n",
        "            if len(out) >= top:\n",
        "                break\n",
        "    return out\n",
        "\n",
        "matches_5w1h = {k: find_matches(sent_5w1h, v) for k,v in patterns_5w1h.items()}\n",
        "matches_5w1h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BT3dxhGgZS7",
        "outputId": "e82cde75-b2f1-4d62-d8f7-fd49b17cf639"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'who': ['Notably, similar to all APIs, data \\ntransmitted through the fine-tuning API is owned by the customers, and OpenAI or any other organization is precluded \\nfrom employing this data for training alternative models, as officially stated.',\n",
              "  'Subsequently, when user \\nqueries arise—such as inquiries regarding company dress codes through a chatbot—pertinent information can be \\nretrieved and presented to the LLM through prompts, proving to be a more practical and efficient approach.',\n",
              "  'Conclusion and Discussion \\nIn this study, we presented methods and implementation cases for developing generative AI services using LLM \\napplication architecture, aiming to explore avenues for advancing the development and industrial utilization of \\ngenerative AI technology.'],\n",
              " 'what': ['In pursuit of overcoming such limitations, OpenAI introduced the capability to fine-tune the GPT-3.5 Turbo \\nmodel, a significant advancement unveiled in August 2023.',\n",
              "  'Notably, in 2021, DeepMind introduced RETRO, utilizing its internal database for information retrieval, \\nwhile OpenAI unveiled WebGPT with Bing-based searching capabilities in the same year.',\n",
              "  'Against this backdrop, this study seeks to propel the advancement of generative AI while surmounting its limitations, \\nby exploring methods to implement LLM applications using the RAG architecture.'],\n",
              " 'when_where': ['Notably, in 2021, DeepMind introduced RETRO, utilizing its internal database for information retrieval, \\nwhile OpenAI unveiled WebGPT with Bing-based searching capabilities in the same year.',\n",
              "  '(2023) A Study on the Service Integration of Traditional Chatbot and ChatGPT, Journal of \\nInformation Technology Application and Management, 30(4), pp.',\n",
              "  'Cho, J.I.(2023) Ultra-Large AI and Generative Artificial Intelligence, TTA Journal, Issue 207.'],\n",
              " 'why': ['We also discussed ways to overcome the \\ninformation scarcity challenge of LLM, either through fine-tuning or direct document information utilization, and delved \\ninto the specific functioning and key stages of the RAG model.',\n",
              "  'Secondly, the consistency and appropriateness of \\nRAG model-generated results may vary, and the challenge of information scarcity might still arise.',\n",
              "  'Exploring and applying various \\ntechniques to enhance the quality of generated content is important.'],\n",
              " 'how': ['This approach \\ninvolves additional training on specific domains using fresh datasets.',\n",
              "  '3 \\n \\nHowever, this approach entails significant costs.',\n",
              "  'Subsequently, when user \\nqueries arise—such as inquiries regarding company dress codes through a chatbot—pertinent information can be \\nretrieved and presented to the LLM through prompts, proving to be a more practical and efficient approach.']}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Fallback fill] Build concise answers\n",
        "def concat_or_fallback(key, fallback):\n",
        "    return \" \".join(matches_5w1h.get(key) or [fallback])\n",
        "\n",
        "who_ans = concat_or_fallback(\"who\", \"The authors of the study (not explicitly named in extracted text).\")\n",
        "what_ans = concat_or_fallback(\"what\", \"A RAG-based LLM application architecture for enterprise data integration.\")\n",
        "when_where_ans = concat_or_fallback(\"when_where\", \"Published in a scientific venue; exact date/location not detected in extracted text.\")\n",
        "why_ans = concat_or_fallback(\"why\", \"To address the need for accurate, context-aware generative AI in enterprise environments.\")\n",
        "how_ans = concat_or_fallback(\"how\", \"Implemented using a retrieval-augmented generation pipeline with document chunking, embedding, vector storage, and LLM-based response generation.\")\n",
        "\n",
        "# Build the 4-sentence paragraph\n",
        "objective = why_ans\n",
        "methodology = how_ans\n",
        "findings = what_ans\n",
        "implications = \"The architecture can be adapted to various enterprise contexts to improve information retrieval, reduce hallucinations, and support decision-making.\"\n",
        "\n",
        "answers5 = {\n",
        "    \"who\": who_ans,\n",
        "    \"what\": what_ans,\n",
        "    \"when_where\": when_where_ans,\n",
        "    \"why\": why_ans,\n",
        "    \"how\": how_ans,\n",
        "    \"objective\": objective,\n",
        "    \"methodology\": methodology,\n",
        "    \"findings\": findings,\n",
        "    \"implications\": implications\n",
        "}\n",
        "\n",
        "answers5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itBoz8p_gcKm",
        "outputId": "62ddb87a-90b4-490f-cdb6-73272e7f9c07"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'who': 'Notably, similar to all APIs, data \\ntransmitted through the fine-tuning API is owned by the customers, and OpenAI or any other organization is precluded \\nfrom employing this data for training alternative models, as officially stated. Subsequently, when user \\nqueries arise—such as inquiries regarding company dress codes through a chatbot—pertinent information can be \\nretrieved and presented to the LLM through prompts, proving to be a more practical and efficient approach. Conclusion and Discussion \\nIn this study, we presented methods and implementation cases for developing generative AI services using LLM \\napplication architecture, aiming to explore avenues for advancing the development and industrial utilization of \\ngenerative AI technology.',\n",
              " 'what': 'In pursuit of overcoming such limitations, OpenAI introduced the capability to fine-tune the GPT-3.5 Turbo \\nmodel, a significant advancement unveiled in August 2023. Notably, in 2021, DeepMind introduced RETRO, utilizing its internal database for information retrieval, \\nwhile OpenAI unveiled WebGPT with Bing-based searching capabilities in the same year. Against this backdrop, this study seeks to propel the advancement of generative AI while surmounting its limitations, \\nby exploring methods to implement LLM applications using the RAG architecture.',\n",
              " 'when_where': 'Notably, in 2021, DeepMind introduced RETRO, utilizing its internal database for information retrieval, \\nwhile OpenAI unveiled WebGPT with Bing-based searching capabilities in the same year. (2023) A Study on the Service Integration of Traditional Chatbot and ChatGPT, Journal of \\nInformation Technology Application and Management, 30(4), pp. Cho, J.I.(2023) Ultra-Large AI and Generative Artificial Intelligence, TTA Journal, Issue 207.',\n",
              " 'why': 'We also discussed ways to overcome the \\ninformation scarcity challenge of LLM, either through fine-tuning or direct document information utilization, and delved \\ninto the specific functioning and key stages of the RAG model. Secondly, the consistency and appropriateness of \\nRAG model-generated results may vary, and the challenge of information scarcity might still arise. Exploring and applying various \\ntechniques to enhance the quality of generated content is important.',\n",
              " 'how': 'This approach \\ninvolves additional training on specific domains using fresh datasets. 3 \\n \\nHowever, this approach entails significant costs. Subsequently, when user \\nqueries arise—such as inquiries regarding company dress codes through a chatbot—pertinent information can be \\nretrieved and presented to the LLM through prompts, proving to be a more practical and efficient approach.',\n",
              " 'objective': 'We also discussed ways to overcome the \\ninformation scarcity challenge of LLM, either through fine-tuning or direct document information utilization, and delved \\ninto the specific functioning and key stages of the RAG model. Secondly, the consistency and appropriateness of \\nRAG model-generated results may vary, and the challenge of information scarcity might still arise. Exploring and applying various \\ntechniques to enhance the quality of generated content is important.',\n",
              " 'methodology': 'This approach \\ninvolves additional training on specific domains using fresh datasets. 3 \\n \\nHowever, this approach entails significant costs. Subsequently, when user \\nqueries arise—such as inquiries regarding company dress codes through a chatbot—pertinent information can be \\nretrieved and presented to the LLM through prompts, proving to be a more practical and efficient approach.',\n",
              " 'findings': 'In pursuit of overcoming such limitations, OpenAI introduced the capability to fine-tune the GPT-3.5 Turbo \\nmodel, a significant advancement unveiled in August 2023. Notably, in 2021, DeepMind introduced RETRO, utilizing its internal database for information retrieval, \\nwhile OpenAI unveiled WebGPT with Bing-based searching capabilities in the same year. Against this backdrop, this study seeks to propel the advancement of generative AI while surmounting its limitations, \\nby exploring methods to implement LLM applications using the RAG architecture.',\n",
              " 'implications': 'The architecture can be adapted to various enterprise contexts to improve information retrieval, reduce hallucinations, and support decision-making.'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [Markdown Output] Ready-to-paste block for Exercise 5\n",
        "md5 = f\"\"\"## Exercise 5: 5W1H Summary\n",
        "\n",
        "**Who conducted the study?**\n",
        "{answers5['who']}\n",
        "\n",
        "**What was developed or proposed?**\n",
        "{answers5['what']}\n",
        "\n",
        "**When and Where was it published?**\n",
        "{answers5['when_where']}\n",
        "\n",
        "**Why is the problem important?**\n",
        "{answers5['why']}\n",
        "\n",
        "**How was the solution implemented?**\n",
        "{answers5['how']}\n",
        "\n",
        "---\n",
        "\n",
        "**Four-sentence paragraph summary:**\n",
        "1) Objective: {answers5['objective']}\n",
        "2) Methodology: {answers5['methodology']}\n",
        "3) Findings / System built: {answers5['findings']}\n",
        "4) Practical implications: {answers5['implications']}\n",
        "\"\"\"\n",
        "\n",
        "print(md5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlwiB89vgf5_",
        "outputId": "54e97ed5-d437-43fd-c918-1f16138f79e5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Exercise 5: 5W1H Summary\n",
            "\n",
            "**Who conducted the study?**  \n",
            "Notably, similar to all APIs, data \n",
            "transmitted through the fine-tuning API is owned by the customers, and OpenAI or any other organization is precluded \n",
            "from employing this data for training alternative models, as officially stated. Subsequently, when user \n",
            "queries arise—such as inquiries regarding company dress codes through a chatbot—pertinent information can be \n",
            "retrieved and presented to the LLM through prompts, proving to be a more practical and efficient approach. Conclusion and Discussion \n",
            "In this study, we presented methods and implementation cases for developing generative AI services using LLM \n",
            "application architecture, aiming to explore avenues for advancing the development and industrial utilization of \n",
            "generative AI technology.\n",
            "\n",
            "**What was developed or proposed?**  \n",
            "In pursuit of overcoming such limitations, OpenAI introduced the capability to fine-tune the GPT-3.5 Turbo \n",
            "model, a significant advancement unveiled in August 2023. Notably, in 2021, DeepMind introduced RETRO, utilizing its internal database for information retrieval, \n",
            "while OpenAI unveiled WebGPT with Bing-based searching capabilities in the same year. Against this backdrop, this study seeks to propel the advancement of generative AI while surmounting its limitations, \n",
            "by exploring methods to implement LLM applications using the RAG architecture.\n",
            "\n",
            "**When and Where was it published?**  \n",
            "Notably, in 2021, DeepMind introduced RETRO, utilizing its internal database for information retrieval, \n",
            "while OpenAI unveiled WebGPT with Bing-based searching capabilities in the same year. (2023) A Study on the Service Integration of Traditional Chatbot and ChatGPT, Journal of \n",
            "Information Technology Application and Management, 30(4), pp. Cho, J.I.(2023) Ultra-Large AI and Generative Artificial Intelligence, TTA Journal, Issue 207.\n",
            "\n",
            "**Why is the problem important?**  \n",
            "We also discussed ways to overcome the \n",
            "information scarcity challenge of LLM, either through fine-tuning or direct document information utilization, and delved \n",
            "into the specific functioning and key stages of the RAG model. Secondly, the consistency and appropriateness of \n",
            "RAG model-generated results may vary, and the challenge of information scarcity might still arise. Exploring and applying various \n",
            "techniques to enhance the quality of generated content is important.\n",
            "\n",
            "**How was the solution implemented?**  \n",
            "This approach \n",
            "involves additional training on specific domains using fresh datasets. 3 \n",
            " \n",
            "However, this approach entails significant costs. Subsequently, when user \n",
            "queries arise—such as inquiries regarding company dress codes through a chatbot—pertinent information can be \n",
            "retrieved and presented to the LLM through prompts, proving to be a more practical and efficient approach.\n",
            "\n",
            "---\n",
            "\n",
            "**Four-sentence paragraph summary:**  \n",
            "1) Objective: We also discussed ways to overcome the \n",
            "information scarcity challenge of LLM, either through fine-tuning or direct document information utilization, and delved \n",
            "into the specific functioning and key stages of the RAG model. Secondly, the consistency and appropriateness of \n",
            "RAG model-generated results may vary, and the challenge of information scarcity might still arise. Exploring and applying various \n",
            "techniques to enhance the quality of generated content is important.  \n",
            "2) Methodology: This approach \n",
            "involves additional training on specific domains using fresh datasets. 3 \n",
            " \n",
            "However, this approach entails significant costs. Subsequently, when user \n",
            "queries arise—such as inquiries regarding company dress codes through a chatbot—pertinent information can be \n",
            "retrieved and presented to the LLM through prompts, proving to be a more practical and efficient approach.  \n",
            "3) Findings / System built: In pursuit of overcoming such limitations, OpenAI introduced the capability to fine-tune the GPT-3.5 Turbo \n",
            "model, a significant advancement unveiled in August 2023. Notably, in 2021, DeepMind introduced RETRO, utilizing its internal database for information retrieval, \n",
            "while OpenAI unveiled WebGPT with Bing-based searching capabilities in the same year. Against this backdrop, this study seeks to propel the advancement of generative AI while surmounting its limitations, \n",
            "by exploring methods to implement LLM applications using the RAG architecture.  \n",
            "4) Practical implications: The architecture can be adapted to various enterprise contexts to improve information retrieval, reduce hallucinations, and support decision-making.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6: Design Reflection — Apply What You Learned"
      ],
      "metadata": {
        "id": "a4G_VKaziTL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Case\n",
        "**Educational RAG Chatbot for Autism Support**  \n",
        "Audience: parents, teachers, educators.  \n",
        "Goal: answer practical questions (communication, routines, behavior management) with **citations to sources**.\n",
        "\n",
        "---\n",
        "\n",
        "### RAG Pipeline (Simplified Sketch)\n",
        "1) **Ingestion** → collect scientific articles (PDF), institutional guides (HTML/PDF), internal FAQs (Docs).  \n",
        "2) **Preprocessing** → text cleaning, normalization, metadata extraction (title, author, year, URL).  \n",
        "3) **Chunking** → split into sections (≈300-500 tokens), 10-20% overlap, store `source`, `page`, `date`.  \n",
        "4) **Embeddings** → encode each chunk (sentence-transformers / `text-embedding-3-large` equivalent).  \n",
        "5) **Vector Store** → upsert (index + metadata).  \n",
        "6) **Retrieval** → top-k (k=4-8) + (optional) BM25/hybrid re-ranking.  \n",
        "7) **Generation** → structured prompt with *citations* (sources + pages), honesty instructions (“say I don't know”).  \n",
        "8) **Guardrails** → filters (off-topic, medical requests -> disclaimer + suggest professional advice), contradiction detection.  \n",
        "9) **Feedback Loop** → “useful/not useful” button, query logging, continuous improvement.\n",
        "\n",
        "---\n",
        "\n",
        "### Data to Collect & Chunk\n",
        "- Scientific articles (PDF): systematic reviews, controlled trials, DSM/APA guidelines.  \n",
        "- Practical guides (institutions, NGOs), classroom protocols, visual supports.  \n",
        "- Internal notes (FAQ), social stories, checklists.  \n",
        "- **Chunking:** by headings/paragraphs; keep `source_url`, `section`, `year`, `doc_type`.\n",
        "\n",
        "---\n",
        "\n",
        "### Vector Database (and Why)\n",
        "- **Chroma** (local, free) for fast prototyping, Python simplicity.  \n",
        "- **Pinecone / Weaviate / Milvus** (managed / scalable) if low latency, scaling, metadata filtering needed (e.g., `year>=2018`, `doc_type='guideline'`).  \n",
        "- **MVP choice:** Chroma (fast dev), then Pinecone if usage increases.\n",
        "\n",
        "---\n",
        "\n",
        "### LLM to Integrate\n",
        "- **MVP (cost/latency):** compact instruct model (e.g., Mistral-7B-Instruct via API) + well-structured prompt.  \n",
        "- **Higher quality:** larger model (e.g., GPT-4-level or equivalent) for better coherence and strict citation use.  \n",
        "- **Prompting:** “Answer with numbered citations [1][2] mapping to sources and page numbers. If unsure, say so and suggest exact search terms.”\n",
        "\n",
        "---\n",
        "\n",
        "### Hallucinations & Outdated Responses\n",
        "- **RAG-first:** answers only from retrieved chunks; refuse if no relevant source (score < threshold).  \n",
        "- **Mandatory citations:** each paragraph includes sources (short title, year, page).  \n",
        "- **Freshness policy:** default filter `year >= 2018`; “include older sources” button if historical context needed.  \n",
        "- **Light cross-check:** re-retrieve to ensure key statements appear in ≥2 sources (if available).  \n",
        "- **Disclaimers:** no personalized medical advice; encourage consulting a professional.\n",
        "\n",
        "---\n",
        "\n",
        "### Anticipated Challenges (and Mitigations)\n",
        "- **Source quality/reliability** → whitelist domains + initial manual review.  \n",
        "- **Heterogeneous data (scanned PDFs, tables)** → OCR + table extraction (Camelot/Tabula) + human QA.  \n",
        "- **Relevance evaluation** → create a set of 50-100 “gold” Q&A pairs with expected sources.  \n",
        "- **Cost/latency** → cache frequent hits, default top-k=4, optional re-ranking.  \n",
        "- **Security & GDPR** → no PII; anonymized logs; forbid upload of personal data.  \n",
        "- **Content drift** → weekly re-crawl & incremental re-index; monitor broken links.\n",
        "\n",
        "---\n",
        "\n",
        "### Success Criteria (Definition of Done)\n",
        "- ≥80% of answers contain **2+ valid citations** (title/year/page).  \n",
        "- **User feedback ≥4/5** on 30 frequent questions (usefulness/clarity/tone).  \n",
        "- **Latency P50 < 2s, P95 < 5s** (on 100 test queries).  \n",
        "- Eval set: EM/F1 (closed QA), NDCG@k (retrieval), human annotation (usefulness, accuracy).\n",
        "\n",
        "---\n",
        "\n",
        "### Minimal Roadmap\n",
        "- **Week 1:** collect 30-50 docs, ingestion → embeddings → index (Chroma), Q/A prototype in notebook.  \n",
        "- **Week 2:** simple UX (Streamlit), citations, metadata filters (year/type), feedback button.  \n",
        "- **Week 3:** evaluation (question set), tuning chunking/top-k, optional re-ranking.  \n",
        "- **Week 4:** hardening (guardrails, disclaimers), basic monitoring, potential switch to Pinecone.\n"
      ],
      "metadata": {
        "id": "C_ptZ4o7jCYg"
      }
    }
  ]
}