{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Daily Challenge: W8_D5"
      ],
      "metadata": {
        "id": "TDDGQlSFZz0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building an Agent with LangGraph and Gemini API\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How to create a stateful application using **LangGraph**.  \n",
        "- How to integrate **Gemini API** (via LangChain) into your LangGraph application.  \n",
        "- How to define and manipulate **state** using `TypedDict` and node functions.  \n",
        "- How to simulate dynamic, tool-augmented behavior (like menus and ordering) in a conversational loop.  \n",
        "- How to model **conditional transitions**, loops, and user interaction using LangGraph.\n",
        "\n",
        "---\n",
        "\n",
        "## What You Will Create\n",
        "\n",
        "A conversational **cafe ordering system** called **BaristaBot**. This bot will:\n",
        "\n",
        "- Use natural language to take coffee/tea orders.  \n",
        "- Offer a real-time menu using a tool.  \n",
        "- Confirm and modify orders.  \n",
        "- Loop through conversation until an order is placed.  \n",
        "- Handle tool calls using LangGraph's `ToolNode` mechanism.  \n",
        "\n",
        "This graph-based app simulates a **real-world cafe assistant** using AI + state management.\n",
        "\n",
        "---\n",
        "\n",
        "## Task Overview\n",
        "\n",
        "In this notebook, you will use **LangGraph** to define a **stateful graph-based application** built on top of the **Gemini API**.\n",
        "\n",
        "You will build a simulated cafe ordering system, **BaristaBot**. It will:\n",
        "\n",
        "- Provide a looping chat interface to customers where they can order cafe beverages using natural language.  \n",
        "- Use nodes to represent the cafe's **live menu** and the **back room** ordering system.  \n",
        "\n",
        "BaristaBot is used in other Gemini API demos. If you prefer a more minimal implementation, check out the **BaristaBot function calling example** that implements a similar system using only the Gemini API Python SDK and function calling.\n",
        "\n",
        "---\n",
        "\n",
        "## Steps\n",
        "\n",
        "1. **Install and import** the LangGraph SDK and LangChain support for Gemini API.  \n",
        "2. **Set up your API key** (via Kaggle Secrets or manual environment variable).  \n",
        "3. **Define the application state and system instructions** (conversation rules + order tracking).  \n",
        "4. **Create a single-turn chatbot node** and connect it in a basic graph.  \n",
        "5. **Run and visualize the graph** to test the first interaction.  \n",
        "6. **Manually add a second user interaction** to simulate a continued conversation.  \n",
        "7. **Add a human node** for automated conversation looping (user ‚Üî chatbot).  \n",
        "8. **Introduce conditional transitions** (exit when user types `q` or finishes order).  \n",
        "9. **Add a live menu tool** using `@tool` and integrate it with the chatbot.  \n",
        "10. **Handle orders with stateful tools** (add to order, clear order, confirm order, place order).  \n",
        "11. **Combine all nodes** into the final graph (chatbot + human + menu + ordering).  \n",
        "12. **Run the full BaristaBot application** and test ordering flow.\n",
        "\n",
        "---\n",
        "\n",
        "## Goal\n",
        "\n",
        "By the end of this notebook, you will have a **fully functional conversational agent** that simulates a cafe ordering system with:\n",
        "\n",
        "- State management via LangGraph  \n",
        "- Tool integration for menu and orders  \n",
        "- Gemini LLM powering the chatbot logic  "
      ],
      "metadata": {
        "id": "nBbaAbVYZzv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Installation and Imports"
      ],
      "metadata": {
        "id": "5XceAY_CcKkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "%pip install -qU 'langgraph==0.2.45' 'langchain-google-genai==2.0.4'\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from typing import Annotated, Literal\n",
        "from typing_extensions import TypedDict\n",
        "from collections.abc import Iterable\n",
        "from random import randint\n",
        "from pprint import pprint\n",
        "\n",
        "# LangGraph and LangChain imports\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "from langchain_core.messages.tool import ToolMessage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJXoMw14aNvU",
        "outputId": "3eb089c3-7d34-4211-b329-f6fad3ee0c83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m497.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by installing the required packages and importing all necessary libraries for building our LangGraph application with Gemini API integration."
      ],
      "metadata": {
        "id": "N2JzPC6cddY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: API Key Setup"
      ],
      "metadata": {
        "id": "qjRl1fPUaV25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1pfEJbLZyl_",
        "outputId": "7bf43c34-20ae-4922-d7c6-92cc019f7665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entre ta cl√© API: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Entre ta cl√© API: \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2 : V√©rifier que la connexion √† Gemini fonctionne\n",
        "try:\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "    print(\"üîÑ Test de connexion √† Gemini...\")\n",
        "\n",
        "    # Cr√©e une instance du mod√®le\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "    # Test simple\n",
        "    response = llm.invoke(\"Dis juste 'Hello' en r√©ponse\")\n",
        "\n",
        "    print(\"‚úÖ Connexion r√©ussie !\")\n",
        "    print(f\"R√©ponse de Gemini : {response.content}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Erreur de connexion :\")\n",
        "    print(f\"D√©tail : {e}\")\n",
        "\n",
        "    # Messages d'aide selon l'erreur\n",
        "    if \"API_KEY\" in str(e):\n",
        "        print(\"üí° Probl√®me de cl√© API - v√©rifies qu'elle est bien configur√©e\")\n",
        "    elif \"quota\" in str(e).lower():\n",
        "        print(\"üí° Probl√®me de quota - tu as peut-√™tre d√©pass√© la limite gratuite\")\n",
        "    elif \"permission\" in str(e).lower():\n",
        "        print(\"üí° Probl√®me de permissions - v√©rifies que l'API Gemini est activ√©e\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otw8z5c7bcHo",
        "outputId": "d6027eff-1e92-427f-f556-7cc9bb055e3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Test de connexion √† Gemini...\n",
            "‚úÖ Connexion r√©ussie !\n",
            "R√©ponse de Gemini : Hello\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Core State and Instructions"
      ],
      "metadata": {
        "id": "-5bn0Y2ncCL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the application state schema\n",
        "class OrderState(TypedDict):\n",
        "    \"\"\"State representing the customer's order conversation.\"\"\"\n",
        "\n",
        "    # Chat conversation history with automatic message appending\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "    # Customer's current order as list of strings\n",
        "    order: list[str]\n",
        "\n",
        "    # Flag to indicate if ordering process is complete\n",
        "    finished: bool\n",
        "\n",
        "# System instruction defining chatbot behavior and rules\n",
        "BARISTABOT_SYSINT = (\n",
        "    \"system\",  # Message type indicator\n",
        "    \"You are a BaristaBot, an interactive cafe ordering system. A human will talk to you about the \"\n",
        "    \"available products you have and you will answer any questions about menu items (and only about \"\n",
        "    \"menu items - no off-topic discussion, but you can chat about the products and their history). \"\n",
        "    \"The customer will place an order for 1 or more items from the menu, which you will structure \"\n",
        "    \"and send to the ordering system after confirming the order with the human. \"\n",
        "    \"\\n\\n\"\n",
        "    \"Add items to the customer's order with add_to_order, and reset the order with clear_order. \"\n",
        "    \"To see the contents of the order so far, call get_order (this is shown to you, not the user) \"\n",
        "    \"Always confirm_order with the user (double-check) before calling place_order. Calling confirm_order will \"\n",
        "    \"display the order items to the user and returns their response to seeing the list. Their response may contain modifications. \"\n",
        "    \"Always verify and respond with drink and modifier names from the MENU before adding them to the order. \"\n",
        "    \"If you are unsure a drink or modifier matches those on the MENU, ask a question to clarify or redirect. \"\n",
        "    \"You only have the modifiers listed on the menu. \"\n",
        "    \"Once the customer has finished ordering items, Call confirm_order to ensure it is correct then make \"\n",
        "    \"any necessary updates and then call place_order. Once place_order has returned, thank the user and \"\n",
        "    \"say goodbye!\",\n",
        ")\n",
        "\n",
        "# Welcome message to start conversations\n",
        "WELCOME_MSG = \"Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\"\n",
        "\n",
        "print(\"‚úÖ State schema and system instructions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMtu7rF6boPZ",
        "outputId": "4e35659f-e5e8-47a1-affb-b1f9bc97a97b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ State schema and system instructions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the state structure using TypedDict to track conversation messages, current order, and completion status. The system instruction establishes the chatbot's role and operational rules."
      ],
      "metadata": {
        "id": "Wxv89kawbtPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Simple Chatbot Node"
      ],
      "metadata": {
        "id": "IWaleauPb_vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gemini model with fast flash variant\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "def chatbot(state: OrderState) -> OrderState:\n",
        "    \"\"\"Simple chatbot node that processes messages and returns AI response.\"\"\"\n",
        "    # Combine system instruction with conversation history\n",
        "    message_history = [BARISTABOT_SYSINT] + state[\"messages\"]\n",
        "    # Get model response and return updated state\n",
        "    return {\"messages\": [llm.invoke(message_history)]}\n",
        "\n",
        "# Create initial graph builder with state schema\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add chatbot as a node in the graph\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Define entry point from START to chatbot\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Compile the graph for execution\n",
        "chat_graph = graph_builder.compile()\n",
        "\n",
        "print(\"‚úÖ Simple chatbot graph created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-dqratcbt33",
        "outputId": "e9f0c9b8-5700-4704-b2e8-aa3dd293c4f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Simple chatbot graph created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a basic single-node graph with a chatbot that can process one conversational turn using the Gemini model."
      ],
      "metadata": {
        "id": "WPZbdtwJb2Ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Visualize Initial Graph"
      ],
      "metadata": {
        "id": "Y3oOmgZgdqzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Display graph structure as visual diagram\n",
        "    from IPython.display import Image\n",
        "    Image(chat_graph.get_graph().draw_mermaid_png())\n",
        "except Exception as e:\n",
        "    print(\"Graph visualization not available in this environment\")\n",
        "    print(\"Graph structure: START -> chatbot -> END\")"
      ],
      "metadata": {
        "id": "X71Nrfi9b7Em"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We attempt to visualize the graph structure to understand the flow of our application."
      ],
      "metadata": {
        "id": "3iIoEJlPdy7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Test Simple Chatbot"
      ],
      "metadata": {
        "id": "m0o4usKHd1rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test single conversation turn\n",
        "user_msg = \"What drinks do you have?\"\n",
        "state = chat_graph.invoke({\"messages\": [(\"user\", user_msg)], \"order\": [], \"finished\": False})\n",
        "\n",
        "# Display conversation messages\n",
        "print(\"=== Conversation ===\")\n",
        "for msg in state[\"messages\"]:\n",
        "    print(f\"{type(msg).__name__}: {msg.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuBWQ3Ehd4oq",
        "outputId": "52e7664f-3529-4c60-b76d-8b0076e42157"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Conversation ===\n",
            "HumanMessage: What drinks do you have?\n",
            "AIMessage: We have a variety of coffee drinks, including espresso, Americano, macchiato, latte, cappuccino, mocha, and iced coffee.  We also offer tea, hot chocolate, and juices.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We test the basic chatbot functionality with a simple user query about available drinks."
      ],
      "metadata": {
        "id": "Q2JOO2c8d-LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Add Second Conversation Turn"
      ],
      "metadata": {
        "id": "4BCcVBXbd-1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue conversation with another turn\n",
        "user_msg = \"Tell me about your coffee options\"\n",
        "\n",
        "# Append new user message to existing state\n",
        "state[\"messages\"].append((\"user\", user_msg))\n",
        "state = chat_graph.invoke(state)\n",
        "\n",
        "print(\"\\n=== Extended Conversation ===\")\n",
        "for msg in state[\"messages\"]:\n",
        "    print(f\"{type(msg).__name__}: {msg.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8rqdiNaeGVI",
        "outputId": "f929170a-f65e-489e-fac1-786f844d57d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Extended Conversation ===\n",
            "HumanMessage: What drinks do you have?\n",
            "AIMessage: We have a variety of coffee drinks, including espresso, Americano, macchiato, latte, cappuccino, mocha, and iced coffee.  We also offer tea, hot chocolate, and juices.\n",
            "\n",
            "HumanMessage: Tell me about your coffee options\n",
            "AIMessage: Our espresso is made with freshly roasted, high-quality beans.  The Americano is espresso diluted with hot water. The macchiato is espresso marked with a dollop of foamed milk.  The latte is espresso with steamed milk and a thin layer of foam. The cappuccino has equal parts espresso, steamed milk, and foamed milk. The mocha is espresso with chocolate syrup, steamed milk, and whipped cream. Our iced coffee is brewed coffee served over ice.  Do any of those sound good to you?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demonstrates how state is maintained across multiple conversation turns."
      ],
      "metadata": {
        "id": "9iJD4FIHeOgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Add Human Interaction Node"
      ],
      "metadata": {
        "id": "_ljQ_uDOePRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def human_node(state: OrderState) -> OrderState:\n",
        "    \"\"\"Handle human interaction - display AI message and get user input.\"\"\"\n",
        "    # Get the last message from the AI\n",
        "    last_msg = state[\"messages\"][-1]\n",
        "    print(\"Model:\", last_msg.content)\n",
        "\n",
        "    # Get user input from console\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # Check for quit commands and set finished flag\n",
        "    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
        "        state[\"finished\"] = True\n",
        "\n",
        "    # Return updated state with new user message\n",
        "    return state | {\"messages\": [(\"user\", user_input)]}\n",
        "\n",
        "def chatbot_with_welcome_msg(state: OrderState) -> OrderState:\n",
        "    \"\"\"Enhanced chatbot with welcome message handling.\"\"\"\n",
        "\n",
        "    if state[\"messages\"]:\n",
        "        # Continue existing conversation\n",
        "        new_output = llm.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
        "    else:\n",
        "        # Start new conversation with welcome\n",
        "        new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    return state | {\"messages\": [new_output]}\n",
        "\n",
        "print(\"‚úÖ Human interaction node created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiPEwAu7eSB9",
        "outputId": "e3bd8d80-cec5-4f2b-cc29-533c4a62d653"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Human interaction node created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add a human node to handle user input and output display, plus enhance the chatbot to handle conversation initiation."
      ],
      "metadata": {
        "id": "ftnWvic5eXRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Add Conditional Routing"
      ],
      "metadata": {
        "id": "XJHLz3JteX3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_exit_human_node(state: OrderState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "    \"\"\"Route to chatbot or end based on finished flag.\"\"\"\n",
        "    if state.get(\"finished\", False):\n",
        "        return END  # Exit application\n",
        "    else:\n",
        "        return \"chatbot\"  # Continue conversation\n",
        "\n",
        "# Build new graph with human interaction\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add both chatbot and human nodes\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "\n",
        "# Define flow: START -> chatbot -> human -> [chatbot|END]\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", \"human\")\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)  # ‚Üê CORRECTION ICI\n",
        "\n",
        "# Compile interactive chat graph\n",
        "chat_with_human_graph = graph_builder.compile()\n",
        "\n",
        "print(\"‚úÖ Interactive chat graph with conditional routing created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbec3_-te2Bh",
        "outputId": "aa264c9e-5976-43ec-c686-f6b49470e616"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Interactive chat graph with conditional routing created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implement conditional routing to allow the conversation to continue or end based on user input, creating a proper chat loop."
      ],
      "metadata": {
        "id": "kShdZDbfe61t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Add Menu Tool"
      ],
      "metadata": {
        "id": "B9a5v0V8e7ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def get_menu() -> str:\n",
        "    \"\"\"Provide the latest up-to-date cafe menu with all available items and modifiers.\"\"\"\n",
        "    return \"\"\"\n",
        "    MENU:\n",
        "    Coffee Drinks:\n",
        "    Espresso\n",
        "    Americano\n",
        "    Cold Brew\n",
        "\n",
        "    Coffee Drinks with Milk:\n",
        "    Latte\n",
        "    Cappuccino\n",
        "    Cortado\n",
        "    Macchiato\n",
        "    Mocha\n",
        "    Flat White\n",
        "\n",
        "    Tea Drinks:\n",
        "    English Breakfast Tea\n",
        "    Green Tea\n",
        "    Earl Grey\n",
        "\n",
        "    Tea Drinks with Milk:\n",
        "    Chai Latte\n",
        "    Matcha Latte\n",
        "    London Fog\n",
        "\n",
        "    Other Drinks:\n",
        "    Steamer\n",
        "    Hot Chocolate\n",
        "\n",
        "    Modifiers:\n",
        "    Milk options: Whole, 2%, Oat, Almond, 2% Lactose Free; Default option: whole\n",
        "    Espresso shots: Single, Double, Triple, Quadruple; default: Double\n",
        "    Caffeine: Decaf, Regular; default: Regular\n",
        "    Hot-Iced: Hot, Iced; Default: Hot\n",
        "    Sweeteners (option to add one or more): vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener\n",
        "    Special requests: any reasonable modification that does not involve items not on the menu, for example: 'extra hot', 'one pump', 'half caff', 'extra foam', etc.\n",
        "\n",
        "    \"dirty\" means add a shot of espresso to a drink that doesn't usually have it, like \"Dirty Chai Latte\".\n",
        "    \"Regular milk\" is the same as 'whole milk'.\n",
        "    \"Sweetened\" means add some regular sugar, not a sweetener.\n",
        "\n",
        "    Soy milk has run out of stock today, so soy is not available.\n",
        "    \"\"\"\n",
        "\n",
        "# Create tool node and bind tools to model\n",
        "tools = [get_menu]\n",
        "tool_node = ToolNode(tools)\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"‚úÖ Menu tool created and bound to model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lbu0V3fe96d",
        "outputId": "94639560-06db-491f-a9c8-513587222fc3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Menu tool created and bound to model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a tool that provides the current menu information, making it available to the chatbot for dynamic menu queries."
      ],
      "metadata": {
        "id": "E6OcWzpXfFap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Enhanced Chatbot with Tools"
      ],
      "metadata": {
        "id": "1eI4XE1bfGMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_route_to_tools(state: OrderState) -> Literal[\"tools\", \"human\"]:\n",
        "    \"\"\"Route between human or tool nodes based on tool calls.\"\"\"\n",
        "    if not (msgs := state.get(\"messages\", [])):\n",
        "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
        "\n",
        "    # Check last message for tool calls\n",
        "    msg = msgs[-1]\n",
        "\n",
        "    # Route to tools if AI made tool calls, otherwise to human\n",
        "    if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
        "        return \"tools\"\n",
        "    else:\n",
        "        return \"human\"\n",
        "\n",
        "def chatbot_with_tools(state: OrderState) -> OrderState:\n",
        "    \"\"\"Enhanced chatbot with tool calling capabilities.\"\"\"\n",
        "    # Set default values for state\n",
        "    defaults = {\"order\": [], \"finished\": False}\n",
        "\n",
        "    if state[\"messages\"]:\n",
        "        # Continue conversation with tool-enabled model\n",
        "        new_output = llm_with_tools.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
        "    else:\n",
        "        # Start with welcome message\n",
        "        new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    # Merge defaults with current state, updating only messages\n",
        "    return defaults | state | {\"messages\": [new_output]}\n",
        "\n",
        "# Build graph with tool support\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add all nodes: chatbot, human, and tools\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Define routing logic - CORRECTIONS ICI\n",
        "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)  # ‚Üê avec S\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)   # ‚Üê avec S\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")  # Tools always return to chatbot\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Compile graph with menu tools\n",
        "graph_with_menu = graph_builder.compile()\n",
        "\n",
        "print(\"‚úÖ Enhanced chatbot with menu tools created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDYgPsgJfZlI",
        "outputId": "7ed7d15a-3432-438e-8cfe-c04ab59af294"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced chatbot with menu tools created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We enhance the chatbot to support tool calling, allowing it to access the menu dynamically during conversations."
      ],
      "metadata": {
        "id": "WhoKKT_Vfd1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Order Management Tools"
      ],
      "metadata": {
        "id": "EeULsJ8dfefz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define order management tool schemas (implementation in separate node)\n",
        "@tool\n",
        "def add_to_order(drink: str, modifiers: Iterable[str]) -> str:\n",
        "    \"\"\"Add specified drink with modifiers to customer's order.\"\"\"\n",
        "    pass  # Implementation in order_node\n",
        "\n",
        "@tool\n",
        "def confirm_order() -> str:\n",
        "    \"\"\"Ask customer to confirm their current order.\"\"\"\n",
        "    pass  # Implementation in order_node\n",
        "\n",
        "@tool\n",
        "def get_order() -> str:\n",
        "    \"\"\"Return current order contents for internal use.\"\"\"\n",
        "    pass  # Implementation in order_node\n",
        "\n",
        "@tool\n",
        "def clear_order():\n",
        "    \"\"\"Remove all items from customer's order.\"\"\"\n",
        "    pass  # Implementation in order_node\n",
        "\n",
        "@tool\n",
        "def place_order() -> int:\n",
        "    \"\"\"Send order to kitchen and return estimated completion time.\"\"\"\n",
        "    pass  # Implementation in order_node\n",
        "\n",
        "print(\"‚úÖ Order management tool schemas defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L-AWdSCfhB8",
        "outputId": "c1c9c27a-4fd4-45d1-a1de-8bf8b6495d77"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Order management tool schemas defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the schemas for order management tools that will be implemented in a dedicated order handling node."
      ],
      "metadata": {
        "id": "TawuR3lpfoI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Order Processing Node"
      ],
      "metadata": {
        "id": "tOs9xIvcfpSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def order_node(state: OrderState) -> OrderState:\n",
        "    \"\"\"Handle all order-related tool calls and state modifications.\"\"\"\n",
        "    # Get the last tool message\n",
        "    tool_msg = state[\"messages\"][-1]\n",
        "    order = state.get(\"order\", []).copy()  # Work with copy to avoid mutation\n",
        "    outbound_msgs = []\n",
        "    order_placed = False\n",
        "\n",
        "    # Process each tool call\n",
        "    for tool_call in tool_msg.tool_calls:\n",
        "\n",
        "        if tool_call[\"name\"] == \"add_to_order\":\n",
        "            # Add drink with modifiers to order\n",
        "            modifiers = tool_call[\"args\"].get(\"modifiers\", [])\n",
        "            modifier_str = \", \".join(modifiers) if modifiers else \"no modifiers\"\n",
        "\n",
        "            order.append(f'{tool_call[\"args\"][\"drink\"]} ({modifier_str})')\n",
        "            response = \"\\n\".join(order)\n",
        "\n",
        "        elif tool_call[\"name\"] == \"confirm_order\":\n",
        "            # Display order to user for confirmation\n",
        "            print(\"Your order:\")\n",
        "            if not order:\n",
        "                print(\"  (no items)\")\n",
        "\n",
        "            for drink in order:\n",
        "                print(f\"  {drink}\")\n",
        "\n",
        "            response = input(\"Is this correct? \")\n",
        "\n",
        "        elif tool_call[\"name\"] == \"get_order\":\n",
        "            # Return current order contents\n",
        "            response = \"\\n\".join(order) if order else \"(no order)\"\n",
        "\n",
        "        elif tool_call[\"name\"] == \"clear_order\":\n",
        "            # Remove all items from order\n",
        "            order.clear()\n",
        "            response = \"Order cleared\"\n",
        "\n",
        "        elif tool_call[\"name\"] == \"place_order\":\n",
        "            # Send order to kitchen\n",
        "            order_text = \"\\n\".join(order)\n",
        "            print(\"Sending order to kitchen!\")\n",
        "            print(order_text)\n",
        "\n",
        "            # Simulate order processing\n",
        "            order_placed = True\n",
        "            response = str(randint(1, 5))  # Random ETA in minutes\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f'Unknown tool call: {tool_call[\"name\"]}')\n",
        "\n",
        "        # Create tool response message\n",
        "        outbound_msgs.append(\n",
        "            ToolMessage(\n",
        "                content=str(response),\n",
        "                name=tool_call[\"name\"],\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return {\"messages\": outbound_msgs, \"order\": order, \"finished\": order_placed}\n",
        "\n",
        "print(\"‚úÖ Order processing node implemented\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9GdQPESfsX6",
        "outputId": "8e81ccce-192b-4960-86e8-3cb7f72e0d7c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Order processing node implemented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This node handles all order-related operations, managing the order state and providing user interaction for order confirmation."
      ],
      "metadata": {
        "id": "VeoZ0vSXfvXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Enhanced Routing Logic"
      ],
      "metadata": {
        "id": "Y381-3jvfvLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_route_to_tools(state: OrderState) -> str:\n",
        "    \"\"\"Enhanced routing between chat, tools, ordering, and end states.\"\"\"\n",
        "    if not (msgs := state.get(\"messages\", [])):\n",
        "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
        "\n",
        "    msg = msgs[-1]\n",
        "\n",
        "    # Check if order is completed\n",
        "    if state.get(\"finished\", False):\n",
        "        return END\n",
        "\n",
        "    # Route based on tool calls\n",
        "    elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
        "        # Check if any tool calls are automated tools\n",
        "        if any(tool[\"name\"] in tool_node.tools_by_name.keys() for tool in msg.tool_calls):\n",
        "            return \"tools\"  # Automated tools\n",
        "        else:\n",
        "            return \"ordering\"  # Manual order tools\n",
        "\n",
        "    else:\n",
        "        return \"human\"  # No tools, go to human interaction\n",
        "\n",
        "print(\"‚úÖ Enhanced routing logic implemented\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EABKfgzef1V9",
        "outputId": "b923d041-6d8e-4ec4-cb10-4cd38e499010"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced routing logic implemented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implement sophisticated routing logic to handle different types of tool calls and application states.\n"
      ],
      "metadata": {
        "id": "BpQmU02ihFDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15: Complete Graph Assembly"
      ],
      "metadata": {
        "id": "G1OluMtlhFbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tool sets\n",
        "auto_tools = [get_menu]  # Automated tools\n",
        "tool_node = ToolNode(auto_tools)\n",
        "\n",
        "order_tools = [add_to_order, confirm_order, get_order, clear_order, place_order]  # Manual tools\n",
        "\n",
        "# Bind all tools to the model\n",
        "llm_with_tools = llm.bind_tools(auto_tools + order_tools)\n",
        "\n",
        "# Update chatbot to use all tools\n",
        "def chatbot_with_tools(state: OrderState) -> OrderState:\n",
        "    \"\"\"Final chatbot with all tool capabilities.\"\"\"\n",
        "    defaults = {\"order\": [], \"finished\": False}\n",
        "\n",
        "    if state[\"messages\"]:\n",
        "        new_output = llm_with_tools.invoke([BARISTABOT_SYSINT] + state[\"messages\"])\n",
        "    else:\n",
        "        new_output = AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "    return defaults | state | {\"messages\": [new_output]}\n",
        "\n",
        "# Build complete graph\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add all nodes\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_node(\"ordering\", order_node)\n",
        "\n",
        "# Define all routing - CORRECTIONS ICI\n",
        "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)  # ‚Üê avec S\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)   # ‚Üê avec S\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(\"ordering\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Compile final graph\n",
        "graph_with_order_tools = graph_builder.compile()\n",
        "\n",
        "print(\"‚úÖ Complete BaristaBot system assembled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR_9N0bchhDm",
        "outputId": "fb2225e4-a77b-4c70-c993-8567b6651231"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Complete BaristaBot system assembled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assemble the complete graph with all nodes and routing logic, creating a fully functional cafe ordering system."
      ],
      "metadata": {
        "id": "HmkpKCLNhm8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 16: Run Complete System"
      ],
      "metadata": {
        "id": "LE1xuqaZhn0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure higher recursion limit for complex conversations\n",
        "config = {\"recursion_limit\": 100}\n",
        "\n",
        "print(\"ü§ñ Starting BaristaBot - Type 'q' to quit\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize and run the complete system\n",
        "try:\n",
        "    state = graph_with_order_tools.invoke({\"messages\": []}, config)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Session completed!\")\n",
        "    print(f\"Final order: {state.get('order', [])}\")\n",
        "    print(f\"Order placed: {state.get('finished', False)}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nSession interrupted by user\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6hQaBEJhqQq",
        "outputId": "24f6feec-54a6-4a24-b0e6-e49eb77f3ebc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Starting BaristaBot - Type 'q' to quit\n",
            "==================================================\n",
            "Model: Welcome to the BaristaBot cafe. Type `q` to quit. How may I serve you today?\n",
            "User: cafe\n",
            "Model: I can make you any of the following:  Espresso, Americano, Cold Brew, Latte, Cappuccino, Cortado, Macchiato, Mocha, Flat White, English Breakfast Tea, Green Tea, Earl Grey, Chai Latte, Matcha Latte, London Fog, Steamer, or Hot Chocolate.  I can also add any of the following modifiers:  Whole milk, 2% milk, Oat milk, Almond milk, 2% Lactose Free milk, single, double, triple, or quadruple espresso shots, decaf, hot or iced, vanilla sweetener, hazelnut sweetener, caramel sauce, chocolate sauce, sugar free vanilla sweetener, and any other reasonable special requests.  What would you like to order?\n",
            "\n",
            "User: espresso\n",
            "Model: Okay, one espresso.  Anything else for you?\n",
            "\n",
            "User: breakfast\n",
            "Model: I'm sorry, we don't serve breakfast.  We do have a variety of coffee and tea drinks, though.  Would you like to order something else from our menu?\n",
            "\n",
            "\n",
            "Session interrupted by user\n"
          ]
        }
      ]
    }
  ]
}