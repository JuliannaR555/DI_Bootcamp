{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Écrit requirements.txt (selon l'énoncé) puis installe.\n",
    "from pathlib import Path\n",
    "requirements = \"\"\"\n",
    "langgraph\n",
    "langchain\n",
    "langchain_core\n",
    "langchain_community\n",
    "langchainhub\n",
    "ipykernel\n",
    "langchain_groq\n",
    "langchain_huggingface\n",
    "beautifulsoup4\n",
    "tiktoken\n",
    "chromadb\n",
    "langchain_google_genai\n",
    "python-dotenv\n",
    "ipywidgets\n",
    "\"\"\"\n",
    "Path(\"requirements.txt\").write_text(requirements)\n",
    "\n",
    "import sys, subprocess\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"requirements.txt\"])\n",
    "    print(\"Dépendances installées.\")\n",
    "except Exception as e:\n",
    "    print(\"Installation ignorée ou partiellement réussie:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612811b3",
   "metadata": {},
   "source": [
    "\n",
    "# Configuration `.env`\n",
    "> Le provider est auto-détecté dans l’ordre: `groq` si clé présente, sinon `google`, sinon `hf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optionnel) Génère un template .env si absent.\n",
    "from pathlib import Path\n",
    "if not Path(\".env\").exists():\n",
    "    Path(\".env\").write_text(\n",
    "        \"LLM_PROVIDER=\\n\"\n",
    "        \"GROQ_API_KEY=\\n\"\n",
    "        \"GOOGLE_API_KEY=\\n\"\n",
    "        \"HUGGINGFACEHUB_API_TOKEN=\\n\"\n",
    "    )\n",
    "    print(\"Template .env créé. Renseignez vos clés avant d'exécuter la suite.\")\n",
    "else:\n",
    "    print(\".env déjà présent.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29511851",
   "metadata": {},
   "source": [
    "\n",
    "# Imports\n",
    "Imports principaux (conformes à l’énoncé) et utilitaires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ad41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports demandés\n",
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Imports utilitaires et LLM/Embeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Aide debug/affichage\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc5995",
   "metadata": {},
   "source": [
    "\n",
    "# Initialisation LLM et Embeddings\n",
    "Sélection dynamique du provider (Groq / Google / HuggingFace). Les embeddings utilisent un modèle `sentence-transformers` standard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e8b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_chat_model():\n",
    "    \"\"\"Retourne un ChatModel LangChain en fonction des clés présentes et/ou LLM_PROVIDER.\"\"\"\n",
    "    provider = os.getenv(\"LLM_PROVIDER\", \"\").strip().lower()\n",
    "    groq_key = os.getenv(\"GROQ_API_KEY\", \"\").strip()\n",
    "    google_key = os.getenv(\"GOOGLE_API_KEY\", \"\").strip()\n",
    "    hf_key = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\", \"\").strip()\n",
    "\n",
    "    # Auto-détection si provider non spécifié\n",
    "    if not provider:\n",
    "        if groq_key:\n",
    "            provider = \"groq\"\n",
    "        elif google_key:\n",
    "            provider = \"google\"\n",
    "        else:\n",
    "            provider = \"hf\"\n",
    "\n",
    "    if provider == \"groq\":\n",
    "        # Modèles recommandés: llama-3.1-70b-versatile, llama-3.1-8b-instant, mixtral-8x7b-32768\n",
    "        return ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "    elif provider == \"google\":\n",
    "        # Modèles possibles: gemini-1.5-pro, gemini-1.5-flash\n",
    "        return ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "    else:\n",
    "        # Fallback simple via Hugging Face (non outillé pour tool-calling avancé, mais suffisant ici)\n",
    "        # Vous pouvez substituer par un provider local si nécessaire.\n",
    "        from langchain_huggingface import HuggingFaceEndpoint\n",
    "        return HuggingFaceEndpoint(\n",
    "            repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "            temperature=0,\n",
    "            max_new_tokens=512,\n",
    "        )\n",
    "\n",
    "def get_embeddings():\n",
    "    \"\"\"Embeddings standard pour Chroma.\"\"\"\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "chat_model = get_chat_model()\n",
    "embeddings = get_embeddings()\n",
    "\n",
    "print(f\"LLM prêt: {chat_model.__class__.__name__}\")\n",
    "print(\"Embeddings prêts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2a41e",
   "metadata": {},
   "source": [
    "\n",
    "# Construction de la base de connaissances (R de RAG)\n",
    "1. Chargement web avec `WebBaseLoader`.\n",
    "2. Découpage (`RecursiveCharacterTextSplitter`).\n",
    "3. Vectorisation avec Chroma.\n",
    "4. Création de l’outil de recherche `retriever_tool`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f20638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) URLs sources (exemples). Adaptez selon vos besoins.\n",
    "URLS = [\n",
    "    # Articles techniques ou de blog (exemples publics) :\n",
    "    \"https://python.langchain.com/docs/get_started/introduction/\",\n",
    "    \"https://python.langchain.com/docs/integrations/vectorstores/chroma/\",\n",
    "]\n",
    "\n",
    "# 2) Chargement\n",
    "loader = WebBaseLoader(URLS)\n",
    "docs = loader.load()\n",
    "\n",
    "# 3) Découpage\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 4) Vectorstore Chroma (persist_dir pour réutiliser)\n",
    "PERSIST_DIR = \"chroma_db_autocorrect_rag\"\n",
    "vectorstore = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=PERSIST_DIR)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 5) Outil retriever\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"knowledge_search\",\n",
    "    description=(\n",
    "        \"Recherche des passages pertinents dans la base de connaissances interne. \"\n",
    "        \"Utilisez cet outil pour répondre aux questions nécessitant des informations issues des documents ingérés.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Documents chargés: {len(docs)} | Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda47133",
   "metadata": {},
   "source": [
    "\n",
    "# Définition de l’état (mémoire) de l’agent\n",
    "`AgentState` contient l’historique des messages (géré par `add_messages`) et un compteur d’itérations anti-boucle infinie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    iteration: int\n",
    "\n",
    "# Helpers pour extraire le dernier ToolMessage (résultats du retriever).\n",
    "def get_last_tool_payload(messages: Sequence[BaseMessage]) -> str:\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, ToolMessage):\n",
    "            # Le contenu du ToolMessage est souvent une chaîne (formatée par le tool)\n",
    "            return msg.content if isinstance(msg.content, str) else str(msg.content)\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deda536",
   "metadata": {},
   "source": [
    "\n",
    "# Nœuds et logique\n",
    "# 1) `assistant` (router)\n",
    "Modèle outillé: décide de répondre directement ou d’appeler l’outil (`knowledge_search`).\n",
    "\n",
    "# 2) `retrieve`\n",
    "`ToolNode` préconstruit qui exécute l’outil de recherche.\n",
    "\n",
    "# 3) `grade_documents` (aiguillage conditionnel)\n",
    "LLM à sortie structurée pour décider si les résultats sont pertinents.\n",
    "\n",
    "# 4) `rewrite`\n",
    "Réécrit la question si la récupération est jugée non pertinente.\n",
    "\n",
    "# 5) `generate`\n",
    "Synthétise la réponse finale à partir des documents validés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Assistant (router) : LLM avec outil bindé\n",
    "assistant_llm = chat_model.bind_tools([retriever_tool])\n",
    "\n",
    "def ai_assistant(state: AgentState) -> AgentState:\n",
    "    \"\"\"Nœud principal: appelle le LLM outillé. Si besoin, le LLM déclenche l'outil de recherche.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = assistant_llm.invoke(messages)\n",
    "    return {\"messages\": [response], \"iteration\": state.get(\"iteration\", 0)}\n",
    "\n",
    "# 2) Retrieve: ToolNode prêt à l'emploi\n",
    "tool_node = ToolNode([retriever_tool])\n",
    "\n",
    "# 3) Grading structuré\n",
    "class GradeDecision(BaseModel):\n",
    "    relevant: Literal[\"yes\", \"no\"] = Field(..., description=\"yes si les documents sont pertinents, no sinon.\")\n",
    "    reasoning: str = Field(..., description=\"Explication courte sur la pertinence.\")\n",
    "\n",
    "grade_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Vous êtes un évaluateur de recherche. On vous donne:\n",
    "\n",
    "- Question utilisateur:\n",
    "{question}\n",
    "\n",
    "- Résultats de la recherche (texte brut):\n",
    "{tool_output}\n",
    "\n",
    "Tâche:\n",
    "1) Indiquez si les résultats sont pertinents pour répondre à la question.\n",
    "2) Répondez strictement via un schéma structuré.\n",
    "\n",
    "Exigences:\n",
    "- relevant: \"yes\" si globalement pertinent, sinon \"no\".\n",
    "- reasoning: explication concise.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "grade_chain = grade_prompt | chat_model.with_structured_output(GradeDecision)\n",
    "\n",
    "def grade_documents(state: AgentState) -> str:\n",
    "    \"\"\"Aiguilleur conditionnel: renvoie 'generate' si pertinent, sinon 'rewrite'.\"\"\"\n",
    "    last_tool = get_last_tool_payload(state[\"messages\")\n",
    "    question = \"\"\n",
    "    # Retrouve la dernière question utilisateur\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            question = msg.content\n",
    "            break\n",
    "    if not last_tool:\n",
    "        # Si pas de payload outil, mieux vaut réécrire pour clarifier la requête.\n",
    "        return \"rewrite\"\n",
    "\n",
    "    decision = grade_chain.invoke({\"question\": question, \"tool_output\": last_tool})\n",
    "    route = \"generate\" if decision.relevant == \"yes\" else \"rewrite\"\n",
    "    # On log l'analyse dans les messages (optionnel, pour audit)\n",
    "    analysis_note = AIMessage(content=f\"[Grading] relevant={decision.relevant} | {decision.reasoning}\")\n",
    "    return route\n",
    "\n",
    "# 4) Rewrite node\n",
    "rewrite_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Réécrivez la question suivante pour l'améliorer (plus claire, précise, contextuelle) en vue d'une recherche documentaire.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Renvoyez UNIQUEMENT la nouvelle question, sans commentaire.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def rewrite_question(state: AgentState) -> AgentState:\n",
    "    # Dernière question utilisateur\n",
    "    question = \"\"\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            question = msg.content\n",
    "            break\n",
    "    improved = (rewrite_prompt | chat_model | StrOutputParser()).invoke({\"question\": question})\n",
    "    # Ajoute la version améliorée en tant que nouveau message utilisateur pour relancer la boucle\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=improved)],\n",
    "        \"iteration\": state.get(\"iteration\", 0) + 1,\n",
    "    }\n",
    "\n",
    "# 5) Generate node\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Vous êtes un assistant utile. Répondez de manière complète et structurée à la question en vous appuyant sur le CONTEXTE fourni.\n",
    "\n",
    "CONTEXTE (résultats de recherche bruts):\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONSIGNES:\n",
    "- Citez vos sources si disponibles (liens/URL dans le contexte).\n",
    "- Si l'information manque, dites-le explicitement et proposez une piste de reformulation.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def generate(state: AgentState) -> AgentState:\n",
    "    # Récupère la dernière question + le dernier résultat d'outil\n",
    "    question = \"\"\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            question = msg.content\n",
    "            break\n",
    "    context = get_last_tool_payload(state[\"messages\"]) or \"(Aucun contexte disponible)\"\n",
    "    answer = (answer_prompt | chat_model | StrOutputParser()).invoke({\"context\": context, \"question\": question})\n",
    "    return {\"messages\": [AIMessage(content=answer)], \"iteration\": state.get(\"iteration\", 0)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e431781",
   "metadata": {},
   "source": [
    "\n",
    "# Construction du graphe LangGraph\n",
    "- `START → assistant`  \n",
    "- `assistant` via `tools_condition` → `tools` si un outil est appelé, sinon `generate`  \n",
    "- `tools` → aiguillage conditionnel via `grade_documents`: `generate` si pertinent, `rewrite` sinon  \n",
    "- `rewrite` → `assistant` (boucle d’auto-correction)  \n",
    "- `generate` → `END`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assemble le StateGraph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"assistant\", ai_assistant)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "graph.add_node(\"rewrite\", rewrite_question)\n",
    "graph.add_node(\"generate\", generate)\n",
    "\n",
    "graph.add_edge(START, \"assistant\")\n",
    "\n",
    "# tools_condition envoie \"tools\" si le LLM a demandé un tool-call, sinon __else__\n",
    "graph.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", \"__else__\": \"generate\"},\n",
    ")\n",
    "\n",
    "# Après exécution du tool, on grade\n",
    "graph.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    grade_documents,\n",
    "    {\"generate\": \"generate\", \"rewrite\": \"rewrite\"},\n",
    ")\n",
    "\n",
    "# Boucle de correction\n",
    "graph.add_edge(\"rewrite\", \"assistant\")\n",
    "\n",
    "# Clôture\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile l'application\n",
    "app = graph.compile()\n",
    "\n",
    "print(\"Graphe compilé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e7abb",
   "metadata": {},
   "source": [
    "\n",
    "# Exécution et Tests\n",
    "La fonction utilitaire `run_query` facilite les tests. Vous pouvez aussi activer le stream pour observer le chemin emprunté.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b46e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_query(query: str, stream: bool = True, recursion_limit: int = 15):\n",
    "    \"\"\"Exécute le graphe sur une requête utilisateur et affiche le résultat final.\n",
    "\n",
    "\n",
    "    - stream=True pour tracer les transitions.\n",
    "\n",
    "    - recursion_limit pour éviter les boucles infinies.\n",
    "\n",
    "    \"\"\"\n",
    "    init_state: AgentState = {\"messages\": [HumanMessage(content=query)], \"iteration\": 0}\n",
    "    cfg = {\"recursion_limit\": recursion_limit}\n",
    "\n",
    "    if stream:\n",
    "        print(\"\\n--- STREAM START ---\")\n",
    "        events = app.stream(init_state, config=cfg, stream_mode=\"values\")\n",
    "        final = None\n",
    "        for step in events:\n",
    "            # step est un dict partiel de l'état\n",
    "            msgs = step.get(\"messages\", [])\n",
    "            if msgs:\n",
    "                last = msgs[-1]\n",
    "                print(f\"[{last.__class__.__name__}] {getattr(last, 'name', '')}\\n{last.content[:300]}...\\n\")\n",
    "            final = step\n",
    "        print(\"--- STREAM END ---\\n\")\n",
    "        return final\n",
    "    else:\n",
    "        result = app.invoke(init_state, config=cfg)\n",
    "        return result\n",
    "\n",
    "# Exemples de tests rapides (adaptez selon les contenus ingérés)\n",
    "tests = [\n",
    "    \"Bonjour !\",  # salutation simple (devrait répondre sans outil)\n",
    "    \"Comment utiliser Chroma avec LangChain ?\",  # précis, covered by docs\n",
    "    \"Parle-moi de vectorisation pour la recherche documentaire\",  # vague: peut déclencher rewrite\n",
    "    \"Quels sont les entraînements de l'équipe nationale de rugby ?\",  # hors-sujet\n",
    "]\n",
    "\n",
    "for i, q in enumerate(tests, 1):\n",
    "    print(f\"===== TEST {i}: {q} =====\")\n",
    "    _ = run_query(q, stream=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269650f1",
   "metadata": {},
   "source": [
    "\n",
    "# Observations\n",
    "- Si la question est claire et couverte par la base, l’agent répond directement ou appelle l’outil puis génère la réponse.\n",
    "- Si la récupération est non pertinente, la réécriture s’active et boucle jusqu’à obtenir des documents mieux ciblés (ou jusqu’à la limite de récursion).\n",
    "- Le grading utilise une sortie structurée (`relevant: yes/no`) pour piloter la logique interne de l’agent.\n",
    "- Vous pouvez enrichir `URLS` avec vos propres sources, ou brancher des loaders (PDF, dossiers…) pour un RAG plus étoffé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c8192",
   "metadata": {},
   "source": [
    "\n",
    "# Bonus : Intégration Streamlit (optionnel)\n",
    "Exécutez la cellule ci-dessous pour générer un fichier `app.py`.  \n",
    "Lancez ensuite localement : `streamlit run app.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa939a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app_code = \"\"\"\n",
    "import os\n",
    "import streamlit as st\n",
    "from langchain_core.messages import HumanMessage\n",
    "from main import app  # Si vous placez le graphe dans un module 'main.py'. Ici, on va recharger localement si besoin.\n",
    "\n",
    "st.set_page_config(page_title=\"Agent RAG Auto-Correcteur\", page_icon=None, layout=\"centered\")\n",
    "\n",
    "st.title(\"Agent RAG Auto-Correcteur (LangGraph)\")\n",
    "st.write(\"Posez une question. L'agent cherchera, évaluera, réécrira si nécessaire, puis répondra.\")\n",
    "\n",
    "query = st.text_area(\"Votre question\", height=100, placeholder=\"Ex: Comment utiliser Chroma avec LangChain ?\")\n",
    "go = st.button(\"Envoyer\" )\n",
    "\n",
    "if go and query.strip():\n",
    "    with st.spinner(\"Traitement...\"):\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        init_state = {\"messages\": [HumanMessage(content=query)], \"iteration\": 0}\n",
    "        result = app.invoke(init_state, config={\"recursion_limit\": 15})\n",
    "        msgs = result.get(\"messages\", [])\n",
    "        if msgs:\n",
    "            st.subheader(\"Réponse\")\n",
    "            st.write(msgs[-1].content)\n",
    "        else:\n",
    "            st.warning(\"Aucune réponse générée.\")\n",
    "\"\"\"\n",
    "\n",
    "# Écrit un app.py minimal (nécessite que ce notebook soit converti en module 'main.py' ou\n",
    "# adaptez pour importer 'app' depuis la cellule courante).\n",
    "from pathlib import Path\n",
    "Path(\"app.py\").write_text(app_code)\n",
    "print(\"Fichier app.py écrit. Adaptez l'import du graphe si vous l'extrayez dans un module.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
